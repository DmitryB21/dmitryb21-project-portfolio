{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Neuro_Doc_Assistant ‚Äî –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "## üéØ –¶–µ–ª—å –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ **Neuro_Doc_Assistant** ‚Äî RAG-—Å–∏—Å—Ç–µ–º—ã —Å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º Agent Controller –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π.\n",
    "\n",
    "### –ß—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è:\n",
    "\n",
    "1. **Test-First –ø–æ–¥—Ö–æ–¥**: –∫–∞–∂–¥—ã–π –º–æ–¥—É–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤\n",
    "2. **Agent Controller (FSM)**: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è state machine —É–ø—Ä–∞–≤–ª—è–µ—Ç –ª–æ–≥–∏–∫–æ–π, –∞ –Ω–µ LLM\n",
    "3. **–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: Ingestion ‚Üí Retrieval ‚Üí Reranking ‚Üí Generation ‚Üí Evaluation\n",
    "4. **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ñ–æ–∫—É—Å**: –º–µ—Ç—Ä–∏–∫–∏ Precision@K, –≤–ª–∏—è–Ω–∏–µ chunk_size, overlap, K, reranking\n",
    "\n",
    "### –ß–µ–º —ç—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç ¬´–æ–±—ã—á–Ω–æ–≥–æ RAG¬ª:\n",
    "\n",
    "- ‚ùå **–û–±—ã—á–Ω—ã–π RAG**: LLM —É–ø—Ä–∞–≤–ª—è–µ—Ç –ª–æ–≥–∏–∫–æ–π, ¬´–º–∞–≥–∏—è –≤ prompt¬ª\n",
    "- ‚úÖ **Neuro_Doc_Assistant**: \n",
    "  - –õ–æ–≥–∏–∫–∞ –≤ Agent Controller (FSM)\n",
    "  - LLM ‚Äî —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å (embeddings / generation / rerank)\n",
    "  - –í—Å–µ —Ä–µ—à–µ–Ω–∏—è —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –≤ DecisionLog\n",
    "  - –ú–µ—Ç—Ä–∏–∫–∏-driven –ø–æ–¥—Ö–æ–¥\n",
    "\n",
    "### –ü—Ä–∏–Ω—Ü–∏–ø Agent Controller:\n",
    "\n",
    "```\n",
    "IDLE ‚Üí VALIDATE_QUERY ‚Üí RETRIEVE ‚Üí METADATA_FILTER ‚Üí RERANK ‚Üí GENERATE ‚Üí VALIDATE_ANSWER ‚Üí LOG_METRICS ‚Üí RETURN_RESPONSE\n",
    "```\n",
    "\n",
    "–ö–∞–∂–¥–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ‚Äî –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –ª–æ–≥–∏—Ä—É–µ–º–æ–µ –≤ DecisionLog.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "\n",
    "0. **–í–≤–µ–¥–µ–Ω–∏–µ** (—ç—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª)\n",
    "1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è** ‚Äî —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "2. **–î–∞–Ω–Ω—ã–µ –∏ –¥–æ–º–µ–Ω** ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "3. **–§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è UC-1** ‚Äî —Ç–µ—Å—Ç-–∫–µ–π—Å—ã –¥–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "4. **Ingestion & Indexing** ‚Äî DocumentLoader, Chunker, EmbeddingService, QdrantIndexer\n",
    "5. **Retrieval Layer** ‚Äî semantic search, –ø–∞—Ä–∞–º–µ—Ç—Ä K\n",
    "6. **Reranking** ‚Äî —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å/–±–µ–∑ reranking\n",
    "7. **Agent Controller (FSM)** ‚Äî state machine –≤ –¥–µ–π—Å—Ç–≤–∏–∏\n",
    "8. **Generation Layer** ‚Äî PromptBuilder, —Å—Ç—Ä–æ–≥–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è\n",
    "9. **Evaluation & Metrics** ‚Äî Precision@K, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π\n",
    "10. **–ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥** ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "---\n",
    "\n",
    "**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: –≠—Ç–æ—Ç notebook –∏—Å–ø–æ–ª—å–∑—É–µ—Ç mock-—Ä–µ–∂–∏–º –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö API –∫–ª—é—á–µ–π. –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –ª–æ–∫–∞–ª—å–Ω–æ –≤ Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "\n",
    "–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "!pip install -q tiktoken  # GPT-style BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è Chunker (–æ–±–Ω–æ–≤–ª–µ–Ω–æ: –±–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)\n",
    "!pip install -q qdrant-client>=1.7.0  # Qdrant –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Qdrant Cloud)\n",
    "!pip install -q requests>=2.32.5  # HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è EmbeddingService (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å langchain –∏ datasets)\n",
    "!pip install -q urllib3>=2.0.2  # Retry —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (–æ–±–Ω–æ–≤–ª–µ–Ω–æ: –≤–µ—Ä—Å–∏—è 2.0.0 –±—ã–ª–∞ –æ—Ç–æ–∑–≤–∞–Ω–∞ –∏–∑-–∑–∞ –±–∞–≥–∞)\n",
    "!pip install -q rich  # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è RAGAS (—Ä–µ–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫)\n",
    "!pip install -q ragas  # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ RAG-—Å–∏—Å—Ç–µ–º\n",
    "!pip install -q langchain-core  # LangChain core –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ RAGAS\n",
    "!pip install -q langchain-community  # LangChain community –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
    "!pip install -q datasets  # Hugging Face datasets –¥–ª—è RAGAS\n",
    "\n",
    "print(\"‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")\n",
    "print(\"‚úÖ RAGAS –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã (—Ä–µ–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫ –¥–æ—Å—Ç—É–ø–µ–Ω)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ—è—Å–Ω–µ–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫:\n",
    "\n",
    "- **tiktoken**: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–¥—Å—á—ë—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–æ–≤ (GPT-style BPE)\n",
    "- **qdrant-client**: –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î Qdrant (–≤ Colab –∏—Å–ø–æ–ª—å–∑—É–µ–º in-memory —Ä–µ–∂–∏–º)\n",
    "- **requests**: HTTP-–∑–∞–ø—Ä–æ—Å—ã –∫ GigaChat API (–≤ mock-—Ä–µ–∂–∏–º–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)\n",
    "- **urllib3**: Retry-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "- **rich**: –ö—Ä–∞—Å–∏–≤—ã–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏\n",
    "- **ragas**: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ RAG-—Å–∏—Å—Ç–µ–º (Faithfulness, Answer Relevancy)\n",
    "- **langchain-core, langchain-community**: LangChain –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ RAGAS\n",
    "- **datasets**: Hugging Face datasets –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ RAGAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "import ssl\n",
    "import urllib3\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.panel import Panel\n",
    "from rich import print as rprint\n",
    "from google.colab import userdata\n",
    "\n",
    "# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö SSL –∑–∞–ø—Ä–æ—Å–∞—Ö (–¥–ª—è GigaChat API)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "console = Console()\n",
    "\n",
    "print(\"‚úÖ –ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ –î–∞–Ω–Ω—ã–µ –∏ –¥–æ–º–µ–Ω\n",
    "\n",
    "–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å Google Drive –∏ –ø–æ—è—Å–Ω—è–µ–º –∏—Ö –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === –ù–ê–°–¢–†–û–ô–ö–ê GOOGLE DRIVE ===\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"–ü–æ–¥–∫–ª—é—á–∞–µ–º Google –î–∏—Å–∫...\")\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –Ω–∞ Google Drive (–∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ —Å–≤–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É)\n",
    "DRIVE_DATA_PATH = Path(\"/content/drive/MyDrive/NeuroDoc_Data\")\n",
    "# –õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è\n",
    "LOCAL_DATA_PATH = Path(\"/content/neurodoc_data\")\n",
    "LOCAL_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞\n",
    "data_structure = {\n",
    "    \"hr/\": \"HR-–¥–æ–∫—É–º–µ–Ω—Ç—ã (–ø–æ–ª–∏—Ç–∏–∫–∏, —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã, –ø—Ä–æ—Ü–µ–¥—É—Ä—ã)\",\n",
    "    \"it/\": \"IT-–¥–æ–∫—É–º–µ–Ω—Ç—ã (SLA —Å–µ—Ä–≤–∏—Å–æ–≤, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, API)\",\n",
    "    \"onboarding/\": \"–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–Ω–±–æ—Ä–¥–∏–Ω–≥–∞ –Ω–æ–≤—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤\",\n",
    "    \"compliance/\": \"–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–æ–º–ø–ª–∞–µ–Ω—Å—É (–ì–ö –†–§, –¢–ö –†–§, –£–ö –†–§)\"\n",
    "}\n",
    "\n",
    "console.print(\"\\n[bold cyan]–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:[/bold cyan]\")\n",
    "for path, desc in data_structure.items():\n",
    "    console.print(f\"  üìÅ {path:20s} ‚Üí {desc}\")\n",
    "\n",
    "# –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å Google Drive (–µ—Å–ª–∏ –æ–Ω–∏ —Ç–∞–º –µ—Å—Ç—å)\n",
    "import shutil\n",
    "if DRIVE_DATA_PATH.exists():\n",
    "    console.print(f\"\\n[cyan]–ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å Google Drive: {DRIVE_DATA_PATH}[/cyan]\")\n",
    "    for category in data_structure.keys():\n",
    "        source_dir = DRIVE_DATA_PATH / category.rstrip('/')\n",
    "        target_dir = LOCAL_DATA_PATH / category.rstrip('/')\n",
    "        if source_dir.exists():\n",
    "            shutil.copytree(source_dir, target_dir, dirs_exist_ok=True)\n",
    "            md_files = list(target_dir.glob(\"*.md\"))\n",
    "            console.print(f\"  ‚úÖ {category}: {len(md_files)} MD —Ñ–∞–π–ª–æ–≤\")\n",
    "else:\n",
    "    console.print(f\"\\n[yellow]‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {DRIVE_DATA_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞[/yellow]\")\n",
    "    console.print(\"  –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...\")\n",
    "    for category in data_structure.keys():\n",
    "        (LOCAL_DATA_PATH / category.rstrip('/')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "console.print(\"\\n[bold yellow]–í–∞–∂–Ω–æ –¥–ª—è UC-1:[/bold yellow]\")\n",
    "console.print(\"  ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ hr/ –∏ it/ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ SLA-–∏—Å—Ç–æ—á–Ω–∏–∫–∏\")\n",
    "console.print(\"  ‚Ä¢ UC-1: '–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?' ‚Äî –∏—â–µ—Ç –≤ it/ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ—è—Å–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n",
    "\n",
    "- **HR –¥–æ–∫—É–º–µ–Ω—Ç—ã**: –ü–æ–ª–∏—Ç–∏–∫–∏ —É–¥–∞–ª—ë–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã, –ø–æ—Ä—è–¥–æ–∫ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –æ—Ç–ø—É—Å–∫–∞, –±–æ–ª—å–Ω–∏—á–Ω—ã–µ, –ø—Ä–µ–º–∏–∏, KPI –∏ —Ç.–¥.\n",
    "- **IT –¥–æ–∫—É–º–µ–Ω—Ç—ã**: SLA —Å–µ—Ä–≤–∏—Å–æ–≤ (–ø–ª–∞—Ç–µ–∂–∏, API, –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞), –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º\n",
    "- **Onboarding**: –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤\n",
    "- **Compliance**: –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–ì–ö –†–§, –¢–ö –†–§, –£–ö –†–§)\n",
    "\n",
    "**–î–ª—è UC-1** –º—ã —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ IT-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ SLA —Å–µ—Ä–≤–∏—Å–æ–≤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ –§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è UC-1 (Test-First)\n",
    "\n",
    "**–ü—Ä–∏–Ω—Ü–∏–ø Test-First**: —Å–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ —Ç–µ—Å—Ç-–∫–µ–π—Å—ã, –∑–∞—Ç–µ–º —Ä–µ–∞–ª–∏–∑—É–µ–º.\n",
    "\n",
    "## –û–ø–∏—Å–∞–Ω–∏–µ UC-1\n",
    "\n",
    "**Use Case**: –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "\n",
    "**–°—Ü–µ–Ω–∞—Ä–∏–π**: –°–æ—Ç—Ä—É–¥–Ω–∏–∫ –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å –ø—Ä–æ SLA —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞**: `\"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"`\n",
    "\n",
    "## SLA-–æ–∂–∏–¥–∞–Ω–∏—è\n",
    "\n",
    "1. **Precision@3 ‚â• 80%**: –í —Ç–æ–ø-3 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ‚â• 80% —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö\n",
    "2. **Faithfulness ‚â• 0.85**: –û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω —Å—Ç—Ä–æ–≥–æ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (RAGAS –º–µ—Ç—Ä–∏–∫–∞)\n",
    "3. **–ù–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π**: –û—Ç–≤–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–π –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö\n",
    "4. **–ï—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏**: –ö–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç-–∫–µ–π—Å—ã UC-1 (–¥–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)\n",
    "\n",
    "def test_uc1_requirements():\n",
    "    \"\"\"\n",
    "    –§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π UC-1 –≤ –≤–∏–¥–µ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤.\n",
    "    \n",
    "    –≠—Ç–∏ —Ç–µ—Å—Ç—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –æ–∂–∏–¥–∞–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –î–û —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "    \"\"\"\n",
    "    requirements = {\n",
    "        \"test_1_documents_found\": {\n",
    "            \"description\": \"–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞–π–¥–µ–Ω—ã\",\n",
    "            \"given\": \"–î–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º SLA –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã\",\n",
    "            \"when\": \"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å –ø—Ä–æ SLA\",\n",
    "            \"then\": \"–°–∏—Å—Ç–µ–º–∞ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ (len(sources) > 0)\"\n",
    "        },\n",
    "        \"test_2_answer_based_on_context\": {\n",
    "            \"description\": \"–û—Ç–≤–µ—Ç –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç\",\n",
    "            \"given\": \"–ù–∞–π–¥–µ–Ω—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏\",\n",
    "            \"when\": \"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –æ—Ç–≤–µ—Ç\",\n",
    "            \"then\": \"–û—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\"\n",
    "        },\n",
    "        \"test_3_no_hallucinations\": {\n",
    "            \"description\": \"–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –≤–Ω–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\",\n",
    "            \"given\": \"–ù–∞–π–¥–µ–Ω—ã —á–∞–Ω–∫–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π\",\n",
    "            \"when\": \"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –æ—Ç–≤–µ—Ç\",\n",
    "            \"then\": \"–¢–µ–∫—Å—Ç –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –æ—Ç–≤–µ—Ç–µ (case-insensitive)\"\n",
    "        },\n",
    "        \"test_4_precision_at_3\": {\n",
    "            \"description\": \"Precision@3 >= 0.8\",\n",
    "            \"given\": \"Ground truth —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω —ç–∫—Å–ø–µ—Ä—Ç–æ–º\",\n",
    "            \"when\": \"–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è retrieval —Å K=3\",\n",
    "            \"then\": \"Precision@3 >= 0.8 (‚â•80% —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤ —Ç–æ–ø-3)\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return requirements\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è\n",
    "requirements = test_uc1_requirements()\n",
    "console.print(\"\\n[bold cyan]–¢–µ—Å—Ç-–∫–µ–π—Å—ã UC-1:[/bold cyan]\")\n",
    "for test_id, test_case in requirements.items():\n",
    "    console.print(f\"\\n[bold]{test_id}:[/bold] {test_case['description']}\")\n",
    "    console.print(f\"  Given: {test_case['given']}\")\n",
    "    console.print(f\"  When: {test_case['when']}\")\n",
    "    console.print(f\"  Then: {test_case['then']}\")\n",
    "\n",
    "print(\"\\n‚úÖ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è —Ñ–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã. –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –º–æ–¥—É–ª–∏.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Ingestion & Indexing (Test-First)\n",
    "\n",
    "–ú–æ–¥—É–ª–∏:\n",
    "1. **DocumentLoader** ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ MD —Ñ–∞–π–ª–æ–≤\n",
    "2. **Chunker** ‚Äî —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ —Å overlap\n",
    "3. **EmbeddingService** ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings (GigaChat / mock)\n",
    "4. **QdrantIndexer** ‚Äî –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant (in-memory –¥–ª—è Colab)\n",
    "\n",
    "**–ü—Ä–∏–Ω—Ü–∏–ø**: –°–Ω–∞—á–∞–ª–∞ —Ç–µ—Å—Ç—ã, –∑–∞—Ç–µ–º —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å 1: DocumentLoader\n",
    "# ============================================\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    metadata: dict\n",
    "\n",
    "class DocumentLoader:\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –ó–∞–≥—Ä—É–∑–∫–∞ MD —Ñ–∞–π–ª–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π\n",
    "    - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤)\n",
    "    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (category: hr/it/compliance)\n",
    "    \n",
    "    –í—Ö–æ–¥: path (str) ‚Äî –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "    –í—ã—Ö–æ–¥: List[Document] ‚Äî —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_documents(self, path: str) -> List[Document]:\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—É—Ç–∏.\"\"\"\n",
    "        path_obj = Path(path)\n",
    "        \n",
    "        if not path_obj.exists():\n",
    "            raise FileNotFoundError(f\"Path does not exist: {path}\")\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        if path_obj.is_file():\n",
    "            if path_obj.suffix.lower() == '.md':\n",
    "                doc = self._load_single_file(path_obj)\n",
    "                if doc:\n",
    "                    documents.append(doc)\n",
    "        elif path_obj.is_dir():\n",
    "            md_files = list(path_obj.glob(\"*.md\"))\n",
    "            for md_file in md_files:\n",
    "                doc = self._load_single_file(md_file)\n",
    "                if doc:\n",
    "                    documents.append(doc)\n",
    "        else:\n",
    "            raise ValueError(f\"Path is neither file nor directory: {path}\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _load_single_file(self, file_path: Path) -> Optional[Document]:\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω MD —Ñ–∞–π–ª.\"\"\"\n",
    "        try:\n",
    "            text = file_path.read_text(encoding='utf-8')\n",
    "            normalized_text = self._normalize_text(text)\n",
    "            metadata = self._extract_metadata(file_path)\n",
    "            doc_id = self._generate_document_id(file_path)\n",
    "            \n",
    "            return Document(\n",
    "                id=doc_id,\n",
    "                text=normalized_text,\n",
    "                metadata=metadata\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _normalize_text(self, text: str) -> str:\n",
    "        \"\"\"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤.\"\"\"\n",
    "        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def _extract_metadata(self, file_path: Path) -> dict:\n",
    "        \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É.\"\"\"\n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "        parent_dir = file_path.parent.name.lower()\n",
    "        category = parent_dir if parent_dir in ['hr', 'it', 'compliance', 'onboarding'] else 'unknown'\n",
    "        \n",
    "        return {\n",
    "            \"category\": category,\n",
    "            \"file_path\": str(file_path),\n",
    "            \"file_name\": file_path.name\n",
    "        }\n",
    "    \n",
    "    def _generate_document_id(self, file_path: Path) -> str:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞.\"\"\"\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "print(\"‚úÖ DocumentLoader —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è DocumentLoader\n",
    "\n",
    "def test_document_loader():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è DocumentLoader (pytest-style).\"\"\"\n",
    "    loader = DocumentLoader()\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞\n",
    "    # –í Colab –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª\n",
    "    # –î–ª—è –¥–µ–º–æ —Å–æ–∑–¥–∞–¥–∏–º mock-—Ñ–∞–π–ª\n",
    "    test_file = Path(\"/tmp/test_doc.md\")\n",
    "    test_file.write_text(\"# Test Document\\n\\nThis is a test document.\", encoding='utf-8')\n",
    "    \n",
    "    try:\n",
    "        docs = loader.load_documents(str(test_file))\n",
    "        assert len(docs) == 1, \"–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç\"\n",
    "        assert docs[0].text is not None, \"–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å None\"\n",
    "        assert len(docs[0].text) > 0, \"–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º\"\n",
    "        assert \"category\" in docs[0].metadata, \"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å category\"\n",
    "        \n",
    "        console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç[/green]\")\n",
    "    finally:\n",
    "        if test_file.exists():\n",
    "            test_file.unlink()\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "    normalized = loader._normalize_text(\"  –ú–Ω–æ–≥–æ    –ø—Ä–æ–±–µ–ª–æ–≤   –∑–¥–µ—Å—å  \")\n",
    "    assert normalized == \"–ú–Ω–æ–≥–æ –ø—Ä–æ–±–µ–ª–æ–≤ –∑–¥–µ—Å—å\", \"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
    "    test_path = Path(\"/tmp/hr/test_doc.md\")\n",
    "    metadata = loader._extract_metadata(test_path)\n",
    "    assert metadata[\"category\"] == \"hr\", \"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 'hr'\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞–µ—Ç[/green]\")\n",
    "\n",
    "test_document_loader()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã DocumentLoader –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å 2: Chunker\n",
    "# ============================================\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\"\"\"\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    text_length: int\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "\n",
    "class Chunker:\n",
    "    \"\"\"\n",
    "    –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ —Å overlap.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —á–∞—Å—Ç–∏\n",
    "    - Overlap 20-30% –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏\n",
    "    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n",
    "    \n",
    "    –í—Ö–æ–¥: documents (List[Document]), chunk_size (int), overlap_percent (float)\n",
    "    –í—ã—Ö–æ–¥: List[Chunk] ‚Äî —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤\n",
    "    \n",
    "    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:\n",
    "    - chunk_size: 200-400 —Ç–æ–∫–µ–Ω–æ–≤ (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 300)\n",
    "    - overlap_percent: 0.2-0.3 (20-30%)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoding_name: str = \"cl100k_base\"):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Chunker.\"\"\"\n",
    "        try:\n",
    "            self.encoding = tiktoken.get_encoding(encoding_name)\n",
    "        except Exception:\n",
    "            self.encoding = None\n",
    "    \n",
    "    def chunk_documents(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        chunk_size: int = 300,\n",
    "        overlap_percent: float = 0.25\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏.\n",
    "        \n",
    "        Args:\n",
    "            documents: –°–ø–∏—Å–æ–∫ Document –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö (200-400)\n",
    "            overlap_percent: –ü—Ä–æ—Ü–µ–Ω—Ç overlap –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ (0.2-0.3)\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            chunks = self._chunk_single_document(doc, chunk_size, overlap_percent)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "    \n",
    "    def _chunk_single_document(\n",
    "        self,\n",
    "        document: Document,\n",
    "        chunk_size: int,\n",
    "        overlap_percent: float\n",
    "    ) -> List[Chunk]:\n",
    "        \"\"\"–†–∞–∑–±–∏–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏.\"\"\"\n",
    "        text = document.text\n",
    "        \n",
    "        # –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        text_length = self._count_tokens(text)\n",
    "        \n",
    "        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–∏–Ω —á–∞–Ω–∫\n",
    "        if text_length <= chunk_size:\n",
    "            chunk_metadata = document.metadata.copy()\n",
    "            if \"source\" not in chunk_metadata:\n",
    "                chunk_metadata[\"source\"] = chunk_metadata.get(\"category\", \"unknown\")\n",
    "            \n",
    "            return [Chunk(\n",
    "                chunk_id=f\"{document.id}_chunk_000\",\n",
    "                doc_id=document.id,\n",
    "                text=text,\n",
    "                text_length=text_length,\n",
    "                metadata=chunk_metadata\n",
    "            )]\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º overlap –≤ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "        overlap_tokens = int(chunk_size * overlap_percent)\n",
    "        step_size = chunk_size - overlap_tokens\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º —Å —É—á—ë—Ç–æ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ: 1 —Ç–æ–∫–µ–Ω ‚âà 3 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        chars_per_token = 3\n",
    "        chunk_size_chars = chunk_size * chars_per_token\n",
    "        overlap_chars = overlap_tokens * chars_per_token\n",
    "        step_size_chars = step_size * chars_per_token\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = min(start + chunk_size_chars, len(text))\n",
    "            chunk_text = text[start:end]\n",
    "            \n",
    "            chunk_metadata = document.metadata.copy()\n",
    "            if \"source\" not in chunk_metadata:\n",
    "                chunk_metadata[\"source\"] = chunk_metadata.get(\"category\", \"unknown\")\n",
    "            \n",
    "            chunk = Chunk(\n",
    "                chunk_id=f\"{document.id}_chunk_{chunk_index:03d}\",\n",
    "                doc_id=document.id,\n",
    "                text=chunk_text,\n",
    "                text_length=self._count_tokens(chunk_text),\n",
    "                metadata=chunk_metadata\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # –°–¥–≤–∏–≥–∞–µ–º –æ–∫–Ω–æ —Å —É—á—ë—Ç–æ–º overlap\n",
    "            start += step_size_chars\n",
    "            chunk_index += 1\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.\"\"\"\n",
    "        if self.encoding:\n",
    "            return len(self.encoding.encode(text))\n",
    "        else:\n",
    "            # Fallback: –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á—ë—Ç (1 —Ç–æ–∫–µ–Ω ‚âà 3 —Å–∏–º–≤–æ–ª–∞)\n",
    "            return len(text) // 3\n",
    "\n",
    "print(\"‚úÖ Chunker —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å 3a: GigaChatAuth (OAuth 2.0)\n",
    "# ============================================\n",
    "\n",
    "class GigaChatAuth:\n",
    "    \"\"\"\n",
    "    –ö–ª–∞—Å—Å –¥–ª—è OAuth 2.0 –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ GigaChat API.\n",
    "    \n",
    "    –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:\n",
    "    - –ü–æ–ª—É—á–µ–Ω–∏–µ access token —á–µ—Ä–µ–∑ OAuth 2.0\n",
    "    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ (–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω 30 –º–∏–Ω—É—Ç)\n",
    "    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –ø–µ—Ä–µ–¥ –∏—Å—Ç–µ—á–µ–Ω–∏–µ–º\n",
    "    \"\"\"\n",
    "    \n",
    "    # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ endpoints GigaChat API\n",
    "    OAUTH_TOKEN_URL = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
    "    \n",
    "    def __init__(self, auth_key: Optional[str] = None, scope: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GigaChatAuth.\n",
    "        \n",
    "        Args:\n",
    "            auth_key: Base64 encoded \"Client ID:Client Secret\" (–µ—Å–ª–∏ None, –±–µ—Ä—ë—Ç—Å—è –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Colab)\n",
    "            scope: Scope –¥–ª—è OAuth (GIGACHAT_API_PERS, GIGACHAT_API_B2B, GIGACHAT_API_CORP)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.auth_key = auth_key or userdata.get(\"GIGACHAT_AUTH_KEY\")\n",
    "        except:\n",
    "            self.auth_key = auth_key\n",
    "        try:\n",
    "            self.scope = scope or userdata.get(\"GIGACHAT_SCOPE\", \"GIGACHAT_API_PERS\")\n",
    "        except:\n",
    "            self.scope = scope or \"GIGACHAT_API_PERS\"\n",
    "        \n",
    "        # –ö—ç—à —Ç–æ–∫–µ–Ω–∞\n",
    "        self._access_token_cache: Optional[str] = None\n",
    "        self._token_expires_at: float = 0\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π SSL\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=2,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            respect_retry_after_header=True\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.verify = False\n",
    "    \n",
    "    def get_access_token(self) -> Optional[str]:\n",
    "        \"\"\"–ü–æ–ª—É—á–∞–µ—Ç access token –¥–ª—è GigaChat API.\"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Ç–æ–∫–µ–Ω–∞\n",
    "        current_time = time.time()\n",
    "        if self._access_token_cache and current_time < self._token_expires_at - 60:\n",
    "            return self._access_token_cache\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω\n",
    "        if not self.auth_key:\n",
    "            return None\n",
    "        \n",
    "        rq_uid = str(uuid.uuid4())\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"RqUID\": rq_uid,\n",
    "            \"Authorization\": f\"Basic {self.auth_key}\"\n",
    "        }\n",
    "        data = {\"scope\": self.scope}\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(self.OAUTH_TOKEN_URL, headers=headers, data=data, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                response_data = response.json()\n",
    "                access_token = response_data.get(\"access_token\")\n",
    "                if access_token:\n",
    "                    self._access_token_cache = access_token\n",
    "                    expires_at_ms = response_data.get(\"expires_at\", 0)\n",
    "                    if expires_at_ms > 1000000000000:\n",
    "                        self._token_expires_at = expires_at_ms / 1000\n",
    "                    else:\n",
    "                        self._token_expires_at = expires_at_ms\n",
    "                    return access_token\n",
    "        except Exception as e:\n",
    "            console.print(f\"[yellow]‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞: {e}[/yellow]\")\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ GigaChatAuth —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è Chunker\n",
    "\n",
    "def test_chunker():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è Chunker.\"\"\"\n",
    "    chunker = Chunker()\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: –ö–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç (–æ–¥–∏–Ω —á–∞–Ω–∫)\n",
    "    short_doc = Document(\n",
    "        id=\"doc1\",\n",
    "        text=\"–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞.\",\n",
    "        metadata={\"category\": \"hr\"}\n",
    "    )\n",
    "    chunks = chunker.chunk_documents([short_doc], chunk_size=300)\n",
    "    assert len(chunks) == 1, \"–ö–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–Ω–∏–º —á–∞–Ω–∫–æ–º\"\n",
    "    assert chunks[0].text == short_doc.text, \"–¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: –ö–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: –î–ª–∏–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (–Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤)\n",
    "    long_text = \" \".join([\"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–æ–º–µ—Ä\"] * 200)  # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "    long_doc = Document(\n",
    "        id=\"doc2\",\n",
    "        text=long_text,\n",
    "        metadata={\"category\": \"it\"}\n",
    "    )\n",
    "    chunks = chunker.chunk_documents([long_doc], chunk_size=100, overlap_percent=0.25)\n",
    "    assert len(chunks) > 1, \"–î–ª–∏–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–∑–±–∏—Ç –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤\"\n",
    "    console.print(f\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: –î–ª–∏–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: Overlap –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏\n",
    "    if len(chunks) >= 2:\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å overlap (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞ –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ –µ—Å—Ç—å –≤ –Ω–∞—á–∞–ª–µ –≤—Ç–æ—Ä–æ–≥–æ)\n",
    "        chunk1_end = chunks[0].text[-50:]\n",
    "        chunk2_start = chunks[1].text[:50]\n",
    "        # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –æ–±—â–∏–µ —Å–ª–æ–≤–∞\n",
    "        words1 = set(chunk1_end.split())\n",
    "        words2 = set(chunk2_start.split())\n",
    "        common_words = words1.intersection(words2)\n",
    "        assert len(common_words) > 0, \"–î–æ–ª–∂–µ–Ω –±—ã—Ç—å overlap –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏\"\n",
    "        console.print(f\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: Overlap —Ä–∞–±–æ—Ç–∞–µ—Ç ({len(common_words)} –æ–±—â–∏—Ö —Å–ª–æ–≤)[/green]\")\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "    console.print(f\"\\n[cyan]–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞–Ω–∫–∏–Ω–≥–∞:[/cyan]\")\n",
    "    console.print(f\"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\")\n",
    "    console.print(f\"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {sum(c.text_length for c in chunks) / len(chunks):.1f} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "    console.print(f\"  –†–∞–∑–º–µ—Ä—ã: {[c.text_length for c in chunks[:5]]}...\")\n",
    "\n",
    "test_chunker()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã Chunker –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å 3: EmbeddingService (GigaChat API)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "from typing import List, Optional\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "class EmbeddingService:\n",
    "    \"\"\"\n",
    "    –°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ GigaChat Embeddings API.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ (–¥–ª—è semantic search)\n",
    "    - –í production: GigaChat Embeddings API\n",
    "    - –í Colab: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mock-—Ä–µ–∂–∏–º –∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π API (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–ª—é—á–µ–π)\n",
    "    \n",
    "    –í—Ö–æ–¥: texts (List[str])\n",
    "    –í—ã—Ö–æ–¥: List[List[float]] ‚Äî —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ embedding_dim\n",
    "    \n",
    "    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:\n",
    "    - embedding_dim: 1536 (GigaChat) –∏–ª–∏ 1024\n",
    "    - batch_size: 10 (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 1536,\n",
    "        batch_size: int = 10,\n",
    "        mock_mode: bool = True,\n",
    "        auth_key: Optional[str] = None,\n",
    "        scope: Optional[str] = None\n",
    "    ):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º auth_key (–∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Colab –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞)\n",
    "        if not auth_key:\n",
    "            try:\n",
    "                auth_key = userdata.get(\"GIGACHAT_AUTH_KEY\")\n",
    "            except:\n",
    "                auth_key = None\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º mock mode\n",
    "        if mock_mode or not auth_key:\n",
    "            self.mock_mode = True\n",
    "        else:\n",
    "            self.mock_mode = False\n",
    "            self.auth = GigaChatAuth(auth_key=auth_key, scope=scope)\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP —Å–µ—Å—Å–∏–∏\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=5,\n",
    "            backoff_factor=2,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            respect_retry_after_header=True\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.verify = False\n",
    "        \n",
    "        # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π endpoint –¥–ª—è GigaChat Embeddings API\n",
    "        self.api_url = \"https://gigachat.devices.sberbank.ru/api/v1/embeddings\"\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embeddings –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.\n",
    "        \n",
    "        –í mock-—Ä–µ–∂–∏–º–µ: —Å–æ–∑–¥–∞—ë—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ hash.\n",
    "        –í production: –≤—ã–∑—ã–≤–∞–µ—Ç GigaChat Embeddings API.\n",
    "        \"\"\"\n",
    "        if self.mock_mode:\n",
    "            return [self._generate_mock_embedding(text) for text in texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –±–∞—Ç—á–∞–º–∏\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            batch_embeddings = []\n",
    "            for text in batch:\n",
    "                embedding = self._call_gigachat_api(text)\n",
    "                batch_embeddings.append(embedding)\n",
    "                time.sleep(0.1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        return all_embeddings\n",
    "    \n",
    "    def _call_gigachat_api(self, text: str) -> List[float]:\n",
    "        \"\"\"–í—ã–∑—ã–≤–∞–µ—Ç GigaChat Embeddings API –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\"\"\"\n",
    "        if self.mock_mode:\n",
    "            return self._generate_mock_embedding(text)\n",
    "        \n",
    "        try:\n",
    "            access_token = self.auth.get_access_token()\n",
    "            if not access_token:\n",
    "                return self._generate_mock_embedding(text)\n",
    "            \n",
    "            request_id = str(uuid.uuid4())\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {access_token}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"X-Request-ID\": request_id\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": \"Embeddings\",\n",
    "                \"input\": text\n",
    "            }\n",
    "            \n",
    "            response = self.session.post(self.api_url, json=payload, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º embedding –∏–∑ –æ—Ç–≤–µ—Ç–∞\n",
    "            embedding = None\n",
    "            if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "                embedding = data[\"data\"][0].get(\"embedding\", [])\n",
    "            elif \"embedding\" in data:\n",
    "                embedding = data[\"embedding\"]\n",
    "            \n",
    "            if embedding and len(embedding) in [1024, 1536]:\n",
    "                self.embedding_dim = len(embedding)  # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
    "                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä (L2 –Ω–æ—Ä–º–∞ = 1.0) –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "                embedding_array = np.array(embedding)\n",
    "                norm = np.linalg.norm(embedding_array)\n",
    "                if norm > 0:\n",
    "                    embedding_array = embedding_array / norm\n",
    "                return embedding_array.tolist()\n",
    "            return self._generate_mock_embedding(text)\n",
    "        except Exception as e:\n",
    "            console.print(f\"[yellow]‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ GigaChat Embeddings API: {e}[/yellow]\")\n",
    "            return self._generate_mock_embedding(text)\n",
    "    \n",
    "    def _generate_mock_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ–∫–æ–≤—ã–π embedding –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞.\"\"\"\n",
    "        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "        embedding = []\n",
    "        for i in range(self.embedding_dim):\n",
    "            hash_index = i % len(text_hash)\n",
    "            char_value = ord(text_hash[hash_index])\n",
    "            normalized_value = (char_value % 200 - 100) / 100.0\n",
    "            embedding.append(normalized_value)\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä (L2 –Ω–æ—Ä–º–∞ = 1.0)\n",
    "        embedding_array = np.array(embedding)\n",
    "        norm = np.linalg.norm(embedding_array)\n",
    "        if norm > 0:\n",
    "            embedding_array = embedding_array / norm\n",
    "        return embedding_array.tolist()\n",
    "\n",
    "print(\"‚úÖ EmbeddingService —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ GigaChat API –∏ mock —Ä–µ–∂–∏–º)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è EmbeddingService\n",
    "\n",
    "def test_embedding_service():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è EmbeddingService.\"\"\"\n",
    "    embedding_service = EmbeddingService(embedding_dim=1536, mock_mode=True)\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings\n",
    "    texts = [\"–¢–µ–∫—Å—Ç –Ω–æ–º–µ—Ä –æ–¥–∏–Ω\", \"–¢–µ–∫—Å—Ç –Ω–æ–º–µ—Ä –¥–≤–∞\"]\n",
    "    embeddings = embedding_service.generate_embeddings(texts)\n",
    "    \n",
    "    assert len(embeddings) == 2, \"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 embeddings\"\n",
    "    assert len(embeddings[0]) == 1536, \"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1536\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: Embeddings –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "    vector = np.array(embeddings[0])\n",
    "    norm = np.linalg.norm(vector)\n",
    "    assert abs(norm - 1.0) < 0.01, f\"–í–µ–∫—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω (norm={norm:.3f})\"\n",
    "    console.print(f\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: –í–µ–∫—Ç–æ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (norm={norm:.3f})[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (–æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ç–µ–∫—Å—Ç –¥–∞—ë—Ç –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –≤–µ–∫—Ç–æ—Ä)\n",
    "    embeddings1 = embedding_service.generate_embeddings([\"–¢–µ—Å—Ç\"])\n",
    "    embeddings2 = embedding_service.generate_embeddings([\"–¢–µ—Å—Ç\"])\n",
    "    assert np.allclose(embeddings1[0], embeddings2[0]), \"Embeddings –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: Embeddings –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω—ã[/green]\")\n",
    "\n",
    "test_embedding_service()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã EmbeddingService –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å 4: QdrantIndexer (In-Memory –¥–ª—è Colab)\n",
    "# ============================================\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π Qdrant –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Colab\n",
    "QDRANT_URL = None\n",
    "QDRANT_API_KEY = None\n",
    "\n",
    "try:\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ userdata\n",
    "    if 'userdata' in globals():\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ç–æ—Ä–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "        # –í Colab userdata.get() –º–æ–∂–µ—Ç –≤—ã–±—Ä–∞—Å—ã–≤–∞—Ç—å KeyError, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º try-except\n",
    "        try:\n",
    "            QDRANT_URL = userdata.get(\"QDRANT_URL\")\n",
    "            # –û—á–∏—â–∞–µ–º –æ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–º–æ–≥—É—Ç –±—ã—Ç—å \\r\\n –≤ –∫–æ–Ω—Ü–µ)\n",
    "            if QDRANT_URL:\n",
    "                QDRANT_URL = QDRANT_URL.strip()\n",
    "        except (KeyError, AttributeError):\n",
    "            QDRANT_URL = None\n",
    "        \n",
    "        try:\n",
    "            QDRANT_API_KEY = userdata.get(\"QDRANT_API_KEY\")\n",
    "            # –û—á–∏—â–∞–µ–º –æ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–º–æ–≥—É—Ç –±—ã—Ç—å \\r\\n –≤ –∫–æ–Ω—Ü–µ)\n",
    "            if QDRANT_API_KEY:\n",
    "                QDRANT_API_KEY = QDRANT_API_KEY.strip()\n",
    "        except (KeyError, AttributeError):\n",
    "            QDRANT_API_KEY = None\n",
    "        \n",
    "        if QDRANT_URL:\n",
    "            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ URL –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n",
    "            url_preview = QDRANT_URL[:30] + \"...\" if len(QDRANT_URL) > 30 else QDRANT_URL\n",
    "            console.print(f\"[green]‚úÖ QDRANT_URL –Ω–∞–π–¥–µ–Ω: {url_preview}[/green]\")\n",
    "        else:\n",
    "            console.print(\"[yellow]‚ö†Ô∏è  QDRANT_URL –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω in-memory —Ä–µ–∂–∏–º[/yellow]\")\n",
    "            console.print(\"[cyan]üí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–µ–∫—Ä–µ—Ç 'QDRANT_URL' –¥–æ–±–∞–≤–ª–µ–Ω –≤ Colab Secrets[/cyan]\")\n",
    "    else:\n",
    "        console.print(\"[yellow]‚ö†Ô∏è  userdata –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–Ω–µ –≤ Colab?), –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω in-memory —Ä–µ–∂–∏–º[/yellow]\")\n",
    "except Exception as e:\n",
    "    # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –¥—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞\n",
    "    console.print(f\"[yellow]‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–µ–∫—Ä–µ—Ç–æ–≤ Qdrant: {type(e).__name__}: {e}[/yellow]\")\n",
    "    console.print(\"[yellow]   –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω in-memory —Ä–µ–∂–∏–º[/yellow]\")\n",
    "\n",
    "class QdrantIndexer:\n",
    "    \"\"\"\n",
    "    –ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–æ–≤ –≤ Qdrant.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant\n",
    "    - –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ —Å embeddings –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏\n",
    "    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ in-memory —Ä–µ–∂–∏–º–∞ –¥–ª—è Colab\n",
    "    \n",
    "    –í—Ö–æ–¥: chunks (List[Chunk]), embeddings (List[List[float]])\n",
    "    –í—ã—Ö–æ–¥: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
    "    \n",
    "    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:\n",
    "    - –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –∏–ª–∏ –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω–∞\n",
    "    - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å embedding_dim\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_client: QdrantClient = None,\n",
    "        collection_name: str = \"neuro_docs\",\n",
    "        embedding_dim: int = 1536,\n",
    "        qdrant_url: Optional[str] = None,\n",
    "        qdrant_api_key: Optional[str] = None\n",
    "    ):\n",
    "        # –ï—Å–ª–∏ qdrant_client –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞—ë–º –µ–≥–æ\n",
    "        if qdrant_client is None:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤\n",
    "            url = qdrant_url or (QDRANT_URL if 'QDRANT_URL' in globals() else None)\n",
    "            api_key = qdrant_api_key or (QDRANT_API_KEY if 'QDRANT_API_KEY' in globals() else None)\n",
    "            \n",
    "            # –û—á–∏—â–∞–µ–º –æ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–º–æ–≥—É—Ç –±—ã—Ç—å \\r\\n –≤ –∫–æ–Ω—Ü–µ –ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤)\n",
    "            if url:\n",
    "                url = url.strip()\n",
    "            if api_key:\n",
    "                api_key = api_key.strip()\n",
    "            \n",
    "            if url:\n",
    "                if api_key:\n",
    "                    self.qdrant_client = QdrantClient(url=url, api_key=api_key)\n",
    "                else:\n",
    "                    self.qdrant_client = QdrantClient(url=url)\n",
    "            else:\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º in-memory —Ä–µ–∂–∏–º\n",
    "                self.qdrant_client = QdrantClient(\":memory:\")\n",
    "        else:\n",
    "            self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def create_collection_if_not_exists(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞—ë—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\"\"\"\n",
    "        try:\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è\n",
    "            collections = self.qdrant_client.get_collections()\n",
    "            collection_names = [c.name for c in collections.collections]\n",
    "            \n",
    "            if self.collection_name not in collection_names:\n",
    "                # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=self.collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.embedding_dim,\n",
    "                        distance=Distance.COSINE\n",
    "                    )\n",
    "                )\n",
    "                console.print(f\"[green]‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å–æ–∑–¥–∞–Ω–∞[/green]\")\n",
    "            else:\n",
    "                console.print(f\"[yellow]‚ÑπÔ∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç[/yellow]\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}[/red]\")\n",
    "            raise\n",
    "    \n",
    "    def index_chunks(\n",
    "        self,\n",
    "        chunks: List[Chunk],\n",
    "        embeddings: List[List[float]]\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –≤ Qdrant.\n",
    "        \n",
    "        Args:\n",
    "            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤\n",
    "            embeddings: –°–ø–∏—Å–æ–∫ embeddings –¥–ª—è —á–∞–Ω–∫–æ–≤\n",
    "            \n",
    "        Returns:\n",
    "            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
    "        \"\"\"\n",
    "        if len(chunks) != len(embeddings):\n",
    "            raise ValueError(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –∏ embeddings –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
    "        self.create_collection_if_not_exists()\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–æ—á–∫–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=i,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –∫–∞–∫ ID (–≤ production –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å chunk_id)\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"chunk_id\": chunk.chunk_id,\n",
    "                    \"doc_id\": chunk.doc_id,\n",
    "                    \"text\": chunk.text,\n",
    "                    \"text_length\": chunk.text_length,\n",
    "                    **chunk.metadata\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–æ—á–∫–∏ –±–∞—Ç—á–∞–º–∏\n",
    "        batch_size = 100\n",
    "        indexed_count = 0\n",
    "        \n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i:i + batch_size]\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=batch\n",
    "            )\n",
    "            indexed_count += len(batch)\n",
    "        \n",
    "        return indexed_count\n",
    "\n",
    "print(\"‚úÖ QdrantIndexer —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è QdrantIndexer\n",
    "\n",
    "def test_qdrant_indexer():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è QdrantIndexer.\"\"\"\n",
    "    # –°–æ–∑–¥–∞—ë–º in-memory Qdrant –∫–ª–∏–µ–Ω—Ç\n",
    "    qdrant_client = QdrantClient(\":memory:\")\n",
    "    indexer = QdrantIndexer(qdrant_client, collection_name=\"test_collection\", embedding_dim=1536)\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n",
    "    indexer.create_collection_if_not_exists()\n",
    "    collections = qdrant_client.get_collections()\n",
    "    collection_names = [c.name for c in collections.collections]\n",
    "    assert \"test_collection\" in collection_names, \"–ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω–∞\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤\n",
    "    chunks = [\n",
    "        Chunk(\n",
    "            chunk_id=\"chunk1\",\n",
    "            doc_id=\"doc1\",\n",
    "            text=\"–¢–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞\",\n",
    "            text_length=10,\n",
    "            metadata={\"category\": \"hr\"}\n",
    "        ),\n",
    "        Chunk(\n",
    "            chunk_id=\"chunk2\",\n",
    "            doc_id=\"doc2\",\n",
    "            text=\"–¢–µ–∫—Å—Ç –≤—Ç–æ—Ä–æ–≥–æ —á–∞–Ω–∫–∞\",\n",
    "            text_length=10,\n",
    "            metadata={\"category\": \"it\"}\n",
    "        )\n",
    "    ]\n",
    "    embedding_service = EmbeddingService(mock_mode=True)\n",
    "    embeddings = embedding_service.generate_embeddings([c.text for c in chunks])\n",
    "    \n",
    "    indexed_count = indexer.index_chunks(chunks, embeddings)\n",
    "    assert indexed_count == 2, \"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ 2 —á–∞–Ω–∫–∞\"\n",
    "    console.print(f\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {indexed_count} —á–∞–Ω–∫–æ–≤[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    results = qdrant_client.scroll(\n",
    "        collection_name=\"test_collection\",\n",
    "        limit=10\n",
    "    )\n",
    "    assert len(results[0]) == 2, \"–í –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 —Ç–æ—á–∫–∏\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: –î–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ Qdrant[/green]\")\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤\n",
    "    console.print(\"\\n[cyan]–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤:[/cyan]\")\n",
    "    for point in results[0][:2]:\n",
    "        payload = point.payload\n",
    "        console.print(f\"  ‚Ä¢ {payload['chunk_id']}: {payload['text'][:50]}...\")\n",
    "        console.print(f\"    –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {payload['category']}, –†–∞–∑–º–µ—Ä: {payload['text_length']} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "\n",
    "test_qdrant_indexer()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã QdrantIndexer –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Ingestion Pipeline\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –ø–æ–∫–∞–∂–µ–º –ø–æ–ª–Ω—ã–π pipeline: –∑–∞–≥—Ä—É–∑–∫–∞ ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí embeddings ‚Üí –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª–Ω—ã–π Ingestion Pipeline (–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è)\n",
    "\n",
    "def run_ingestion_pipeline_demo():\n",
    "    \"\"\"\n",
    "    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ pipeline –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.\n",
    "    \n",
    "    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç:\n",
    "    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
    "    - –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤\n",
    "    - –ü—Ä–∏–º–µ—Ä—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
    "    \"\"\"\n",
    "    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–≤ —Ä–µ–∞–ª—å–Ω–æ–º Colab –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ data/)\n",
    "    test_docs = [\n",
    "        Document(\n",
    "            id=\"doc_sla_1\",\n",
    "            text=\"SLA —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π: –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å 99.9%, –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–µ –±–æ–ª–µ–µ 200–º—Å. –°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç 24/7.\",\n",
    "            metadata={\"category\": \"it\", \"file_name\": \"sla_payments.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            id=\"doc_sla_2\",\n",
    "            text=\"SLA —Å–µ—Ä–≤–∏—Å–∞ API: –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å 99.95%, –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–µ –±–æ–ª–µ–µ 100–º—Å. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ rate limiting.\",\n",
    "            metadata={\"category\": \"it\", \"file_name\": \"sla_api.md\"}\n",
    "        ),\n",
    "        Document(\n",
    "            id=\"doc_hr_1\",\n",
    "            text=\"–ü–æ–ª–∏—Ç–∏–∫–∞ —É–¥–∞–ª—ë–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã: —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —É–¥–∞–ª—ë–Ω–Ω–æ –¥–æ 3 –¥–Ω–µ–π –≤ –Ω–µ–¥–µ–ª—é.\",\n",
    "            metadata={\"category\": \"hr\", \"file_name\": \"remote_work.md\"}\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    console.print(\"\\n[bold cyan]=== Ingestion Pipeline ===[/bold cyan]\")\n",
    "    \n",
    "    # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    console.print(\"\\n[bold]–®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤[/bold]\")\n",
    "    console.print(f\"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(test_docs)}\")\n",
    "    for doc in test_docs:\n",
    "        console.print(f\"    ‚Ä¢ {doc.metadata['file_name']} ({doc.metadata['category']})\")\n",
    "    \n",
    "    # –®–∞–≥ 2: –ß–∞–Ω–∫–∏–Ω–≥\n",
    "    console.print(\"\\n[bold]–®–∞–≥ 2: –ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤[/bold]\")\n",
    "    chunker = Chunker()\n",
    "    chunks = chunker.chunk_documents(test_docs, chunk_size=50, overlap_percent=0.25)\n",
    "    console.print(f\"  –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\")\n",
    "    console.print(f\"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {sum(c.text_length for c in chunks) / len(chunks):.1f} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤\n",
    "    console.print(\"\\n  –ü—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤:\")\n",
    "    for i, chunk in enumerate(chunks[:3], 1):\n",
    "        console.print(f\"    [{i}] {chunk.chunk_id}\")\n",
    "        console.print(f\"        –¢–µ–∫—Å—Ç: {chunk.text[:60]}...\")\n",
    "        console.print(f\"        –†–∞–∑–º–µ—Ä: {chunk.text_length} —Ç–æ–∫–µ–Ω–æ–≤, –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {chunk.metadata['category']}\")\n",
    "    \n",
    "    # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings\n",
    "    console.print(\"\\n[bold]–®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings[/bold]\")\n",
    "    embedding_service = EmbeddingService(mock_mode=True, embedding_dim=1536)\n",
    "    embeddings = embedding_service.generate_embeddings([c.text for c in chunks])\n",
    "    console.print(f\"  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ embeddings: {len(embeddings)}\")\n",
    "    console.print(f\"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(embeddings[0])}\")\n",
    "    \n",
    "    # –®–∞–≥ 4: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant\n",
    "    console.print(\"\\n[bold]–®–∞–≥ 4: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant[/bold]\")\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω–µ—à–Ω–∏–π Qdrant, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "    try:\n",
    "        qdrant_url = QDRANT_URL if 'QDRANT_URL' in globals() and QDRANT_URL else None\n",
    "        qdrant_api_key = QDRANT_API_KEY if 'QDRANT_API_KEY' in globals() and QDRANT_API_KEY else None\n",
    "        \n",
    "        # –û—á–∏—â–∞–µ–º –æ—Ç –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–º–æ–≥—É—Ç –±—ã—Ç—å \\r\\n –≤ –∫–æ–Ω—Ü–µ –ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤)\n",
    "        if qdrant_url:\n",
    "            qdrant_url = qdrant_url.strip()\n",
    "        if qdrant_api_key:\n",
    "            qdrant_api_key = qdrant_api_key.strip()\n",
    "        \n",
    "        if qdrant_url:\n",
    "            if qdrant_api_key:\n",
    "                qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
    "            else:\n",
    "                qdrant_client = QdrantClient(url=qdrant_url)\n",
    "            console.print(f\"[cyan]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω–µ—à–Ω–∏–π Qdrant: {qdrant_url}[/cyan]\")\n",
    "        else:\n",
    "            qdrant_client = QdrantClient(\":memory:\")\n",
    "            console.print(\"[cyan]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è in-memory Qdrant[/cyan]\")\n",
    "    except Exception as e:\n",
    "        qdrant_client = QdrantClient(\":memory:\")\n",
    "        console.print(f\"[yellow]–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –≤–Ω–µ—à–Ω–µ–º—É Qdrant: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è in-memory[/yellow]\")\n",
    "    \n",
    "    indexer = QdrantIndexer(qdrant_client, collection_name=\"neuro_docs_demo\", embedding_dim=1536)\n",
    "    indexed_count = indexer.index_chunks(chunks, embeddings)\n",
    "    console.print(f\"  –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {indexed_count}\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º count –≤–º–µ—Å—Ç–æ get_collection –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏–π\n",
    "    try:\n",
    "        count_result = qdrant_client.count(\"neuro_docs_demo\")\n",
    "        points_count = count_result.count\n",
    "        console.print(f\"  –†–∞–∑–º–µ—Ä –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {points_count} —Ç–æ—á–µ–∫\")\n",
    "    except Exception as e:\n",
    "        # –ï—Å–ª–∏ count –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º get_collection\n",
    "        try:\n",
    "            collection_info = qdrant_client.get_collection(\"neuro_docs_demo\")\n",
    "            console.print(f\"  –†–∞–∑–º–µ—Ä –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_info.points_count} —Ç–æ—á–µ–∫\")\n",
    "        except Exception as e2:\n",
    "            # –ï—Å–ª–∏ –æ–±–∞ –º–µ—Ç–æ–¥–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
    "            console.print(f\"  [yellow]‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–æ—à–∏–±–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏–π)[/yellow]\")\n",
    "            console.print(f\"  [cyan]–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {indexed_count}[/cyan]\")\n",
    "    \n",
    "    console.print(\"\\n[bold green]‚úÖ Ingestion Pipeline –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ![/bold green]\")\n",
    "    \n",
    "    return qdrant_client, chunks\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é\n",
    "qdrant_demo, chunks_demo = run_ingestion_pipeline_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Retrieval Layer\n",
    "\n",
    "Semantic search –ø–æ Qdrant —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º K. –ù–∞–≥–ª—è–¥–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º retrieved –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å: Retriever\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    \"\"\"–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ retrieved —á–∞–Ω–∫–∞ –∏–∑ Qdrant.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    score: float\n",
    "    metadata: dict\n",
    "\n",
    "class Retriever:\n",
    "    \"\"\"\n",
    "    Retriever –¥–ª—è semantic search –ø–æ Qdrant.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - Semantic search –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ neuro_docs\n",
    "    - –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—è K (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ retrieved —á–∞–Ω–∫–æ–≤: 3, 5, 8)\n",
    "    - –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ scores\n",
    "    \n",
    "    –í—Ö–æ–¥: query (str), k (int)\n",
    "    –í—ã—Ö–æ–¥: List[RetrievedChunk] ‚Äî —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ score\n",
    "    \n",
    "    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:\n",
    "    - k: 3, 5, 8 (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 3 –¥–ª—è Precision@3)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_client: QdrantClient,\n",
    "        embedding_service: EmbeddingService,\n",
    "        collection_name: str = \"neuro_docs\"\n",
    "    ):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.embedding_service = embedding_service\n",
    "        self.collection_name = collection_name\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 3,\n",
    "        score_threshold: Optional[float] = None\n",
    "    ) -> List[RetrievedChunk]:\n",
    "        \"\"\"\n",
    "        –í—ã–ø–æ–ª–Ω—è–µ—Ç semantic search –ø–æ Qdrant.\n",
    "        \n",
    "        Args:\n",
    "            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ retrieved —á–∞–Ω–∫–æ–≤ (3, 5, 8)\n",
    "            score_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "        \"\"\"\n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞\n",
    "        try:\n",
    "            query_embedding = self.embedding_service.generate_embeddings([query])[0]\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding: {str(e)}\")\n",
    "        \n",
    "        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant\n",
    "        # –í –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö qdrant-client API –∏–∑–º–µ–Ω–∏–ª—Å—è\n",
    "        search_results = None\n",
    "        last_error = None\n",
    "        \n",
    "        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã API –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏ qdrant-client\n",
    "        # –í–∞—Ä–∏–∞–Ω—Ç 1: query_points —Å –ø—Ä—è–º–æ–π –ø–µ—Ä–µ–¥–∞—á–µ–π –≤–µ–∫—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ >= 1.8.0)\n",
    "        try:\n",
    "            query_result = self.qdrant_client.query_points(\n",
    "                collection_name=self.collection_name,\n",
    "                query=query_embedding,  # –ü—Ä—è–º–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –≤–µ–∫—Ç–æ—Ä–∞ –∫–∞–∫ —Å–ø–∏—Å–∫–∞\n",
    "                limit=k,\n",
    "                with_payload=True\n",
    "            )\n",
    "            # query_points –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º points\n",
    "            search_results = query_result.points if hasattr(query_result, 'points') else []\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            # –ü—Ä–æ–±—É–µ–º query_points —Å NamedVector\n",
    "            try:\n",
    "                from qdrant_client.models import NamedVector\n",
    "                query_result = self.qdrant_client.query_points(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query=NamedVector(\n",
    "                        vector=query_embedding\n",
    "                    ),\n",
    "                    limit=k,\n",
    "                    with_payload=True\n",
    "                )\n",
    "                search_results = query_result.points if hasattr(query_result, 'points') else []\n",
    "            except Exception as e2:\n",
    "                last_error = e2\n",
    "        \n",
    "        # –í–∞—Ä–∏–∞–Ω—Ç 2: search_points (–≤–µ—Ä—Å–∏–∏ 1.7.x)\n",
    "        if search_results is None:\n",
    "            try:\n",
    "                search_result = self.qdrant_client.search_points(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=query_embedding,\n",
    "                    limit=k,\n",
    "                    with_payload=True\n",
    "                )\n",
    "                search_results = search_result.points if hasattr(search_result, 'points') else []\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "        \n",
    "        # –í–∞—Ä–∏–∞–Ω—Ç 3: search (—Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ < 1.7.0)\n",
    "        if search_results is None:\n",
    "            try:\n",
    "                search_results = self.qdrant_client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=query_embedding,\n",
    "                    limit=k\n",
    "                )\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "        \n",
    "        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ\n",
    "        if search_results is None:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n",
    "            available_methods = [m for m in dir(self.qdrant_client) if ('search' in m.lower() or 'query' in m.lower()) and not m.startswith('_')]\n",
    "            client_type = type(self.qdrant_client).__name__\n",
    "            raise ValueError(\n",
    "                f\"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–∞–±–æ—á–∏–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ –≤ QdrantClient ({client_type}). \"\n",
    "                f\"–ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {str(last_error)}. \"\n",
    "                f\"–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å 'search' –∏–ª–∏ 'query': {available_methods}. \"\n",
    "                f\"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–µ—Ä—Å–∏—é qdrant-client: !pip show qdrant-client. \"\n",
    "                f\"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å: !pip install --upgrade qdrant-client\"\n",
    "            )\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ RetrievedChunk\n",
    "        retrieved_chunks = []\n",
    "        for result in search_results:\n",
    "            payload = result.payload\n",
    "            \n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º score_threshold, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω\n",
    "            if score_threshold is not None and result.score < score_threshold:\n",
    "                continue\n",
    "            \n",
    "            chunk = RetrievedChunk(\n",
    "                id=payload.get(\"chunk_id\", str(result.id)),\n",
    "                text=payload.get(\"text\", \"\"),\n",
    "                score=result.score,\n",
    "                metadata={\n",
    "                    \"doc_id\": payload.get(\"doc_id\"),\n",
    "                    \"category\": payload.get(\"category\", \"unknown\"),\n",
    "                    \"file_name\": payload.get(\"file_name\", \"\"),\n",
    "                    **{k: v for k, v in payload.items() if k not in [\"chunk_id\", \"doc_id\", \"text\", \"category\", \"file_name\"]}\n",
    "                }\n",
    "            )\n",
    "            retrieved_chunks.append(chunk)\n",
    "        \n",
    "        return retrieved_chunks\n",
    "\n",
    "print(\"‚úÖ Retriever —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Retrieval Layer\n",
    "\n",
    "def demonstrate_retrieval():\n",
    "    \"\"\"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è semantic search —Å –Ω–∞–≥–ª—è–¥–Ω—ã–º –≤—ã–≤–æ–¥–æ–º.\"\"\"\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Qdrant –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ pipeline\n",
    "    embedding_service = EmbeddingService(mock_mode=True)\n",
    "    retriever = Retriever(\n",
    "        qdrant_client=qdrant_demo,\n",
    "        embedding_service=embedding_service,\n",
    "        collection_name=\"neuro_docs_demo\"\n",
    "    )\n",
    "    \n",
    "    # –ó–∞–ø—Ä–æ—Å UC-1\n",
    "    query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "    \n",
    "    console.print(f\"\\n[bold cyan]=== Retrieval Layer ===[/bold cyan]\")\n",
    "    console.print(f\"[bold]–ó–∞–ø—Ä–æ—Å:[/bold] {query}\")\n",
    "    console.print(f\"[bold]K:[/bold] 3\")\n",
    "    \n",
    "    # –í—ã–ø–æ–ª–Ω—è–µ–º retrieval\n",
    "    retrieved_chunks = retriever.retrieve(query, k=3)\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã\n",
    "    table = Table(title=\"Retrieved Documents\", show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(\"‚Ññ\", style=\"dim\", width=3)\n",
    "    table.add_column(\"Score\", justify=\"right\", style=\"cyan\")\n",
    "    table.add_column(\"Source\", style=\"green\")\n",
    "    table.add_column(\"Chunk ID\", style=\"yellow\")\n",
    "    table.add_column(\"Text Preview\", style=\"white\", width=50)\n",
    "    \n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        table.add_row(\n",
    "            str(i),\n",
    "            f\"{chunk.score:.4f}\",\n",
    "            chunk.metadata.get(\"category\", \"unknown\"),\n",
    "            chunk.id[:20] + \"...\" if len(chunk.id) > 20 else chunk.id,\n",
    "            chunk.text[:60] + \"...\" if len(chunk.text) > 60 else chunk.text\n",
    "        )\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "    console.print(f\"\\n[cyan]–ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤:[/cyan] {len(retrieved_chunks)}\")\n",
    "    console.print(f\"[cyan]–°—Ä–µ–¥–Ω–∏–π score:[/cyan] {sum(c.score for c in retrieved_chunks) / len(retrieved_chunks):.4f}\")\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "retrieved_demo = demonstrate_retrieval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è Retriever\n",
    "\n",
    "def test_retriever():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è Retriever (pytest-style).\"\"\"\n",
    "    from unittest.mock import Mock, MagicMock\n",
    "    \n",
    "    # –°–æ–∑–¥–∞—ë–º mock Qdrant client\n",
    "    mock_qdrant = MagicMock()\n",
    "    # –°–æ–∑–¥–∞—ë–º mock —Ç–æ—á–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞\n",
    "    mock_points = [\n",
    "        Mock(\n",
    "            id=0,\n",
    "            score=0.95,\n",
    "            payload={\n",
    "                \"chunk_id\": \"chunk_001\",\n",
    "                \"text\": \"SLA —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 99.9%\",\n",
    "                \"doc_id\": \"doc_001\",\n",
    "                \"category\": \"it\"\n",
    "            }\n",
    "        ),\n",
    "        Mock(\n",
    "            id=1,\n",
    "            score=0.88,\n",
    "            payload={\n",
    "                \"chunk_id\": \"chunk_002\",\n",
    "                \"text\": \"–í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π –Ω–µ –±–æ–ª–µ–µ 200–º—Å\",\n",
    "                \"doc_id\": \"doc_001\",\n",
    "                \"category\": \"it\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º mock –¥–ª—è query_points (–Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏) - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º points\n",
    "    mock_query_result = Mock()\n",
    "    mock_query_result.points = mock_points\n",
    "    mock_qdrant.query_points.return_value = mock_query_result\n",
    "    \n",
    "    # –¢–∞–∫–∂–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –¥–ª—è search_points (–≤–µ—Ä—Å–∏–∏ 1.7.x)\n",
    "    mock_search_result = Mock()\n",
    "    mock_search_result.points = mock_points\n",
    "    mock_qdrant.search_points.return_value = mock_search_result\n",
    "    \n",
    "    # –ò –¥–ª—è search (—Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏) - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–ø—Ä—è–º—É—é\n",
    "    mock_qdrant.search.return_value = mock_points\n",
    "    \n",
    "    # –°–æ–∑–¥–∞—ë–º embedding service\n",
    "    embedding_service = EmbeddingService(mock_mode=True)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞—ë–º retriever\n",
    "    retriever = Retriever(\n",
    "        qdrant_client=mock_qdrant,\n",
    "        embedding_service=embedding_service,\n",
    "        collection_name=\"test_collection\"\n",
    "    )\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: retrieve –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ RetrievedChunk\n",
    "    query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "    results = retriever.retrieve(query, k=2)\n",
    "    \n",
    "    assert len(results) == 2, \"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 —á–∞–Ω–∫–∞\"\n",
    "    assert all(isinstance(chunk, RetrievedChunk) for chunk in results)\n",
    "    assert all(chunk.text is not None for chunk in results)\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: retrieve –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç RetrievedChunk[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: K –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n",
    "    # –û–±–Ω–æ–≤–ª—è–µ–º mock –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤\n",
    "    mock_query_result.points = mock_points[:1]  # –û–¥–∏–Ω —á–∞–Ω–∫\n",
    "    mock_search_result.points = mock_points[:1]\n",
    "    mock_qdrant.search.return_value = mock_points[:1]\n",
    "    results = retriever.retrieve(query, k=1)\n",
    "    assert len(results) == 1, \"–î–æ–ª–∂–µ–Ω –±—ã—Ç—å 1 —á–∞–Ω–∫ –ø—Ä–∏ k=1\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: K –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ score\n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±–∞ —á–∞–Ω–∫–∞ –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤\n",
    "    mock_query_result.points = mock_points\n",
    "    mock_search_result.points = mock_points\n",
    "    mock_qdrant.search.return_value = mock_points\n",
    "    results = retriever.retrieve(query, k=2)\n",
    "    scores = [chunk.score for chunk in results]\n",
    "    assert scores == sorted(scores, reverse=True), \"–ß–∞–Ω–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ score\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ score[/green]\")\n",
    "\n",
    "test_retriever()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã Retriever –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ Reranking (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ)\n",
    "\n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤: **–±–µ–∑ reranking** vs **—Å reranking**. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–ª–∏—è–Ω–∏–µ –Ω–∞ Precision@K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å: Reranker\n",
    "# ============================================\n",
    "\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class RerankedChunk:\n",
    "    \"\"\"–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ reranked —á–∞–Ω–∫–∞.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    score: float\n",
    "    rerank_score: float\n",
    "    metadata: dict\n",
    "\n",
    "class Reranker:\n",
    "    \"\"\"\n",
    "    Reranker –¥–ª—è –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è retrieved –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ retrieved —á–∞–Ω–∫–æ–≤ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –∑–∞–ø—Ä–æ—Å—É\n",
    "    - –ü–æ–≤—ã—à–µ–Ω–∏–µ Precision@3 –ø—Ä–∏ –±–æ–ª—å—à–æ–º –æ–±—ä—ë–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ semantic search score –∏ keyword-based reranking\n",
    "    \n",
    "    –í—Ö–æ–¥: query (str), chunks (List[RetrievedChunk]), top_k (int)\n",
    "    –í—ã—Ö–æ–¥: List[RerankedChunk] ‚Äî –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏\n",
    "    \n",
    "    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:\n",
    "    - keyword_weight: 0.3, original_score_weight: 0.7 (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, keyword_weight: float = 0.3, original_score_weight: float = 0.7):\n",
    "        self.keyword_weight = keyword_weight\n",
    "        self.original_score_weight = original_score_weight\n",
    "    \n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞.\"\"\"\n",
    "        stop_words = {\n",
    "            \"–∏\", \"–≤\", \"–Ω–∞\", \"—Å\", \"–ø–æ\", \"–¥–ª—è\", \"–æ—Ç\", \"–¥–æ\", \"–∏–∑\", \"–∫\", \"–æ\", \"–æ–±\",\n",
    "            \"–∞\", \"–Ω–æ\", \"–∏–ª–∏\", \"–∫–∞–∫\", \"—á—Ç–æ\", \"—ç—Ç–æ\", \"—Ç–∞–∫\", \"—É–∂–µ\", \"–µ—â–µ\", \"–µ—â—ë\"\n",
    "        }\n",
    "        text_lower = text.lower()\n",
    "        words = re.findall(r'\\b[–∞-—è—ëa-z]+\\b', text_lower)\n",
    "        keywords = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "        return keywords\n",
    "    \n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        chunks: List[RetrievedChunk],\n",
    "        top_k: Optional[int] = None\n",
    "    ) -> List[RerankedChunk]:\n",
    "        \"\"\"\n",
    "        –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç —á–∞–Ω–∫–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.\n",
    "        \n",
    "        Args:\n",
    "            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            chunks: –°–ø–∏—Å–æ–∫ retrieved —á–∞–Ω–∫–æ–≤\n",
    "            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ reranking (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è len(chunks))\n",
    "        \"\"\"\n",
    "        query_keywords = set(self._extract_keywords(query))\n",
    "        \n",
    "        reranked = []\n",
    "        for chunk in chunks:\n",
    "            chunk_keywords = set(self._extract_keywords(chunk.text))\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º keyword overlap\n",
    "            common_keywords = query_keywords.intersection(chunk_keywords)\n",
    "            keyword_score = len(common_keywords) / max(len(query_keywords), 1)\n",
    "            \n",
    "            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º scores\n",
    "            rerank_score = (\n",
    "                self.original_score_weight * chunk.score +\n",
    "                self.keyword_weight * keyword_score\n",
    "            )\n",
    "            \n",
    "            reranked_chunk = RerankedChunk(\n",
    "                id=chunk.id,\n",
    "                text=chunk.text,\n",
    "                score=chunk.score,\n",
    "                rerank_score=rerank_score,\n",
    "                metadata=chunk.metadata\n",
    "            )\n",
    "            reranked.append(reranked_chunk)\n",
    "        \n",
    "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ rerank_score\n",
    "        reranked.sort(key=lambda x: x.rerank_score, reverse=True)\n",
    "        \n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º top_k\n",
    "        if top_k is not None:\n",
    "            reranked = reranked[:top_k]\n",
    "        \n",
    "        return reranked\n",
    "\n",
    "print(\"‚úÖ Reranker —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è Reranker\n",
    "\n",
    "def test_reranker():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è Reranker.\"\"\"\n",
    "    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏\n",
    "    sample_chunks = [\n",
    "        RetrievedChunk(\n",
    "            id=\"chunk_1\",\n",
    "            text=\"SLA —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 99.9%. –í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ –Ω–µ –±–æ–ª–µ–µ 200–º—Å.\",\n",
    "            score=0.85,\n",
    "            metadata={\"doc_id\": \"doc_1\", \"category\": \"it\"}\n",
    "        ),\n",
    "        RetrievedChunk(\n",
    "            id=\"chunk_2\",\n",
    "            text=\"HR –ø–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏ –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –æ—Ç–ø—É—Å–∫–æ–≤ –∏ –±–æ–ª—å–Ω–∏—á–Ω—ã—Ö.\",\n",
    "            score=0.78,\n",
    "            metadata={\"doc_id\": \"doc_2\", \"category\": \"hr\"}\n",
    "        ),\n",
    "        RetrievedChunk(\n",
    "            id=\"chunk_3\",\n",
    "            text=\"–ü–ª–∞—Ç–µ–∂–Ω—ã–π —Å–µ—Ä–≤–∏—Å –∏–º–µ–µ—Ç SLA 99.9% –∏ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ 200–º—Å.\",\n",
    "            score=0.72,\n",
    "            metadata={\"doc_id\": \"doc_3\", \"category\": \"it\"}\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "    reranker = Reranker()\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: rerank –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç RerankedChunk\n",
    "    reranked = reranker.rerank(query=query, chunks=sample_chunks, top_k=3)\n",
    "    \n",
    "    assert len(reranked) == 3, \"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å 3 reranked —á–∞–Ω–∫–∞\"\n",
    "    assert all(hasattr(chunk, \"rerank_score\") for chunk in reranked)\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: rerank –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç RerankedChunk[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: rerank_score —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞\n",
    "    for chunk in reranked:\n",
    "        assert isinstance(chunk.rerank_score, float)\n",
    "        assert 0.0 <= chunk.rerank_score <= 1.0\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: rerank_score —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: top_k –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n",
    "    reranked_2 = reranker.rerank(query=query, chunks=sample_chunks, top_k=2)\n",
    "    assert len(reranked_2) == 2, \"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 —á–∞–Ω–∫–∞ –ø—Ä–∏ top_k=2\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: top_k –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 4: –ß–∞–Ω–∫–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ rerank_score\n",
    "    scores = [chunk.rerank_score for chunk in reranked]\n",
    "    assert scores == sorted(scores, reverse=True), \"–ß–∞–Ω–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ rerank_score\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 4 –ø—Ä–æ–π–¥–µ–Ω: –ß–∞–Ω–∫–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ rerank_score[/green]\")\n",
    "\n",
    "test_reranker()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã Reranker –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7Ô∏è‚É£ Agent Controller (FSM)\n",
    "\n",
    "**–ö–ª—é—á–µ–≤–æ–π –º–æ–¥—É–ª—å**: —è–≤–Ω–∞—è state machine —É–ø—Ä–∞–≤–ª—è–µ—Ç –ª–æ–≥–∏–∫–æ–π, –∞ –Ω–µ LLM.\n",
    "\n",
    "–ü–æ–∫–∞–∑—ã–≤–∞–µ–º:\n",
    "- –ü–µ—Ä–µ—Ö–æ–¥—ã —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
    "- –õ–æ–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "- –ü–æ—á–µ–º—É –ø—Ä–∏–Ω—è—Ç–æ —Ç–æ –∏–ª–∏ –∏–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å: Agent State Machine\n",
    "# ============================================\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class AgentState(Enum):\n",
    "    \"\"\"–°–æ—Å—Ç–æ—è–Ω–∏—è –∞–≥–µ–Ω—Ç–∞.\"\"\"\n",
    "    IDLE = \"IDLE\"\n",
    "    VALIDATE_QUERY = \"VALIDATE_QUERY\"\n",
    "    REQUEST_CLARIFICATION = \"REQUEST_CLARIFICATION\"\n",
    "    RETRIEVE = \"RETRIEVE\"\n",
    "    METADATA_FILTER = \"METADATA_FILTER\"\n",
    "    RERANK = \"RERANK\"\n",
    "    GENERATE = \"GENERATE\"\n",
    "    VALIDATE_ANSWER = \"VALIDATE_ANSWER\"\n",
    "    LOG_METRICS = \"LOG_METRICS\"\n",
    "    RETURN_RESPONSE = \"RETURN_RESPONSE\"\n",
    "\n",
    "class AgentStateMachine:\n",
    "    \"\"\"\n",
    "    –ö–æ–Ω–µ—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–∞.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏ –∞–≥–µ–Ω—Ç–∞\n",
    "    - –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏\n",
    "    - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
    "    \n",
    "    –í—Ö–æ–¥: new_state (AgentState)\n",
    "    –í—ã—Ö–æ–¥: –ø–µ—Ä–µ—Ö–æ–¥ –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_state = AgentState.IDLE\n",
    "        self.state_history: List[AgentState] = [AgentState.IDLE]\n",
    "    \n",
    "    def transition_to(self, new_state: AgentState) -> None:\n",
    "        \"\"\"–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.\"\"\"\n",
    "        self.current_state = new_state\n",
    "        self.state_history.append(new_state)\n",
    "    \n",
    "    def get_history(self) -> List[AgentState]:\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–æ—Å—Ç–æ—è–Ω–∏–π.\"\"\"\n",
    "        return self.state_history.copy()\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç state machine –≤ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ.\"\"\"\n",
    "        self.current_state = AgentState.IDLE\n",
    "        self.state_history = [AgentState.IDLE]\n",
    "\n",
    "class DecisionLog:\n",
    "    \"\"\"\n",
    "    –õ–æ–≥ —Ä–µ—à–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –∞–≥–µ–Ω—Ç–∞\n",
    "    - –§–∏–∫—Å–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    - –ü–æ–Ω–∏–º–∞–Ω–∏–µ, –ø–æ—á–µ–º—É –ø—Ä–∏–Ω—è—Ç–æ —Ç–æ –∏–ª–∏ –∏–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logs: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def log_decision(\n",
    "        self,\n",
    "        state: str,\n",
    "        action: str,\n",
    "        input_data: Any,\n",
    "        output_data: Any,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ):\n",
    "        \"\"\"–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–µ—à–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞.\"\"\"\n",
    "        log_entry = {\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"input\": str(input_data),\n",
    "            \"output\": str(output_data),\n",
    "            \"metadata\": metadata or {}\n",
    "        }\n",
    "        self.logs.append(log_entry)\n",
    "    \n",
    "    def get_logs(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –ª–æ–≥–∏.\"\"\"\n",
    "        return self.logs.copy()\n",
    "\n",
    "print(\"‚úÖ Agent State Machine —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è AgentStateMachine\n",
    "\n",
    "def test_agent_state_machine():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è AgentStateMachine.\"\"\"\n",
    "    state_machine = AgentStateMachine()\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ IDLE\n",
    "    assert state_machine.current_state == AgentState.IDLE, \"–ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å IDLE\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ IDLE[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏\n",
    "    state_machine.transition_to(AgentState.VALIDATE_QUERY)\n",
    "    assert state_machine.current_state == AgentState.VALIDATE_QUERY\n",
    "    \n",
    "    state_machine.transition_to(AgentState.RETRIEVE)\n",
    "    assert state_machine.current_state == AgentState.RETRIEVE\n",
    "    \n",
    "    state_machine.transition_to(AgentState.GENERATE)\n",
    "    assert state_machine.current_state == AgentState.GENERATE\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏ —Ä–∞–±–æ—Ç–∞—é—Ç[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è\n",
    "    history = state_machine.get_history()\n",
    "    assert len(history) >= 3, \"–ò—Å—Ç–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 3 —Å–æ—Å—Ç–æ—è–Ω–∏—è\"\n",
    "    assert AgentState.IDLE in history, \"–ò—Å—Ç–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å IDLE\"\n",
    "    assert AgentState.VALIDATE_QUERY in history, \"–ò—Å—Ç–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å VALIDATE_QUERY\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 4: –ü–æ–ª–Ω—ã–π flow UC-1\n",
    "    state_machine.reset()\n",
    "    states = [\n",
    "        AgentState.VALIDATE_QUERY,\n",
    "        AgentState.RETRIEVE,\n",
    "        AgentState.GENERATE,\n",
    "        AgentState.VALIDATE_ANSWER,\n",
    "        AgentState.LOG_METRICS,\n",
    "        AgentState.RETURN_RESPONSE,\n",
    "        AgentState.IDLE\n",
    "    ]\n",
    "    for state in states:\n",
    "        state_machine.transition_to(state)\n",
    "    assert state_machine.current_state == AgentState.IDLE, \"–§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å IDLE\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 4 –ø—Ä–æ–π–¥–µ–Ω: –ü–æ–ª–Ω—ã–π flow UC-1 —Ä–∞–±–æ—Ç–∞–µ—Ç[/green]\")\n",
    "\n",
    "test_agent_state_machine()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã AgentStateMachine –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Agent Controller (FSM)\n",
    "\n",
    "def demonstrate_agent_fsm():\n",
    "    \"\"\"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Agent Controller —Å FSM.\"\"\"\n",
    "    state_machine = AgentStateMachine()\n",
    "    decision_log = DecisionLog()\n",
    "    \n",
    "    query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "    \n",
    "    console.print(f\"\\n[bold cyan]=== Agent Controller (FSM) ===[/bold cyan]\")\n",
    "    console.print(f\"[bold]–ó–∞–ø—Ä–æ—Å:[/bold] {query}\\n\")\n",
    "    \n",
    "    # IDLE ‚Üí VALIDATE_QUERY\n",
    "    state_machine.transition_to(AgentState.VALIDATE_QUERY)\n",
    "    decision_log.log_decision(\n",
    "        state=state_machine.current_state.value,\n",
    "        action=\"validate_query\",\n",
    "        input_data=query,\n",
    "        output_data=\"valid\",\n",
    "        metadata={}\n",
    "    )\n",
    "    console.print(f\"[green]‚Üí {state_machine.current_state.value}[/green]: –ó–∞–ø—Ä–æ—Å –≤–∞–ª–∏–¥–µ–Ω\")\n",
    "    \n",
    "    # VALIDATE_QUERY ‚Üí RETRIEVE\n",
    "    state_machine.transition_to(AgentState.RETRIEVE)\n",
    "    decision_log.log_decision(\n",
    "        state=state_machine.current_state.value,\n",
    "        action=\"retrieve_chunks\",\n",
    "        input_data=query,\n",
    "        output_data=\"3 chunks retrieved\",\n",
    "        metadata={\"k\": 3}\n",
    "    )\n",
    "    console.print(f\"[green]‚Üí {state_machine.current_state.value}[/green]: –ù–∞–π–¥–µ–Ω–æ 3 —á–∞–Ω–∫–∞\")\n",
    "    \n",
    "    # RETRIEVE ‚Üí RERANK (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    use_reranking = True\n",
    "    if use_reranking:\n",
    "        state_machine.transition_to(AgentState.RERANK)\n",
    "        decision_log.log_decision(\n",
    "            state=state_machine.current_state.value,\n",
    "            action=\"rerank_chunks\",\n",
    "            input_data=\"3 chunks before rerank\",\n",
    "            output_data=\"3 chunks after rerank\",\n",
    "            metadata={\"rerank_top_k\": 3}\n",
    "        )\n",
    "        console.print(f\"[green]‚Üí {state_machine.current_state.value}[/green]: Reranking –ø—Ä–∏–º–µ–Ω—ë–Ω\")\n",
    "    \n",
    "    # RERANK ‚Üí GENERATE\n",
    "    state_machine.transition_to(AgentState.GENERATE)\n",
    "    decision_log.log_decision(\n",
    "        state=state_machine.current_state.value,\n",
    "        action=\"generate_answer\",\n",
    "        input_data=\"prompt built\",\n",
    "        output_data=\"answer generated\",\n",
    "        metadata={}\n",
    "    )\n",
    "    console.print(f\"[green]‚Üí {state_machine.current_state.value}[/green]: –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω\")\n",
    "    \n",
    "    # GENERATE ‚Üí VALIDATE_ANSWER\n",
    "    state_machine.transition_to(AgentState.VALIDATE_ANSWER)\n",
    "    decision_log.log_decision(\n",
    "        state=state_machine.current_state.value,\n",
    "        action=\"validate_answer\",\n",
    "        input_data=\"answer\",\n",
    "        output_data=\"answer validated\",\n",
    "        metadata={}\n",
    "    )\n",
    "    console.print(f\"[green]‚Üí {state_machine.current_state.value}[/green]: –û—Ç–≤–µ—Ç –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω\")\n",
    "    \n",
    "    # VALIDATE_ANSWER ‚Üí LOG_METRICS\n",
    "    state_machine.transition_to(AgentState.LOG_METRICS)\n",
    "    decision_log.log_decision(\n",
    "        state=state_machine.current_state.value,\n",
    "        action=\"log_metrics\",\n",
    "        input_data=\"metrics calculated\",\n",
    "        output_data=\"metrics logged\",\n",
    "        metadata={\"precision_at_3\": 0.85, \"faithfulness\": 0.90}\n",
    "    )\n",
    "    console.print(f\"[green]‚Üí {state_machine.current_state.value}[/green]: –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã\")\n",
    "    \n",
    "    # LOG_METRICS ‚Üí RETURN_RESPONSE\n",
    "    state_machine.transition_to(AgentState.RETURN_RESPONSE)\n",
    "    console.print(f\"[green]‚Üí {state_machine.current_state.value}[/green]: –û—Ç–≤–µ—Ç –≤–æ–∑–≤—Ä–∞—â—ë–Ω\")\n",
    "    \n",
    "    # RETURN_RESPONSE ‚Üí IDLE\n",
    "    state_machine.transition_to(AgentState.IDLE)\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
    "    console.print(f\"\\n[bold yellow]–ò—Å—Ç–æ—Ä–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π:[/bold yellow]\")\n",
    "    history = state_machine.get_history()\n",
    "    for i, state in enumerate(history, 1):\n",
    "        console.print(f\"  {i}. {state.value}\")\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º Decision Log\n",
    "    console.print(f\"\\n[bold yellow]Decision Log:[/bold yellow]\")\n",
    "    logs = decision_log.get_logs()\n",
    "    for i, log in enumerate(logs, 1):\n",
    "        console.print(f\"\\n  [{i}] {log['state']} - {log['action']}\")\n",
    "        console.print(f\"      Input: {log['input'][:50]}...\")\n",
    "        console.print(f\"      Output: {log['output']}\")\n",
    "        if log['metadata']:\n",
    "            console.print(f\"      Metadata: {log['metadata']}\")\n",
    "    \n",
    "    return state_machine, decision_log\n",
    "\n",
    "state_machine_demo, decision_log_demo = demonstrate_agent_fsm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8Ô∏è‚É£ Generation Layer\n",
    "\n",
    "**PromptBuilder** —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç prompt —Å–æ —Å—Ç—Ä–æ–≥–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π: ¬´–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞¬ª.\n",
    "\n",
    "–ü–æ–∫–∞–∑—ã–≤–∞–µ–º:\n",
    "- Prompt\n",
    "- Context\n",
    "- Raw response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å: PromptBuilder\n",
    "# ============================================\n",
    "\n",
    "class PromptBuilder:\n",
    "    \"\"\"\n",
    "    –ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å prompt –¥–ª—è LLM.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ prompt —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ retrieved —á–∞–Ω–∫–æ–≤\n",
    "    - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–≥–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ¬´–æ—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É¬ª\n",
    "    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ prompt –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã LLM\n",
    "    \n",
    "    –í—Ö–æ–¥: query (str), retrieved_chunks (List[RetrievedChunk])\n",
    "    –í—ã—Ö–æ–¥: prompt (str) ‚Äî —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π prompt\n",
    "    \n",
    "    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:\n",
    "    - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—Ç—Ä–æ–≥–æ–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, instruction_template: str = None):\n",
    "        self.instruction_template = instruction_template or self._default_instruction()\n",
    "    \n",
    "    def _default_instruction(self) -> str:\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.\"\"\"\n",
    "        return (\n",
    "            \"–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å—Ç—Ä–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\\n\"\n",
    "            \"–í–ê–ñ–ù–û:\\n\"\n",
    "            \"- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\\n\"\n",
    "            \"- –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ\\n\"\n",
    "            \"- –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ñ–∞–∫—Ç—ã, –Ω–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ\\n\"\n",
    "            \"- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º\\n\"\n",
    "            \"- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ—Å–ª–æ–≤–Ω–æ –∏–ª–∏ –±–ª–∏–∑–∫–æ –∫ —Ç–µ–∫—Å—Ç—É\\n\"\n",
    "        )\n",
    "    \n",
    "    def build_prompt(\n",
    "        self,\n",
    "        query: str,\n",
    "        retrieved_chunks: List[RetrievedChunk]\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        –§–æ—Ä–º–∏—Ä—É–µ—Ç prompt —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∑–∞–ø—Ä–æ—Å–æ–º.\n",
    "        \n",
    "        Args:\n",
    "            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            retrieved_chunks: –°–ø–∏—Å–æ–∫ retrieved —á–∞–Ω–∫–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º\n",
    "        \"\"\"\n",
    "        if not retrieved_chunks:\n",
    "            return (\n",
    "                f\"{self.instruction_template}\\n\\n\"\n",
    "                f\"–ö–æ–Ω—Ç–µ–∫—Å—Ç: –í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\\n\\n\"\n",
    "                f\"–í–æ–ø—Ä–æ—Å: {query}\\n\\n\"\n",
    "                f\"–û—Ç–≤–µ—Ç: –í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.\"\n",
    "            )\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö retrieved —á–∞–Ω–∫–æ–≤\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(retrieved_chunks, start=1):\n",
    "            context_parts.append(f\"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}]\\n{chunk.text}\\n\")\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π prompt\n",
    "        prompt = (\n",
    "            f\"{self.instruction_template}\\n\\n\"\n",
    "            f\"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:\\n{context}\\n\\n\"\n",
    "            f\"–í–æ–ø—Ä–æ—Å: {query}\\n\\n\"\n",
    "            f\"–û—Ç–≤–µ—Ç (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞):\"\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"\n",
    "    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ GigaChat API –∏ mock —Ä–µ–∂–∏–º).\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –í—ã–∑–æ–≤ LLM API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤\n",
    "    - –í production: GigaChat API\n",
    "    - –í Colab: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mock-—Ä–µ–∂–∏–º –∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π API (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–ª—é—á–µ–π)\n",
    "    \n",
    "    –í—Ö–æ–¥: prompt (str)\n",
    "    –í—ã—Ö–æ–¥: answer (str) ‚Äî —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mock_mode: bool = True,\n",
    "        model: str = \"GigaChat\",\n",
    "        temperature: float = 0.1,\n",
    "        max_tokens: int = 500,\n",
    "        auth_key: Optional[str] = None,\n",
    "        scope: Optional[str] = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º auth_key (–∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Colab –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞)\n",
    "        if not auth_key:\n",
    "            try:\n",
    "                auth_key = userdata.get(\"GIGACHAT_AUTH_KEY\")\n",
    "            except:\n",
    "                auth_key = None\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º scope (–∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤ Colab –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞)\n",
    "        if not scope:\n",
    "            try:\n",
    "                scope = userdata.get(\"GIGACHAT_SCOPE\", \"GIGACHAT_API_PERS\")\n",
    "            except:\n",
    "                scope = \"GIGACHAT_API_PERS\"\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º mock mode\n",
    "        if mock_mode or not auth_key:\n",
    "            self.mock_mode = True\n",
    "        else:\n",
    "            self.mock_mode = False\n",
    "            self.auth = GigaChatAuth(auth_key=auth_key, scope=scope)\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP —Å–µ—Å—Å–∏–∏\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"POST\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.verify = False\n",
    "        \n",
    "        # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π endpoint –¥–ª—è GigaChat Chat Completions API\n",
    "        self.api_url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\"\n",
    "    \n",
    "    def generate_answer(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ prompt.\n",
    "        \n",
    "        –í mock-—Ä–µ–∂–∏–º–µ: –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n",
    "        –í production: –≤—ã–∑—ã–≤–∞–µ—Ç GigaChat API.\n",
    "        \"\"\"\n",
    "        if self.mock_mode:\n",
    "            # Mock: –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "            # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç LLM\n",
    "            if \"SLA\" in prompt and \"–ø–ª–∞—Ç–µ–∂\" in prompt.lower():\n",
    "                return \"SLA —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π: –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å 99.9%, –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–µ –±–æ–ª–µ–µ 200–º—Å. –°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç 24/7.\"\n",
    "            elif \"SLA\" in prompt and \"API\" in prompt:\n",
    "                return \"SLA —Å–µ—Ä–≤–∏—Å–∞ API: –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å 99.95%, –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–µ –±–æ–ª–µ–µ 100–º—Å. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ rate limiting.\"\n",
    "            else:\n",
    "                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "                lines = prompt.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if line.strip() and not line.startswith(\"[\") and not line.startswith(\"–¢—ã\"):\n",
    "                        return line.strip()\n",
    "                return \"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.\"\n",
    "        else:\n",
    "            # Production: –≤—ã–∑–æ–≤ GigaChat API\n",
    "            try:\n",
    "                response_data = self._call_gigachat_api(prompt)\n",
    "                answer = self._extract_answer(response_data)\n",
    "                \n",
    "                if not answer or not answer.strip():\n",
    "                    raise ValueError(\"Empty answer received from GigaChat API\")\n",
    "                \n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                # –ü—Ä–∏ –æ—à–∏–±–∫–µ API –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ mock mode\n",
    "                console.print(f\"[yellow]‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ GigaChat API: {e}[/yellow]\")\n",
    "                console.print(\"[yellow]   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è mock —Ä–µ–∂–∏–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏[/yellow]\")\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π mock fallback\n",
    "                lines = prompt.split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if line.strip() and not line.startswith(\"[\") and not line.startswith(\"–¢—ã\") and not line.startswith(\"–ö–æ–Ω—Ç–µ–∫—Å—Ç\"):\n",
    "                        return line.strip()\n",
    "                return \"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.\"\n",
    "    \n",
    "    def _call_gigachat_api(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        –í—ã–∑—ã–≤–∞–µ—Ç GigaChat API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Prompt –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "            \n",
    "        Returns:\n",
    "            –û—Ç–≤–µ—Ç –æ—Ç API –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è\n",
    "        \"\"\"\n",
    "        # –ü–æ–ª—É—á–∞–µ–º access token —á–µ—Ä–µ–∑ OAuth 2.0\n",
    "        if not self.auth:\n",
    "            raise ValueError(\"GigaChatAuth –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.\")\n",
    "        \n",
    "        access_token = self.auth.get_access_token()\n",
    "        if not access_token:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å access token –¥–ª—è GigaChat API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ GIGACHAT_AUTH_KEY –∏ GIGACHAT_SCOPE.\")\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ (UUID4)\n",
    "        request_id = str(uuid.uuid4())\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {access_token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Request-ID\": request_id\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens\n",
    "        }\n",
    "        \n",
    "        response = self.session.post(\n",
    "            self.api_url,\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=60  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º timeout –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    def _extract_answer(self, response_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–∞ API.\n",
    "        \n",
    "        Args:\n",
    "            response_data: –û—Ç–≤–µ—Ç –æ—Ç GigaChat API\n",
    "            \n",
    "        Returns:\n",
    "            –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞\n",
    "        \"\"\"\n",
    "        # –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ GigaChat API:\n",
    "        # {\n",
    "        #   \"choices\": [\n",
    "        #     {\n",
    "        #       \"message\": {\n",
    "        #         \"content\": \"—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞\"\n",
    "        #       }\n",
    "        #     }\n",
    "        #   ],\n",
    "        #   \"usage\": {...}\n",
    "        # }\n",
    "        \n",
    "        if \"choices\" in response_data and len(response_data[\"choices\"]) > 0:\n",
    "            first_choice = response_data[\"choices\"][0]\n",
    "            if \"message\" in first_choice and \"content\" in first_choice[\"message\"]:\n",
    "                return first_choice[\"message\"][\"content\"]\n",
    "        \n",
    "        raise ValueError(f\"Unexpected API response format: {response_data}\")\n",
    "\n",
    "print(\"‚úÖ Generation Layer —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ GigaChat API –∏ mock —Ä–µ–∂–∏–º)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ—Å—Ç—ã –¥–ª—è Retriever\n",
    "\n",
    "def test_retriever():\n",
    "    \"\"\"–¢–µ—Å—Ç—ã –¥–ª—è Retriever (pytest-style).\"\"\"\n",
    "    from unittest.mock import Mock, MagicMock\n",
    "    \n",
    "    # –°–æ–∑–¥–∞—ë–º mock Qdrant client\n",
    "    mock_qdrant = MagicMock()\n",
    "    # –°–æ–∑–¥–∞—ë–º mock —Ç–æ—á–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞\n",
    "    mock_points = [\n",
    "        Mock(\n",
    "            id=0,\n",
    "            score=0.95,\n",
    "            payload={\n",
    "                \"chunk_id\": \"chunk_001\",\n",
    "                \"text\": \"SLA —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 99.9%\",\n",
    "                \"doc_id\": \"doc_001\",\n",
    "                \"category\": \"it\"\n",
    "            }\n",
    "        ),\n",
    "        Mock(\n",
    "            id=1,\n",
    "            score=0.88,\n",
    "            payload={\n",
    "                \"chunk_id\": \"chunk_002\",\n",
    "                \"text\": \"–í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π –Ω–µ –±–æ–ª–µ–µ 200–º—Å\",\n",
    "                \"doc_id\": \"doc_001\",\n",
    "                \"category\": \"it\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º mock –¥–ª—è query_points (–Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏) - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º points\n",
    "    mock_query_result = Mock()\n",
    "    mock_query_result.points = mock_points\n",
    "    mock_qdrant.query_points.return_value = mock_query_result\n",
    "    \n",
    "    # –¢–∞–∫–∂–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –¥–ª—è search_points (–≤–µ—Ä—Å–∏–∏ 1.7.x)\n",
    "    mock_search_result = Mock()\n",
    "    mock_search_result.points = mock_points\n",
    "    mock_qdrant.search_points.return_value = mock_search_result\n",
    "    \n",
    "    # –ò –¥–ª—è search (—Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏) - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞–ø—Ä—è–º—É—é\n",
    "    mock_qdrant.search.return_value = mock_points\n",
    "    \n",
    "    # –°–æ–∑–¥–∞—ë–º embedding service\n",
    "    embedding_service = EmbeddingService(mock_mode=True)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞—ë–º retriever\n",
    "    retriever = Retriever(\n",
    "        qdrant_client=mock_qdrant,\n",
    "        embedding_service=embedding_service,\n",
    "        collection_name=\"test_collection\"\n",
    "    )\n",
    "    \n",
    "    # –¢–µ—Å—Ç 1: retrieve –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ RetrievedChunk\n",
    "    query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "    results = retriever.retrieve(query, k=2)\n",
    "    \n",
    "    assert len(results) == 2, \"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 —á–∞–Ω–∫–∞\"\n",
    "    assert all(isinstance(chunk, RetrievedChunk) for chunk in results)\n",
    "    assert all(chunk.text is not None for chunk in results)\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 1 –ø—Ä–æ–π–¥–µ–Ω: retrieve –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç RetrievedChunk[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 2: K –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n",
    "    # –û–±–Ω–æ–≤–ª—è–µ–º mock –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤\n",
    "    mock_query_result.points = mock_points[:1]  # –û–¥–∏–Ω —á–∞–Ω–∫\n",
    "    mock_search_result.points = mock_points[:1]\n",
    "    mock_qdrant.search.return_value = mock_points[:1]\n",
    "    results = retriever.retrieve(query, k=1)\n",
    "    assert len(results) == 1, \"–î–æ–ª–∂–µ–Ω –±—ã—Ç—å 1 —á–∞–Ω–∫ –ø—Ä–∏ k=1\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 2 –ø—Ä–æ–π–¥–µ–Ω: K –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç[/green]\")\n",
    "    \n",
    "    # –¢–µ—Å—Ç 3: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ score\n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±–∞ —á–∞–Ω–∫–∞ –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤\n",
    "    mock_query_result.points = mock_points\n",
    "    mock_search_result.points = mock_points\n",
    "    mock_qdrant.search.return_value = mock_points\n",
    "    results = retriever.retrieve(query, k=2)\n",
    "    scores = [chunk.score for chunk in results]\n",
    "    assert scores == sorted(scores, reverse=True), \"–ß–∞–Ω–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ score\"\n",
    "    console.print(\"[green]‚úÖ –¢–µ—Å—Ç 3 –ø—Ä–æ–π–¥–µ–Ω: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ score[/green]\")\n",
    "\n",
    "test_retriever()\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã Retriever –ø—Ä–æ–π–¥–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Generation Layer\n",
    "\n",
    "def demonstrate_generation():\n",
    "    \"\"\"–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–∫–∞–∑–æ–º prompt –∏ context.\"\"\"\n",
    "    query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "    \n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º retrieved chunks –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞\n",
    "    embedding_service = EmbeddingService(mock_mode=True)\n",
    "    retriever = Retriever(\n",
    "        qdrant_client=qdrant_demo,\n",
    "        embedding_service=embedding_service,\n",
    "        collection_name=\"neuro_docs_demo\"\n",
    "    )\n",
    "    retrieved_chunks = retriever.retrieve(query, k=3)\n",
    "    \n",
    "    # –§–æ—Ä–º–∏—Ä—É–µ–º prompt\n",
    "    prompt_builder = PromptBuilder()\n",
    "    prompt = prompt_builder.build_prompt(query, retrieved_chunks)\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç\n",
    "    llm_client = LLMClient(mock_mode=True)\n",
    "    answer = llm_client.generate_answer(prompt)\n",
    "    \n",
    "    console.print(f\"\\n[bold cyan]=== Generation Layer ===[/bold cyan]\")\n",
    "    console.print(f\"[bold]–ó–∞–ø—Ä–æ—Å:[/bold] {query}\\n\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º prompt\n",
    "    console.print(\"[bold yellow]Prompt (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è + –∫–æ–Ω—Ç–µ–∫—Å—Ç + –≤–æ–ø—Ä–æ—Å):[/bold yellow]\")\n",
    "    console.print(Panel(prompt, title=\"Full Prompt\", border_style=\"cyan\"))\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç–¥–µ–ª—å–Ω–æ\n",
    "    console.print(\"\\n[bold yellow]Context (retrieved chunks):[/bold yellow]\")\n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        console.print(f\"\\n  [–ò—Å—Ç–æ—á–Ω–∏–∫ {i}] (score: {chunk.score:.4f})\")\n",
    "        console.print(f\"  {chunk.text}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç\n",
    "    console.print(\"\\n[bold yellow]Raw Response:[/bold yellow]\")\n",
    "    console.print(Panel(answer, title=\"Generated Answer\", border_style=\"green\"))\n",
    "    \n",
    "    return prompt, answer\n",
    "\n",
    "prompt_demo, answer_demo = demonstrate_generation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9Ô∏è‚É£ Evaluation & Metrics (–∫–ª—é—á–µ–≤–æ–π –±–ª–æ–∫)\n",
    "\n",
    "**–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ñ–æ–∫—É—Å**: Precision@K, –≤–ª–∏—è–Ω–∏–µ chunk_size, overlap, K, reranking.\n",
    "\n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü + –∫—Ä–∞—Ç–∫–∏–µ –≤—ã–≤–æ–¥—ã.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å: RAGAS Adapters (LangChain-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ)\n",
    "# ============================================\n",
    "\n",
    "# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç LangChain\n",
    "try:\n",
    "    from langchain_core.language_models.llms import LLM\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "    from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "    from pydantic import PrivateAttr\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "    # –°–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Ç–∏–ø–æ–≤\n",
    "    LLM = object\n",
    "    Embeddings = object\n",
    "    CallbackManagerForLLMRun = Optional\n",
    "    PrivateAttr = lambda x: None\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ LLMClient –∏ EmbeddingService —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n",
    "if 'LLMClient' not in globals():\n",
    "    raise NameError(\"–ö–ª–∞—Å—Å 'LLMClient' –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫—É —Å LLMClient.\")\n",
    "if 'EmbeddingService' not in globals():\n",
    "    raise NameError(\"–ö–ª–∞—Å—Å 'EmbeddingService' –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫—É —Å EmbeddingService.\")\n",
    "\n",
    "if LANGCHAIN_AVAILABLE:\n",
    "    class GigaChatLLMAdapter(LLM):\n",
    "        \"\"\"\n",
    "        LangChain-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è GigaChat LLMClient.\n",
    "        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è RAGAS –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(\n",
    "            self,\n",
    "            llm_client: LLMClient,\n",
    "            **kwargs\n",
    "        ):\n",
    "            super().__init__(**kwargs)\n",
    "            self._llm_client = llm_client\n",
    "        \n",
    "        @property\n",
    "        def _llm_type(self) -> str:\n",
    "            return \"gigachat\"\n",
    "        \n",
    "        def _call(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "            **kwargs: Any,\n",
    "        ) -> str:\n",
    "            return self._llm_client.generate_answer(prompt)\n",
    "        \n",
    "        @property\n",
    "        def _identifying_params(self) -> dict:\n",
    "            return {\n",
    "                \"model\": self._llm_client.model,\n",
    "                \"temperature\": self._llm_client.temperature,\n",
    "                \"max_tokens\": self._llm_client.max_tokens,\n",
    "            }\n",
    "\n",
    "\n",
    "if LANGCHAIN_AVAILABLE:\n",
    "    class GigaChatEmbeddingsAdapter(Embeddings):\n",
    "        \"\"\"\n",
    "        LangChain-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è GigaChat EmbeddingService.\n",
    "        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è RAGAS –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–µ—Ç—Ä–∏–∫.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(\n",
    "            self,\n",
    "            embedding_service: EmbeddingService,\n",
    "            **kwargs\n",
    "        ):\n",
    "            super().__init__(**kwargs)\n",
    "            self._embedding_service = embedding_service\n",
    "        \n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            return self._embedding_service.generate_embeddings(texts)\n",
    "        \n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            embeddings = self._embedding_service.generate_embeddings([text])\n",
    "            return embeddings[0] if embeddings else []\n",
    "\n",
    "# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ñ–ª–∞–≥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "RAGAS_ADAPTERS_AVAILABLE = LANGCHAIN_AVAILABLE\n",
    "\n",
    "if RAGAS_ADAPTERS_AVAILABLE:\n",
    "    print(\"‚úÖ RAGAS LangChain –∞–¥–∞–ø—Ç–µ—Ä—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã\")\n",
    "else:\n",
    "    print(\"[yellow]‚ö†Ô∏è  LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. RAGAS LangChain –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.[/yellow]\")\n",
    "    print(\"[cyan]üí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ RAGAS —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install langchain-core langchain-community ragas datasets[/cyan]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ú–æ–¥—É–ª—å: MetricsCollector\n",
    "# ============================================\n",
    "\n",
    "class MetricsCollector:\n",
    "    \"\"\"\n",
    "    –°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏—Å—Ç–µ–º—ã.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –†–∞—Å—á—ë—Ç Precision@K –¥–ª—è retrieved —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    - –°–±–æ—Ä latency –º–µ—Ç—Ä–∏–∫ (retrieval, generation, end-to-end)\n",
    "    - –°–±–æ—Ä throughput –º–µ—Ç—Ä–∏–∫ (QPS)\n",
    "    \n",
    "    –í—Ö–æ–¥: retrieved_chunks, ground_truth_relevant, k\n",
    "    –í—ã—Ö–æ–¥: precision_at_k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calculate_precision_at_k(\n",
    "        self,\n",
    "        retrieved_chunks: List[RetrievedChunk],\n",
    "        ground_truth_relevant: List[str],\n",
    "        k: int\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Precision@K.\n",
    "        \n",
    "        Precision@K = (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ —Ç–æ–ø-K) / K\n",
    "        \"\"\"\n",
    "        if not retrieved_chunks or not ground_truth_relevant:\n",
    "            return 0.0\n",
    "        \n",
    "        # –ë–µ—Ä—ë–º —Ç–æ–ø-K —á–∞–Ω–∫–æ–≤\n",
    "        top_k_chunks = retrieved_chunks[:k]\n",
    "        \n",
    "        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ —Ç–æ–ø-K\n",
    "        relevant_count = sum(\n",
    "            1 for chunk in top_k_chunks\n",
    "            if chunk.id in ground_truth_relevant\n",
    "        )\n",
    "        \n",
    "        # Precision@K = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö / K\n",
    "        precision = relevant_count / len(top_k_chunks)\n",
    "        \n",
    "        return precision\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ RAGAS (–Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥—É–ª—è)\n",
    "try:\n",
    "    from ragas import evaluate\n",
    "    from datasets import Dataset\n",
    "    # –í –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö RAGAS (0.4+) –º–µ—Ç—Ä–∏–∫–∏ - —ç—Ç–æ –∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑ –º–æ–¥—É–ª–µ–π\n",
    "    try:\n",
    "        # –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–± (ragas >= 0.4): –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ –º–æ–¥—É–ª–µ–π collections\n",
    "        from ragas.metrics.collections.faithfulness import Faithfulness\n",
    "        from ragas.metrics.collections.answer_relevancy import AnswerRelevancy\n",
    "    except ImportError:\n",
    "        # –°—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± (ragas < 0.4): –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å—ã –Ω–∞–ø—Ä—è–º—É—é\n",
    "        from ragas.metrics import faithfulness as Faithfulness\n",
    "        from ragas.metrics import answer_relevancy as AnswerRelevancy\n",
    "    RAGAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RAGAS_AVAILABLE = False\n",
    "\n",
    "class RAGASEvaluator:\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω—â–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ RAGAS.\n",
    "    \n",
    "    –ó–∞—á–µ–º –Ω—É–∂–Ω–∞:\n",
    "    - –†–∞—Å—á—ë—Ç Faithfulness (–Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)\n",
    "    - –†–∞—Å—á—ë—Ç Answer Relevancy (–Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –∑–∞–ø—Ä–æ—Å—É)\n",
    "    - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å RAGAS –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mock_mode: bool = False,\n",
    "        llm_adapter=None,\n",
    "        embeddings_adapter=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAGASEvaluator.\n",
    "        \n",
    "        Args:\n",
    "            mock_mode: –ï—Å–ª–∏ True, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–∫-—Ä–µ–∂–∏–º (–±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ RAGAS)\n",
    "            llm_adapter: LangChain-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π LLM –∞–¥–∞–ø—Ç–µ—Ä (–¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ RAGAS)\n",
    "            embeddings_adapter: LangChain-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π Embeddings –∞–¥–∞–ø—Ç–µ—Ä (–¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ RAGAS)\n",
    "        \"\"\"\n",
    "        self.mock_mode = mock_mode\n",
    "        self.llm_adapter = llm_adapter\n",
    "        self.embeddings_adapter = embeddings_adapter\n",
    "        \n",
    "        if not self.mock_mode:\n",
    "            if not RAGAS_AVAILABLE:\n",
    "                print(\"Warning: ragas not installed. RAGASEvaluator will operate in mock mode.\")\n",
    "                self.ragas_available = False\n",
    "                self.mock_mode = True\n",
    "            elif not llm_adapter or not embeddings_adapter:\n",
    "                print(\"Warning: LLM or Embeddings adapter not provided. RAGASEvaluator will operate in mock mode.\")\n",
    "                self.ragas_available = False\n",
    "                self.mock_mode = True\n",
    "            else:\n",
    "                self.ragas_available = True\n",
    "        else:\n",
    "            self.ragas_available = False\n",
    "    \n",
    "    def evaluate_faithfulness(\n",
    "        self,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        contexts: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Faithfulness score —á–µ—Ä–µ–∑ RAGAS.\n",
    "        \n",
    "        Faithfulness –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.\n",
    "        –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞: ‚â• 0.85\n",
    "        \n",
    "        Args:\n",
    "            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            answer: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç\n",
    "            contexts: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (—Ç–µ–∫—Å—Ç—ã retrieved —á–∞–Ω–∫–æ–≤)\n",
    "            \n",
    "        Returns:\n",
    "            Faithfulness score (0.0-1.0)\n",
    "        \"\"\"\n",
    "        if self.mock_mode:\n",
    "            # –í –º–æ–∫-—Ä–µ–∂–∏–º–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤\n",
    "            answer_lower = answer.lower()\n",
    "            contexts_text = \" \".join(contexts).lower()\n",
    "            \n",
    "            # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤, faithfulness –≤—ã—Å–æ–∫–∏–π\n",
    "            if any(context.lower() in answer_lower for context in contexts):\n",
    "                return 0.90  # –í—ã—Å–æ–∫–∏–π faithfulness\n",
    "            else:\n",
    "                return 0.50  # –ù–∏–∑–∫–∏–π faithfulness\n",
    "        \n",
    "        # –†–µ–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å RAGAS\n",
    "        if not self.ragas_available:\n",
    "            raise RuntimeError(\"RAGAS not available. Check installation and adapters.\")\n",
    "        \n",
    "        try:\n",
    "            # –°–æ–∑–¥–∞—ë–º dataset –¥–ª—è RAGAS\n",
    "            # RAGAS –æ–∂–∏–¥–∞–µ—Ç: contexts - —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç - —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "            dataset_dict = {\n",
    "                \"question\": [question],\n",
    "                \"answer\": [answer],\n",
    "                \"contexts\": [contexts]  # contexts —É–∂–µ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "            }\n",
    "            dataset = Dataset.from_dict(dataset_dict)\n",
    "            \n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ü–µ–Ω–∫—É faithfulness\n",
    "            # –í RAGAS –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ ragas.metrics —É–∂–µ —è–≤–ª—è—é—Ç—Å—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤\n",
    "            # —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Å –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç—ã)\n",
    "            # llm –∏ embeddings –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ evaluate()\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∏–∑ —É—Ä–æ–≤–Ω—è –º–æ–¥—É–ª—è\n",
    "            import ragas.metrics as ragas_metrics\n",
    "            faithfulness_obj = ragas_metrics.faithfulness\n",
    "            faithfulness_class = type(faithfulness_obj)\n",
    "            faithfulness_metric = faithfulness_class()\n",
    "            result = evaluate(\n",
    "                dataset=dataset,\n",
    "                metrics=[faithfulness_metric],\n",
    "                llm=self.llm_adapter,\n",
    "                embeddings=self.embeddings_adapter\n",
    "            )\n",
    "            \n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º score (—Ä–µ–∑—É–ª—å—Ç–∞—Ç - DataFrame —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π)\n",
    "            faithfulness_score = result[\"faithfulness\"].iloc[0] if hasattr(result, \"iloc\") else result[\"faithfulness\"][0]\n",
    "            return float(faithfulness_score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating faithfulness with RAGAS: {e}\")\n",
    "            # Fallback –∫ mock mode –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
    "            return 0.75\n",
    "    \n",
    "    def evaluate_answer_relevancy(\n",
    "        self,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        contexts: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Answer Relevancy score —á–µ—Ä–µ–∑ RAGAS.\n",
    "        \n",
    "        Answer Relevancy –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –≤–æ–ø—Ä–æ—Å—É.\n",
    "        –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞: ‚â• 0.80\n",
    "        \n",
    "        Args:\n",
    "            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "            answer: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç\n",
    "            contexts: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (—Ç–µ–∫—Å—Ç—ã retrieved —á–∞–Ω–∫–æ–≤)\n",
    "            \n",
    "        Returns:\n",
    "            Answer Relevancy score (0.0-1.0)\n",
    "        \"\"\"\n",
    "        if self.mock_mode:\n",
    "            # –í –º–æ–∫-—Ä–µ–∂–∏–º–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\n",
    "            question_lower = question.lower()\n",
    "            answer_lower = answer.lower()\n",
    "            \n",
    "            # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞, relevancy –≤—ã—Å–æ–∫–∏–π\n",
    "            question_keywords = set(question_lower.split())\n",
    "            answer_keywords = set(answer_lower.split())\n",
    "            \n",
    "            overlap = len(question_keywords.intersection(answer_keywords))\n",
    "            if overlap > 0:\n",
    "                return 0.85  # –í—ã—Å–æ–∫–∏–π relevancy\n",
    "            else:\n",
    "                return 0.60  # –ù–∏–∑–∫–∏–π relevancy\n",
    "        \n",
    "        # –†–µ–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å RAGAS\n",
    "        if not self.ragas_available:\n",
    "            raise RuntimeError(\"RAGAS not available. Check installation and adapters.\")\n",
    "        \n",
    "        try:\n",
    "            # –°–æ–∑–¥–∞—ë–º dataset –¥–ª—è RAGAS\n",
    "            # RAGAS –æ–∂–∏–¥–∞–µ—Ç: contexts - —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç - —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "            dataset_dict = {\n",
    "                \"question\": [question],\n",
    "                \"answer\": [answer],\n",
    "                \"contexts\": [contexts]  # contexts —É–∂–µ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "            }\n",
    "            dataset = Dataset.from_dict(dataset_dict)\n",
    "            \n",
    "            # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ü–µ–Ω–∫—É answer_relevancy\n",
    "            # –í RAGAS –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ ragas.metrics —É–∂–µ —è–≤–ª—è—é—Ç—Å—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤\n",
    "            # —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Å –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç—ã)\n",
    "            # llm –∏ embeddings –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ evaluate()\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∏–∑ —É—Ä–æ–≤–Ω—è –º–æ–¥—É–ª—è\n",
    "            import ragas.metrics as ragas_metrics\n",
    "            answer_relevancy_obj = ragas_metrics.answer_relevancy\n",
    "            answer_relevancy_class = type(answer_relevancy_obj)\n",
    "            answer_relevancy_metric = answer_relevancy_class()\n",
    "            result = evaluate(\n",
    "                dataset=dataset,\n",
    "                metrics=[answer_relevancy_metric],\n",
    "                llm=self.llm_adapter,\n",
    "                embeddings=self.embeddings_adapter\n",
    "            )\n",
    "            \n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º score (—Ä–µ–∑—É–ª—å—Ç–∞—Ç - DataFrame —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π)\n",
    "            relevancy_score = result[\"answer_relevancy\"].iloc[0] if hasattr(result, \"iloc\") else result[\"answer_relevancy\"][0]\n",
    "            return float(relevancy_score)\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating answer_relevancy with RAGAS: {e}\")\n",
    "            # Fallback –∫ mock mode –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
    "            return 0.75\n",
    "    \n",
    "    def evaluate_all(\n",
    "        self,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        contexts: List[str],\n",
    "        ground_truth: Optional[str] = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ RAGAS.\"\"\"\n",
    "        faithfulness = self.evaluate_faithfulness(question, answer, contexts)\n",
    "        answer_relevancy = self.evaluate_answer_relevancy(question, answer, contexts)\n",
    "        \n",
    "        return {\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"answer_relevancy\": answer_relevancy\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Evaluation & Metrics —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ò–ù–°–¢–†–£–ö–¶–ò–Ø: –ö–∞–∫ –∑–∞–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è Production Pipeline\n",
    "# ============================================\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 1: –ó–∞–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Jupyter/Colab)\n",
    "USER_QUERY = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–µ—Ä–µ–¥–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "# result = run_production_pipeline(\"–í–∞—à –∑–∞–ø—Ä–æ—Å –∑–¥–µ—Å—å\")\n",
    "\n",
    "print(\"‚úÖ –ó–∞–ø—Ä–æ—Å –∑–∞–¥–∞–Ω. –¢–µ–ø–µ—Ä—å –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —è—á–µ–π–∫—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ pipeline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Production Pipeline ‚Äî –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "\n",
    "**–†–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ‚Üí –ü–æ–ª–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è ‚Üí Production —Ä–µ–∂–∏–º ‚Üí –ú–µ—Ç—Ä–∏–∫–∏**\n",
    "\n",
    "–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤ production —Ä–µ–∂–∏–º–µ:\n",
    "- –†–µ–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant Cloud\n",
    "- Production —Ä–µ–∂–∏–º –¥–ª—è EmbeddingService –∏ LLMClient (GigaChat API)\n",
    "- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞\n",
    "- –ü–æ–ª–Ω—ã–π –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Production Pipeline ‚Äî –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "# ============================================\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def run_production_pipeline(user_query: str = None):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≤ production —Ä–µ–∂–∏–º–µ:\n",
    "    1. –ó–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    2. –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    3. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant Cloud\n",
    "    4. Retrieval ‚Üí Reranking ‚Üí Generation (production —Ä–µ–∂–∏–º)\n",
    "    5. –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    \n",
    "    Args:\n",
    "        user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–≤–æ–¥ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π USER_QUERY\n",
    "    \"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª–∞—Å—Å—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n",
    "    required_classes = ['DocumentLoader', 'Chunker', 'EmbeddingService', 'QdrantIndexer', \n",
    "                       'Retriever', 'Reranker', 'PromptBuilder', 'LLMClient', \n",
    "                       'MetricsCollector', 'RAGASEvaluator']\n",
    "    missing_classes = [cls for cls in required_classes if cls not in globals()]\n",
    "    if missing_classes:\n",
    "        raise NameError(\n",
    "            f\"–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–ª–∞—Å—Å—ã: {', '.join(missing_classes)}. \"\n",
    "            f\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —è—á–µ–π–∫–∏ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –º–æ–¥—É–ª–µ–π.\"\n",
    "        )\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ LOCAL_DATA_PATH\n",
    "    if 'LOCAL_DATA_PATH' not in globals():\n",
    "        raise NameError(\n",
    "            \"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è LOCAL_DATA_PATH –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞. \"\n",
    "            \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫—É 6 (–î–∞–Ω–Ω—ã–µ –∏ –¥–æ–º–µ–Ω) –ø–µ—Ä–µ–¥ —ç—Ç–æ–π —è—á–µ–π–∫–æ–π.\"\n",
    "        )\n",
    "    \n",
    "    console.print(\"\\n[bold cyan]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó[/bold cyan]\")\n",
    "    console.print(\"[bold cyan]‚ïë   üöÄ PRODUCTION PIPELINE ‚Äî –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω               ‚ïë[/bold cyan]\")\n",
    "    console.print(\"[bold cyan]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold cyan]\\n\")\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 1: –ó–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]üìù –®–∞–≥ 1: –í–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–∞[/bold yellow]\")\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å: –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–≤–æ–¥–∞\n",
    "    if user_query is None:\n",
    "        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–≤–æ–¥\n",
    "        try:\n",
    "            user_query = input(\"–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å: \")\n",
    "            # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–≤—ë–ª –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, –ø—Ä–æ–±—É–µ–º fallback\n",
    "            if not user_query or not user_query.strip():\n",
    "                raise ValueError(\"Empty input\")\n",
    "        except (EOFError, KeyboardInterrupt, ValueError):\n",
    "            # –ï—Å–ª–∏ input() –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–ª–∏ –≤–≤–æ–¥ –ø—É—Å—Ç–æ–π, –ø—Ä–æ–±—É–µ–º USER_QUERY\n",
    "            if 'USER_QUERY' in globals() and globals()['USER_QUERY'] and globals()['USER_QUERY'].strip():\n",
    "                user_query = globals()['USER_QUERY']\n",
    "                console.print(f\"[cyan]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π USER_QUERY: {user_query}[/cyan]\")\n",
    "            else:\n",
    "                # –ï—Å–ª–∏ –∏ USER_QUERY –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å\n",
    "                user_query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "                console.print(f\"[yellow]‚ö†Ô∏è  –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {user_query}[/yellow]\")\n",
    "                console.print(\"[cyan]üí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ó–∞–¥–∞–π—Ç–µ –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é USER_QUERY[/cyan]\")\n",
    "                console.print(\"[cyan]   –ü—Ä–∏–º–µ—Ä: USER_QUERY = '–í–∞—à –∑–∞–ø—Ä–æ—Å' –∏–ª–∏ run_production_pipeline('–í–∞—à –∑–∞–ø—Ä–æ—Å')[/cyan]\")\n",
    "    elif isinstance(user_query, str) and not user_query.strip():\n",
    "        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä, –ø—Ä–æ–±—É–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–≤–æ–¥\n",
    "        try:\n",
    "            user_query = input(\"–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å: \")\n",
    "            if not user_query or not user_query.strip():\n",
    "                raise ValueError(\"Empty input\")\n",
    "        except (EOFError, KeyboardInterrupt, ValueError):\n",
    "            # Fallback –Ω–∞ USER_QUERY –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å\n",
    "            if 'USER_QUERY' in globals() and globals()['USER_QUERY'] and globals()['USER_QUERY'].strip():\n",
    "                user_query = globals()['USER_QUERY']\n",
    "                console.print(f\"[cyan]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π USER_QUERY: {user_query}[/cyan]\")\n",
    "            else:\n",
    "                user_query = \"–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?\"\n",
    "                console.print(f\"[yellow]‚ö†Ô∏è  –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {user_query}[/yellow]\")\n",
    "    \n",
    "    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å\n",
    "    if not user_query or (isinstance(user_query, str) and not user_query.strip()):\n",
    "        console.print(\"[red]‚ùå –ó–∞–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º![/red]\")\n",
    "        console.print(\"[cyan]üí° –ó–∞–¥–∞–π—Ç–µ –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä: run_production_pipeline('–í–∞—à –∑–∞–ø—Ä–æ—Å')[/cyan]\")\n",
    "        console.print(\"[cyan]   –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é: USER_QUERY = '–í–∞—à –∑–∞–ø—Ä–æ—Å'[/cyan]\")\n",
    "        return None\n",
    "    \n",
    "    console.print(f\"[green]‚úÖ –ó–∞–ø—Ä–æ—Å –ø–æ–ª—É—á–µ–Ω: {user_query}[/green]\\n\")\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]üìö –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤[/bold yellow]\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        loader = DocumentLoader()\n",
    "        all_documents = []\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ –ø–∞–ø–æ–∫ hr –∏ it\n",
    "        categories = ['hr', 'it']\n",
    "        for category in categories:\n",
    "            category_path = LOCAL_DATA_PATH / category\n",
    "            if category_path.exists():\n",
    "                docs = loader.load_documents(str(category_path))\n",
    "                all_documents.extend(docs)\n",
    "                console.print(f\"  üìÅ {category}: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "            else:\n",
    "                console.print(f\"  ‚ö†Ô∏è  –ü–∞–ø–∫–∞ {category} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {category_path}\")\n",
    "        \n",
    "        if not all_documents:\n",
    "            console.print(\"[yellow]‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å LOCAL_DATA_PATH[/yellow]\")\n",
    "            return None\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        console.print(f\"[green]‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_documents)} (–≤—Ä–µ–º—è: {load_time:.2f}—Å)[/green]\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}[/red]\")\n",
    "        return None\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 3: –ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]‚úÇÔ∏è  –®–∞–≥ 3: –ß–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤[/bold yellow]\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        chunker = Chunker()\n",
    "        chunks = chunker.chunk_documents(\n",
    "            all_documents,\n",
    "            chunk_size=300,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä\n",
    "            overlap_percent=0.25  # 25% overlap\n",
    "        )\n",
    "        chunk_time = time.time() - start_time\n",
    "        \n",
    "        console.print(f\"  üìä –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\")\n",
    "        console.print(f\"  üìè –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {sum(c.text_length for c in chunks) / len(chunks):.1f} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "        console.print(f\"[green]‚úÖ –ß–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω (–≤—Ä–µ–º—è: {chunk_time:.2f}—Å)[/green]\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á–∞–Ω–∫–∏–Ω–≥–µ: {e}[/red]\")\n",
    "        return None\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant Cloud\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]üîç –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant Cloud[/bold yellow]\")\n",
    "    \n",
    "    # –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è production\n",
    "    collection_name = \"neuro_docs_production\"\n",
    "    \n",
    "    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Qdrant\n",
    "    try:\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Qdrant Cloud (–∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤)\n",
    "        if 'QDRANT_URL' not in globals() or not QDRANT_URL:\n",
    "            console.print(\"[red]‚ùå QDRANT_URL –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Å–µ–∫—Ä–µ—Ç QDRANT_URL[/red]\")\n",
    "            return None\n",
    "        \n",
    "        qdrant_url = QDRANT_URL.strip() if QDRANT_URL else None\n",
    "        qdrant_api_key = QDRANT_API_KEY.strip() if 'QDRANT_API_KEY' in globals() and QDRANT_API_KEY else None\n",
    "        \n",
    "        if qdrant_url:\n",
    "            if qdrant_api_key:\n",
    "                qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
    "            else:\n",
    "                qdrant_client = QdrantClient(url=qdrant_url)\n",
    "            console.print(f\"  üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Qdrant Cloud: {qdrant_url[:50]}...\")\n",
    "        else:\n",
    "            console.print(\"[yellow]‚ö†Ô∏è  QDRANT_URL –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è in-memory —Ä–µ–∂–∏–º[/yellow]\")\n",
    "            qdrant_client = QdrantClient(\":memory:\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫\n",
    "        collection_exists = False\n",
    "        points_count = 0\n",
    "        need_indexing = True\n",
    "        \n",
    "        try:\n",
    "            collections = qdrant_client.get_collections()\n",
    "            collection_names = [c.name for c in collections.collections]\n",
    "            \n",
    "            if collection_name in collection_names:\n",
    "                collection_exists = True\n",
    "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n",
    "                try:\n",
    "                    count_result = qdrant_client.count(collection_name)\n",
    "                    points_count = count_result.count\n",
    "                except Exception as e:\n",
    "                    # –ï—Å–ª–∏ count –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º get_collection\n",
    "                    try:\n",
    "                        collection_info = qdrant_client.get_collection(collection_name)\n",
    "                        points_count = collection_info.points_count\n",
    "                    except:\n",
    "                        points_count = 0\n",
    "                \n",
    "                # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—á–∫–∏ (—Ö–æ—Ç—è –±—ã 80% –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤),\n",
    "                # —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞\n",
    "                if points_count > 0:\n",
    "                    chunks_count = len(chunks)\n",
    "                    # –ï—Å–ª–∏ —Ç–æ—á–µ–∫ –±–æ–ª—å—à–µ 80% –æ—Ç —á–∞–Ω–∫–æ–≤, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞\n",
    "                    if points_count >= chunks_count * 0.8:\n",
    "                        need_indexing = False\n",
    "                        console.print(f\"  ‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\")\n",
    "                        console.print(f\"  üìä –¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {points_count} —Ç–æ—á–µ–∫\")\n",
    "                        console.print(f\"  üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {chunks_count}\")\n",
    "                        console.print(f\"  ‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é embeddings –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é)\\n\")\n",
    "                    else:\n",
    "                        console.print(f\"  ‚ö†Ô∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ–Ω—å—à–µ —Ç–æ—á–µ–∫ ({points_count} < {chunks_count * 0.8})\")\n",
    "                        console.print(f\"  üîÑ –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è\\n\")\n",
    "                else:\n",
    "                    console.print(f\"  ‚ÑπÔ∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –ø—É—Å—Ç–∞\")\n",
    "                    console.print(f\"  üîÑ –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è\\n\")\n",
    "            else:\n",
    "                console.print(f\"  ‚ÑπÔ∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\")\n",
    "                console.print(f\"  üîÑ –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è\\n\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}\")\n",
    "            console.print(f\"  üîÑ –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ Qdrant: {e}[/red]\")\n",
    "        return None\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
    "    # ============================================\n",
    "    embeddings = None\n",
    "    actual_embedding_dim = 1536  # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
    "    embedding_service = None\n",
    "    embedding_time = 0.0  # –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings (–µ—Å–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–æ, –±—É–¥–µ—Ç 0)\n",
    "    index_time = 0.0  # –í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–µ—Å–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–æ, –±—É–¥–µ—Ç 0)\n",
    "    \n",
    "    if need_indexing:\n",
    "        console.print(\"[bold yellow]üß† –®–∞–≥ 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings (Production —Ä–µ–∂–∏–º)[/bold yellow]\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Production —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π GigaChat API\n",
    "            embedding_service = EmbeddingService(\n",
    "                embedding_dim=1536,\n",
    "                batch_size=10,\n",
    "                mock_mode=False  # Production —Ä–µ–∂–∏–º!\n",
    "            )\n",
    "            \n",
    "            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤\n",
    "            chunk_texts = [chunk.text for chunk in chunks]\n",
    "            embeddings = embedding_service.generate_embeddings(chunk_texts)\n",
    "            \n",
    "            embedding_time = time.time() - start_time\n",
    "            \n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å embeddings –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏\n",
    "            actual_embedding_dim = len(embeddings[0]) if embeddings else 1536\n",
    "            \n",
    "            console.print(f\"  üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ embeddings: {len(embeddings)}\")\n",
    "            console.print(f\"  üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {actual_embedding_dim}\")\n",
    "            console.print(f\"[green]‚úÖ Embeddings —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã (–≤—Ä–µ–º—è: {embedding_time:.2f}—Å, —Å–∫–æ—Ä–æ—Å—Ç—å: {len(embeddings)/embedding_time:.1f} —á–∞–Ω–∫–æ–≤/—Å)[/green]\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings: {e}[/red]\")\n",
    "            console.print(\"[yellow]üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GIGACHAT_AUTH_KEY –∏ GIGACHAT_SCOPE –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö[/yellow]\")\n",
    "            return None\n",
    "    else:\n",
    "        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞, –≤—Å—ë —Ä–∞–≤–Ω–æ —Å–æ–∑–¥–∞—ë–º embedding_service –¥–ª—è retrieval\n",
    "        embedding_service = EmbeddingService(\n",
    "            embedding_dim=1536,\n",
    "            batch_size=10,\n",
    "            mock_mode=False\n",
    "        )\n",
    "        console.print(\"[bold yellow]üß† –®–∞–≥ 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings[/bold yellow]\")\n",
    "        console.print(\"  ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ embeddings –≤ Qdrant)\\n\")\n",
    "        embedding_time = 0.0  # –í—Ä–µ–º—è = 0, —Ç–∞–∫ –∫–∞–∫ —à–∞–≥ –ø—Ä–æ–ø—É—â–µ–Ω\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 6: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant Cloud (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
    "    # ============================================\n",
    "    if need_indexing:\n",
    "        console.print(\"[bold yellow]‚òÅÔ∏è  –®–∞–≥ 6: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant Cloud[/bold yellow]\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é: –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é, —É–¥–∞–ª—è–µ–º\n",
    "            if collection_exists:\n",
    "                try:\n",
    "                    existing_dim = None\n",
    "                    try:\n",
    "                        collection_info = qdrant_client.get_collection(collection_name)\n",
    "                        existing_dim = collection_info.config.params.vectors.size\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    \n",
    "                    if existing_dim is not None and existing_dim != actual_embedding_dim:\n",
    "                        console.print(f\"  ‚ö†Ô∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {existing_dim}, —Ç—Ä–µ–±—É–µ—Ç—Å—è {actual_embedding_dim}\")\n",
    "                        console.print(f\"  üîÑ –£–¥–∞–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é...\")\n",
    "                        qdrant_client.delete_collection(collection_name)\n",
    "                        console.print(f\"  ‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞\")\n",
    "                    elif existing_dim is None:\n",
    "                        # –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å - —É–¥–∞–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è\n",
    "                        console.print(f\"  ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º...\")\n",
    "                        qdrant_client.delete_collection(collection_name)\n",
    "                        console.print(f\"  ‚úÖ –°—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞\")\n",
    "                except Exception as e:\n",
    "                    console.print(f\"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}\")\n",
    "                    try:\n",
    "                        qdrant_client.delete_collection(collection_name)\n",
    "                        console.print(f\"  ‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ–º\")\n",
    "                    except:\n",
    "                        pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –º–æ–∂–µ—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å\n",
    "            \n",
    "            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é (–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∏–∑ embeddings)\n",
    "            indexer = QdrantIndexer(\n",
    "                qdrant_client=qdrant_client,\n",
    "                collection_name=collection_name,\n",
    "                embedding_dim=actual_embedding_dim  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ embeddings\n",
    "            )\n",
    "            \n",
    "            indexed_count = indexer.index_chunks(chunks, embeddings)\n",
    "            index_time = time.time() - start_time\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏\n",
    "            try:\n",
    "                count_result = qdrant_client.count(collection_name)\n",
    "                points_count = count_result.count\n",
    "            except:\n",
    "                points_count = indexed_count\n",
    "            \n",
    "            console.print(f\"  üìä –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {indexed_count}\")\n",
    "            console.print(f\"  üìà –†–∞–∑–º–µ—Ä –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {points_count} —Ç–æ—á–µ–∫\")\n",
    "            console.print(f\"[green]‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (–≤—Ä–µ–º—è: {index_time:.2f}—Å, —Å–∫–æ—Ä–æ—Å—Ç—å: {indexed_count/index_time:.1f} —á–∞–Ω–∫–æ–≤/—Å)[/green]\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}[/red]\")\n",
    "            return None\n",
    "    else:\n",
    "        console.print(\"[bold yellow]‚òÅÔ∏è  –®–∞–≥ 6: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant Cloud[/bold yellow]\")\n",
    "        console.print(\"  ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–∫–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞)\\n\")\n",
    "        index_time = 0.0  # –í—Ä–µ–º—è = 0, —Ç–∞–∫ –∫–∞–∫ —à–∞–≥ –ø—Ä–æ–ø—É—â–µ–Ω\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 7: Retrieval\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]üîç –®–∞–≥ 7: Semantic Search (Retrieval)[/bold yellow]\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        retriever = Retriever(\n",
    "            qdrant_client=qdrant_client,\n",
    "            embedding_service=embedding_service,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        \n",
    "        retrieved_chunks = retriever.retrieve(user_query, k=5)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        console.print(f\"  üìä –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(retrieved_chunks)}\")\n",
    "        console.print(f\"  ‚≠ê –°—Ä–µ–¥–Ω–∏–π score: {sum(c.score for c in retrieved_chunks) / len(retrieved_chunks):.4f}\" if retrieved_chunks else \"  ‚≠ê –°—Ä–µ–¥–Ω–∏–π score: N/A\")\n",
    "        console.print(f\"[green]‚úÖ Retrieval –∑–∞–≤–µ—Ä—à—ë–Ω (–≤—Ä–µ–º—è: {retrieval_time:.3f}—Å)[/green]\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ retrieval: {e}[/red]\")\n",
    "        return None\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 8: Reranking\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]üéØ –®–∞–≥ 8: Reranking[/bold yellow]\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        reranker = Reranker()\n",
    "        reranked_chunks = reranker.rerank(user_query, retrieved_chunks, top_k=3)\n",
    "        rerank_time = time.time() - start_time\n",
    "        \n",
    "        console.print(f\"  üìä –û—Ç–æ–±—Ä–∞–Ω–æ —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ reranking: {len(reranked_chunks)}\")\n",
    "        console.print(f\"  ‚≠ê –°—Ä–µ–¥–Ω–∏–π rerank score: {sum(c.rerank_score for c in reranked_chunks) / len(reranked_chunks):.4f}\" if reranked_chunks else \"  ‚≠ê –°—Ä–µ–¥–Ω–∏–π rerank score: N/A\")\n",
    "        console.print(f\"[green]‚úÖ Reranking –∑–∞–≤–µ—Ä—à—ë–Ω (–≤—Ä–µ–º—è: {rerank_time:.3f}—Å)[/green]\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ reranking: {e}[/red]\")\n",
    "        return None\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 9: Generation (Production —Ä–µ–∂–∏–º)\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]üí¨ –®–∞–≥ 9: Generation –æ—Ç–≤–µ—Ç–∞ (Production —Ä–µ–∂–∏–º)[/bold yellow]\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        prompt_builder = PromptBuilder()\n",
    "        prompt = prompt_builder.build_prompt(user_query, reranked_chunks)\n",
    "        \n",
    "        # Production —Ä–µ–∂–∏–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π GigaChat API\n",
    "        # LLMClient –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—Å—è –Ω–∞ mock —Ä–µ–∂–∏–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö API\n",
    "        llm_client = LLMClient(mock_mode=False)  # Production —Ä–µ–∂–∏–º!\n",
    "        answer = llm_client.generate_answer(prompt)\n",
    "        \n",
    "        generation_time = time.time() - start_time\n",
    "        console.print(f\"[green]‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω (–≤—Ä–µ–º—è: {generation_time:.2f}—Å)[/green]\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}[/red]\")\n",
    "        console.print(\"[yellow]üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GIGACHAT_AUTH_KEY –∏ GIGACHAT_SCOPE –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö[/yellow]\")\n",
    "        return None\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 10: –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    # ============================================\n",
    "    console.print(\"[bold yellow]üìä –®–∞–≥ 10: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫[/bold yellow]\")\n",
    "    \n",
    "    try:\n",
    "        metrics_collector = MetricsCollector()\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAGASEvaluator –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ\n",
    "        # –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω—ã LLMClient –∏ EmbeddingService –≤ —Ä–µ–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ\n",
    "        llm_for_ragas = LLMClient(mock_mode=False)\n",
    "        embedding_for_ragas = EmbeddingService(mock_mode=False)\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–Ω—ã –ª–∏ –∞–¥–∞–ø—Ç–µ—Ä—ã LangChain\n",
    "        try:\n",
    "            from langchain_core.language_models.llms import LLM\n",
    "            from langchain_core.embeddings import Embeddings\n",
    "            from ragas import evaluate\n",
    "            from datasets import Dataset\n",
    "            # –ú–µ—Ç—Ä–∏–∫–∏ —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥—É–ª—è (Faithfulness, AnswerRelevancy)\n",
    "            \n",
    "            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã (–µ—Å–ª–∏ –æ–Ω–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ –Ω–æ—É—Ç–±—É–∫–µ)\n",
    "            if 'GigaChatLLMAdapter' in globals() and 'GigaChatEmbeddingsAdapter' in globals():\n",
    "                llm_adapter = GigaChatLLMAdapter(llm_client=llm_for_ragas)\n",
    "                embeddings_adapter = GigaChatEmbeddingsAdapter(embedding_service=embedding_for_ragas)\n",
    "                ragas_evaluator = RAGASEvaluator(\n",
    "                    mock_mode=False,\n",
    "                    llm_adapter=llm_adapter,\n",
    "                    embeddings_adapter=embeddings_adapter\n",
    "                )\n",
    "                console.print(\"[green]‚úÖ RAGASEvaluator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ —Ä–µ–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ.[/green]\")\n",
    "            else:\n",
    "                ragas_evaluator = RAGASEvaluator(mock_mode=True)\n",
    "                console.print(\"[yellow]‚ö†Ô∏è  LangChain –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. RAGASEvaluator –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ mock —Ä–µ–∂–∏–º–µ.[/yellow]\")\n",
    "        except ImportError as e:\n",
    "            ragas_evaluator = RAGASEvaluator(mock_mode=True)\n",
    "            console.print(f\"[yellow]‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å RAGAS –∏–ª–∏ LangChain: {e}. RAGASEvaluator –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ mock —Ä–µ–∂–∏–º–µ.[/yellow]\")\n",
    "        except Exception as e:\n",
    "            ragas_evaluator = RAGASEvaluator(mock_mode=True)\n",
    "            console.print(f\"[yellow]‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ RAGAS: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è mock —Ä–µ–∂–∏–º.[/yellow]\")\n",
    "        \n",
    "        contexts = [chunk.text for chunk in reranked_chunks]\n",
    "        ragas_metrics = ragas_evaluator.evaluate_all(user_query, answer, contexts)\n",
    "        \n",
    "        console.print(\"[green]‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã[/green]\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        console.print(f\"[yellow]‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}[/yellow]\")\n",
    "        ragas_metrics = {\"faithfulness\": 0.0, \"answer_relevancy\": 0.0}\n",
    "    \n",
    "    # ============================================\n",
    "    # –®–∞–≥ 11: –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    # ============================================\n",
    "    console.print(\"[bold cyan]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó[/bold cyan]\")\n",
    "    console.print(\"[bold cyan]‚ïë                    üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´                         ‚ïë[/bold cyan]\")\n",
    "    console.print(\"[bold cyan]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold cyan]\\n\")\n",
    "    \n",
    "    # –ó–∞–ø—Ä–æ—Å\n",
    "    console.print(Panel(f\"[bold]{user_query}[/bold]\", title=\"üë§ –í–∞—à –∑–∞–ø—Ä–æ—Å\", border_style=\"cyan\"))\n",
    "    \n",
    "    # –û—Ç–≤–µ—Ç\n",
    "    console.print(Panel(answer, title=\"ü§ñ –û—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º—ã\", border_style=\"green\"))\n",
    "    \n",
    "    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏\n",
    "    console.print(\"\\n[bold yellow]üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (Retrieved & Reranked):[/bold yellow]\")\n",
    "    sources_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    sources_table.add_column(\"‚Ññ\", style=\"dim\", width=3)\n",
    "    sources_table.add_column(\"Score\", justify=\"right\", style=\"cyan\", width=8)\n",
    "    sources_table.add_column(\"Rerank Score\", justify=\"right\", style=\"green\", width=12)\n",
    "    sources_table.add_column(\"–ò—Å—Ç–æ—á–Ω–∏–∫\", style=\"yellow\", width=20)\n",
    "    sources_table.add_column(\"–¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 60 —Å–∏–º–≤–æ–ª–æ–≤)\", style=\"white\", width=60)\n",
    "    \n",
    "    for i, chunk in enumerate(reranked_chunks, 1):\n",
    "        source_name = chunk.metadata.get(\"file_name\", chunk.metadata.get(\"category\", \"unknown\"))\n",
    "        sources_table.add_row(\n",
    "            str(i),\n",
    "            f\"{chunk.score:.4f}\",\n",
    "            f\"{chunk.rerank_score:.4f}\",\n",
    "            source_name,\n",
    "            chunk.text[:60] + \"...\" if len(chunk.text) > 60 else chunk.text\n",
    "        )\n",
    "    \n",
    "    console.print(sources_table)\n",
    "    \n",
    "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
    "    console.print(\"\\n[bold yellow]üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:[/bold yellow]\")\n",
    "    metrics_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    metrics_table.add_column(\"–ú–µ—Ç—Ä–∏–∫–∞\", style=\"cyan\", width=30)\n",
    "    metrics_table.add_column(\"–ó–Ω–∞—á–µ–Ω–∏–µ\", justify=\"right\", style=\"green\", width=15)\n",
    "    metrics_table.add_column(\"–û–ø–∏—Å–∞–Ω–∏–µ\", style=\"white\")\n",
    "    \n",
    "    metrics_table.add_row(\"Faithfulness\", f\"{ragas_metrics['faithfulness']:.3f}\",\n",
    "                          \"–ù–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (0-1)\")\n",
    "    metrics_table.add_row(\"Answer Relevancy\", f\"{ragas_metrics['answer_relevancy']:.3f}\",\n",
    "                          \"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –∑–∞–ø—Ä–æ—Å—É (0-1)\")\n",
    "    \n",
    "    console.print(metrics_table)\n",
    "    \n",
    "    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "    total_time = load_time + chunk_time + embedding_time + index_time + retrieval_time + rerank_time + generation_time\n",
    "    console.print(\"\\n[bold yellow]‚è±Ô∏è  –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:[/bold yellow]\")\n",
    "    perf_table = Table(show_header=True, header_style=\"bold magenta\")\n",
    "    perf_table.add_column(\"–≠—Ç–∞–ø\", style=\"cyan\", width=25)\n",
    "    perf_table.add_column(\"–í—Ä–µ–º—è\", justify=\"right\", style=\"yellow\", width=12)\n",
    "    perf_table.add_column(\"% –æ—Ç –æ–±—â–µ–≥–æ\", justify=\"right\", style=\"dim\", width=12)\n",
    "    \n",
    "    stages = [\n",
    "        (\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\", load_time),\n",
    "        (\"–ß–∞–Ω–∫–∏–Ω–≥\", chunk_time),\n",
    "        (\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings\", embedding_time),\n",
    "        (\"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è\", index_time),\n",
    "        (\"Retrieval\", retrieval_time),\n",
    "        (\"Reranking\", rerank_time),\n",
    "        (\"Generation\", generation_time)\n",
    "    ]\n",
    "    \n",
    "    for stage_name, stage_time in stages:\n",
    "        percentage = (stage_time / total_time * 100) if total_time > 0 else 0\n",
    "        perf_table.add_row(stage_name, f\"{stage_time:.2f}—Å\", f\"{percentage:.1f}%\")\n",
    "    \n",
    "    perf_table.add_row(\"[bold]–ò—Ç–æ–≥–æ[/bold]\", f\"[bold]{total_time:.2f}—Å[/bold]\", \"100.0%\")\n",
    "    console.print(perf_table)\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    console.print(\"\\n[bold yellow]üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:[/bold yellow]\")\n",
    "    console.print(f\"  ‚Ä¢ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_documents)}\")\n",
    "    console.print(f\"  ‚Ä¢ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\")\n",
    "    console.print(f\"  ‚Ä¢ Retrieved —á–∞–Ω–∫–æ–≤: {len(retrieved_chunks)}\")\n",
    "    console.print(f\"  ‚Ä¢ Reranked —á–∞–Ω–∫–æ–≤: {len(reranked_chunks)}\")\n",
    "    console.print(f\"  ‚Ä¢ –†–∞–∑–º–µ—Ä –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant: {points_count} —Ç–æ—á–µ–∫\")\n",
    "    \n",
    "    console.print(\"\\n[bold green]‚úÖ Production Pipeline –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ![/bold green]\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": user_query,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": reranked_chunks,\n",
    "        \"metrics\": ragas_metrics,\n",
    "        \"performance\": {\n",
    "            \"total_time\": total_time,\n",
    "            \"stages\": stages\n",
    "        },\n",
    "        \"statistics\": {\n",
    "            \"documents\": len(all_documents),\n",
    "            \"chunks\": len(chunks),\n",
    "            \"retrieved\": len(retrieved_chunks),\n",
    "            \"reranked\": len(reranked_chunks),\n",
    "            \"qdrant_points\": points_count\n",
    "        }\n",
    "    }\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º production pipeline (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è USER_QUERY –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —è—á–µ–π–∫–∏)\n",
    "result = run_production_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –±–µ–∑ reranking vs —Å reranking (Production –¥–∞–Ω–Ω—ã–µ)\n",
    "\n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ retrieval —Å reranking –∏ –±–µ–∑ reranking –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Production Pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –±–µ–∑ reranking vs —Å reranking (Production –¥–∞–Ω–Ω—ã–µ)\n",
    "# ============================================\n",
    "\n",
    "def compare_reranking_production(query: str = None, qdrant_client=None, collection_name: str = \"neuro_docs_production\"):\n",
    "    \"\"\"\n",
    "    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å reranking –∏ –±–µ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Production Pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query: –ó–∞–ø—Ä–æ—Å. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ result production pipeline\n",
    "        qdrant_client: Qdrant –∫–ª–∏–µ–Ω—Ç. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ production pipeline\n",
    "        collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é production –∫–æ–ª–ª–µ–∫—Ü–∏—è)\n",
    "    \"\"\"\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ production pipeline, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã\n",
    "    if 'result' not in globals() or result is None:\n",
    "        raise ValueError(\"–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline! –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'result' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.\")\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ result –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
    "    if query is None:\n",
    "        query = result.get('query', '–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?')\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º Qdrant –∫–ª–∏–µ–Ω—Ç –∏–∑ production pipeline\n",
    "    if qdrant_client is None:\n",
    "        # –ù—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ production pipeline –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π\n",
    "        if 'QDRANT_URL' in globals() and QDRANT_URL:\n",
    "            qdrant_url = QDRANT_URL.strip() if QDRANT_URL else None\n",
    "            qdrant_api_key = QDRANT_API_KEY.strip() if 'QDRANT_API_KEY' in globals() and QDRANT_API_KEY else None\n",
    "            if qdrant_url:\n",
    "                if qdrant_api_key:\n",
    "                    qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
    "                else:\n",
    "                    qdrant_client = QdrantClient(url=qdrant_url)\n",
    "            else:\n",
    "                raise ValueError(\"QDRANT_URL –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline —Å–Ω–∞—á–∞–ª–∞.\")\n",
    "        else:\n",
    "            raise ValueError(\"QDRANT_URL –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline —Å–Ω–∞—á–∞–ª–∞.\")\n",
    "    \n",
    "    # Production —Ä–µ–∂–∏–º –¥–ª—è embedding service\n",
    "    embedding_service = EmbeddingService(mock_mode=False)\n",
    "    \n",
    "    # –ë–µ–∑ reranking\n",
    "    retriever = Retriever(\n",
    "        qdrant_client=qdrant_client,\n",
    "        embedding_service=embedding_service,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    chunks_without_rerank = retriever.retrieve(query, k=5)\n",
    "    \n",
    "    # –° reranking\n",
    "    reranker = Reranker()\n",
    "    reranked_chunks = reranker.rerank(query, chunks_without_rerank, top_k=3)\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "    console.print(f\"\\n[bold cyan]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó[/bold cyan]\")\n",
    "    console.print(f\"[bold cyan]‚ïë   üìä –°–†–ê–í–ù–ï–ù–ò–ï RERANKING (Production –¥–∞–Ω–Ω—ã–µ)            ‚ïë[/bold cyan]\")\n",
    "    console.print(f\"[bold cyan]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold cyan]\")\n",
    "    console.print(f\"[bold]–ó–∞–ø—Ä–æ—Å:[/bold] {query}\\n\")\n",
    "    \n",
    "    # –¢–∞–±–ª–∏—Ü–∞: –±–µ–∑ reranking\n",
    "    table1 = Table(title=\"–ë–ï–ó Reranking\", show_header=True, header_style=\"bold red\")\n",
    "    table1.add_column(\"‚Ññ\", style=\"dim\", width=3)\n",
    "    table1.add_column(\"Score\", justify=\"right\", style=\"cyan\", width=10)\n",
    "    table1.add_column(\"–ò—Å—Ç–æ—á–Ω–∏–∫\", style=\"yellow\", width=20)\n",
    "    table1.add_column(\"Text Preview\", style=\"white\", width=50)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks_without_rerank[:5], 1):\n",
    "        source_name = chunk.metadata.get(\"file_name\", chunk.metadata.get(\"category\", \"unknown\"))\n",
    "        table1.add_row(\n",
    "            str(i),\n",
    "            f\"{chunk.score:.4f}\",\n",
    "            source_name[:20] + \"...\" if len(source_name) > 20 else source_name,\n",
    "            chunk.text[:60] + \"...\" if len(chunk.text) > 60 else chunk.text\n",
    "        )\n",
    "    \n",
    "    console.print(table1)\n",
    "    \n",
    "    # –¢–∞–±–ª–∏—Ü–∞: —Å reranking\n",
    "    table2 = Table(title=\"–° Reranking\", show_header=True, header_style=\"bold green\")\n",
    "    table2.add_column(\"‚Ññ\", style=\"dim\", width=3)\n",
    "    table2.add_column(\"Original Score\", justify=\"right\", style=\"cyan\", width=12)\n",
    "    table2.add_column(\"Rerank Score\", justify=\"right\", style=\"green\", width=12)\n",
    "    table2.add_column(\"–ò—Å—Ç–æ—á–Ω–∏–∫\", style=\"yellow\", width=20)\n",
    "    table2.add_column(\"Text Preview\", style=\"white\", width=50)\n",
    "    \n",
    "    for i, chunk in enumerate(reranked_chunks, 1):\n",
    "        source_name = chunk.metadata.get(\"file_name\", chunk.metadata.get(\"category\", \"unknown\"))\n",
    "        table2.add_row(\n",
    "            str(i),\n",
    "            f\"{chunk.score:.4f}\",\n",
    "            f\"{chunk.rerank_score:.4f}\",\n",
    "            source_name[:20] + \"...\" if len(source_name) > 20 else source_name,\n",
    "            chunk.text[:60] + \"...\" if len(chunk.text) > 60 else chunk.text\n",
    "        )\n",
    "    \n",
    "    console.print(table2)\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
    "    console.print(\"\\n[bold yellow]–ê–Ω–∞–ª–∏–∑:[/bold yellow]\")\n",
    "    order_changed = False\n",
    "    rerank_ids = [chunk.id for chunk in reranked_chunks]\n",
    "    original_ids = [chunk.id for chunk in chunks_without_rerank[:len(reranked_chunks)]]\n",
    "    \n",
    "    for i, (orig_id, rerank_id) in enumerate(zip(original_ids, rerank_ids), 1):\n",
    "        if orig_id != rerank_id:\n",
    "            order_changed = True\n",
    "            console.print(f\"  –ü–æ–∑–∏—Ü–∏—è {i}: –∏–∑–º–µ–Ω—ë–Ω –ø–æ—Ä—è–¥–æ–∫ ({orig_id[:20]}... ‚Üí {rerank_id[:20]}...)\")\n",
    "    \n",
    "    if not order_changed:\n",
    "        console.print(\"  –ü–æ—Ä—è–¥–æ–∫ –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è (–¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ reranking –Ω–µ –ø–æ–≤–ª–∏—è–ª –Ω–∞ –ø–æ—Ä—è–¥–æ–∫)\")\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ scores\n",
    "    avg_original_score = sum(c.score for c in chunks_without_rerank[:len(reranked_chunks)]) / len(reranked_chunks)\n",
    "    avg_rerank_score = sum(c.rerank_score for c in reranked_chunks) / len(reranked_chunks)\n",
    "    console.print(f\"\\n[bold]–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:[/bold]\")\n",
    "    console.print(f\"  –°—Ä–µ–¥–Ω–∏–π original score (—Ç–æ–ø-{len(reranked_chunks)}): {avg_original_score:.4f}\")\n",
    "    console.print(f\"  –°—Ä–µ–¥–Ω–∏–π rerank score: {avg_rerank_score:.4f}\")\n",
    "    console.print(f\"  –†–∞–∑–Ω–∏—Ü–∞: {avg_rerank_score - avg_original_score:+.4f}\")\n",
    "    \n",
    "    return chunks_without_rerank, reranked_chunks\n",
    "\n",
    "# –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "if 'result' in globals() and result:\n",
    "    chunks_no_rerank_prod, chunks_reranked_prod = compare_reranking_production()\n",
    "else:\n",
    "    console.print(\"[yellow]‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline![/yellow]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (Production –¥–∞–Ω–Ω—ã–µ)\n",
    "\n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (K, reranking) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Production Pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (Production –¥–∞–Ω–Ω—ã–µ)\n",
    "# ============================================\n",
    "\n",
    "def run_experiments_production(query: str = None, qdrant_client=None, collection_name: str = \"neuro_docs_production\"):\n",
    "    \"\"\"\n",
    "    –ó–∞–ø—É—Å–∫–∞–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "    \n",
    "    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç:\n",
    "    - K: 3, 5, 8\n",
    "    - reranking: on/off\n",
    "    \n",
    "    Args:\n",
    "        query: –ó–∞–ø—Ä–æ—Å. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ result production pipeline\n",
    "        qdrant_client: Qdrant –∫–ª–∏–µ–Ω—Ç. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π\n",
    "        collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é production –∫–æ–ª–ª–µ–∫—Ü–∏—è)\n",
    "    \"\"\"\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ production pipeline\n",
    "    if 'result' not in globals() or result is None:\n",
    "        raise ValueError(\"–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline! –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'result' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.\")\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ result –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞\n",
    "    if query is None:\n",
    "        query = result.get('query', '–ö–∞–∫–æ–π SLA —É —Å–µ—Ä–≤–∏—Å–∞ –ø–ª–∞—Ç–µ–∂–µ–π?')\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º Qdrant –∫–ª–∏–µ–Ω—Ç\n",
    "    if qdrant_client is None:\n",
    "        if 'QDRANT_URL' in globals() and QDRANT_URL:\n",
    "            qdrant_url = QDRANT_URL.strip() if QDRANT_URL else None\n",
    "            qdrant_api_key = QDRANT_API_KEY.strip() if 'QDRANT_API_KEY' in globals() and QDRANT_API_KEY else None\n",
    "            if qdrant_url:\n",
    "                if qdrant_api_key:\n",
    "                    qdrant_client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
    "                else:\n",
    "                    qdrant_client = QdrantClient(url=qdrant_url)\n",
    "            else:\n",
    "                raise ValueError(\"QDRANT_URL –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline —Å–Ω–∞—á–∞–ª–∞.\")\n",
    "        else:\n",
    "            raise ValueError(\"QDRANT_URL –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline —Å–Ω–∞—á–∞–ª–∞.\")\n",
    "    \n",
    "    experiments = []\n",
    "    \n",
    "    # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ K –∏ reranking, –±–µ–∑ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)\n",
    "    configs = [\n",
    "        {\"k\": 3, \"reranking\": False},\n",
    "        {\"k\": 5, \"reranking\": False},\n",
    "        {\"k\": 3, \"reranking\": True},\n",
    "        {\"k\": 5, \"reranking\": True},\n",
    "    ]\n",
    "    \n",
    "    # Production —Ä–µ–∂–∏–º\n",
    "    embedding_service = EmbeddingService(mock_mode=False)\n",
    "    llm_client = LLMClient(mock_mode=False)\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAGAS evaluator –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞\n",
    "    try:\n",
    "        from langchain_core.language_models.llms import LLM\n",
    "        from langchain_core.embeddings import Embeddings\n",
    "        from ragas import evaluate\n",
    "        from datasets import Dataset\n",
    "        \n",
    "        if 'GigaChatLLMAdapter' in globals() and 'GigaChatEmbeddingsAdapter' in globals():\n",
    "            llm_adapter = GigaChatLLMAdapter(llm_client=llm_client)\n",
    "            embeddings_adapter = GigaChatEmbeddingsAdapter(embedding_service=embedding_service)\n",
    "            ragas_evaluator = RAGASEvaluator(\n",
    "                mock_mode=False,\n",
    "                llm_adapter=llm_adapter,\n",
    "                embeddings_adapter=embeddings_adapter\n",
    "            )\n",
    "        else:\n",
    "            ragas_evaluator = RAGASEvaluator(mock_mode=True)\n",
    "    except:\n",
    "        ragas_evaluator = RAGASEvaluator(mock_mode=True)\n",
    "    \n",
    "    metrics_collector = MetricsCollector()\n",
    "    \n",
    "    console.print(f\"\\n[bold cyan]üî¨ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (Production —Ä–µ–∂–∏–º)[/bold cyan]\")\n",
    "    console.print(f\"[bold]–ó–∞–ø—Ä–æ—Å:[/bold] {query}\\n\")\n",
    "    \n",
    "    for i, config in enumerate(configs, 1):\n",
    "        console.print(f\"[cyan]–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {i}/{len(configs)}: K={config['k']}, reranking={'on' if config['reranking'] else 'off'}[/cyan]\")\n",
    "        \n",
    "        try:\n",
    "            # Retrieval\n",
    "            retriever = Retriever(\n",
    "                qdrant_client=qdrant_client,\n",
    "                embedding_service=embedding_service,\n",
    "                collection_name=collection_name\n",
    "            )\n",
    "            retrieved_chunks = retriever.retrieve(query, k=config[\"k\"])\n",
    "            \n",
    "            # Reranking (–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω)\n",
    "            if config[\"reranking\"]:\n",
    "                reranker = Reranker()\n",
    "                reranked_chunks = reranker.rerank(query, retrieved_chunks, top_k=config[\"k\"])\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º reranked chunks\n",
    "                final_chunks = reranked_chunks\n",
    "            else:\n",
    "                final_chunks = retrieved_chunks\n",
    "            \n",
    "            # Generation\n",
    "            prompt_builder = PromptBuilder()\n",
    "            prompt = prompt_builder.build_prompt(query, final_chunks[:3])  # –ë–µ—Ä—ë–º —Ç–æ–ø-3 –¥–ª—è generation\n",
    "            answer = llm_client.generate_answer(prompt)\n",
    "            \n",
    "            # Metrics (RAGAS)\n",
    "            contexts = [chunk.text for chunk in final_chunks[:3]]\n",
    "            ragas_metrics = ragas_evaluator.evaluate_all(query, answer, contexts)\n",
    "            \n",
    "            experiments.append({\n",
    "                \"config\": config,\n",
    "                \"faithfulness\": ragas_metrics[\"faithfulness\"],\n",
    "                \"answer_relevancy\": ragas_metrics[\"answer_relevancy\"],\n",
    "                \"retrieved_count\": len(final_chunks),\n",
    "                \"avg_score\": sum(c.score for c in final_chunks) / len(final_chunks) if final_chunks else 0.0\n",
    "            })\n",
    "            \n",
    "            console.print(f\"  ‚úÖ –ó–∞–≤–µ—Ä—à—ë–Ω: faithfulness={ragas_metrics['faithfulness']:.3f}, relevancy={ragas_metrics['answer_relevancy']:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            console.print(f\"  ‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
    "            experiments.append({\n",
    "                \"config\": config,\n",
    "                \"faithfulness\": 0.0,\n",
    "                \"answer_relevancy\": 0.0,\n",
    "                \"retrieved_count\": 0,\n",
    "                \"avg_score\": 0.0\n",
    "            })\n",
    "    \n",
    "    return experiments\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\n",
    "if 'result' in globals() and result:\n",
    "    experiments_prod = run_experiments_production()\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    console.print(f\"\\n[bold cyan]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó[/bold cyan]\")\n",
    "    console.print(f\"[bold cyan]‚ïë   üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í (Production)                ‚ïë[/bold cyan]\")\n",
    "    console.print(f\"[bold cyan]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold cyan]\\n\")\n",
    "    \n",
    "    table = Table(title=\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π\", show_header=True, header_style=\"bold magenta\")\n",
    "    table.add_column(\"Config\", style=\"cyan\", width=25)\n",
    "    table.add_column(\"Faithfulness\", justify=\"right\", style=\"yellow\", width=12)\n",
    "    table.add_column(\"Answer Relevancy\", justify=\"right\", style=\"blue\", width=15)\n",
    "    table.add_column(\"Retrieved\", justify=\"right\", style=\"dim\", width=10)\n",
    "    table.add_column(\"Avg Score\", justify=\"right\", style=\"green\", width=10)\n",
    "    \n",
    "    for exp in experiments_prod:\n",
    "        config = exp[\"config\"]\n",
    "        config_str = f\"K={config['k']}, rerank={'on' if config['reranking'] else 'off'}\"\n",
    "        table.add_row(\n",
    "            config_str,\n",
    "            f\"{exp['faithfulness']:.3f}\",\n",
    "            f\"{exp['answer_relevancy']:.3f}\",\n",
    "            str(exp['retrieved_count']),\n",
    "            f\"{exp['avg_score']:.4f}\"\n",
    "        )\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    console.print(\"\\n[bold yellow]üìà –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:[/bold yellow]\")\n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ faithfulness\n",
    "    best_faith = max(experiments_prod, key=lambda x: x['faithfulness'])\n",
    "    console.print(f\"\\n  üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ Faithfulness:\")\n",
    "    console.print(f\"     Config: K={best_faith['config']['k']}, rerank={'on' if best_faith['config']['reranking'] else 'off'}\")\n",
    "    console.print(f\"     Faithfulness: {best_faith['faithfulness']:.3f}\")\n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ answer_relevancy\n",
    "    best_relevancy = max(experiments_prod, key=lambda x: x['answer_relevancy'])\n",
    "    console.print(f\"\\n  üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ Answer Relevancy:\")\n",
    "    console.print(f\"     Config: K={best_relevancy['config']['k']}, rerank={'on' if best_relevancy['config']['reranking'] else 'off'}\")\n",
    "    console.print(f\"     Answer Relevancy: {best_relevancy['answer_relevancy']:.3f}\")\n",
    "    \n",
    "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å reranking –∏ –±–µ–∑\n",
    "    with_rerank = [e for e in experiments_prod if e['config']['reranking']]\n",
    "    without_rerank = [e for e in experiments_prod if not e['config']['reranking']]\n",
    "    if with_rerank and without_rerank:\n",
    "        avg_faith_with = sum(e['faithfulness'] for e in with_rerank) / len(with_rerank)\n",
    "        avg_faith_without = sum(e['faithfulness'] for e in without_rerank) / len(without_rerank)\n",
    "        console.print(f\"\\n  üìä –í–ª–∏—è–Ω–∏–µ reranking –Ω–∞ Faithfulness:\")\n",
    "        console.print(f\"     –° reranking: {avg_faith_with:.3f}\")\n",
    "        console.print(f\"     –ë–µ–∑ reranking: {avg_faith_without:.3f}\")\n",
    "        console.print(f\"     –†–∞–∑–Ω–∏—Ü–∞: {avg_faith_with - avg_faith_without:+.3f}\")\n",
    "else:\n",
    "    console.print(\"[yellow]‚ö†Ô∏è  –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ Production Pipeline![/yellow]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîü –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥ (Production –¥–∞–Ω–Ω—ã–µ)\n",
    "\n",
    "–ß—Ç–æ –ø–æ–∫–∞–∑–∞–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –∫–∞–∫–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã, —á—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∞—Ç—å –¥–∞–ª—å—à–µ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥ (Production –¥–∞–Ω–Ω—ã–µ)\n",
    "# ============================================\n",
    "\n",
    "console.print(\"\\n[bold cyan]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó[/bold cyan]\")\n",
    "console.print(\"[bold cyan]‚ïë   üîü –ò–¢–û–ì–û–í–´–ô –í–´–í–û–î (Production Pipeline)              ‚ïë[/bold cyan]\")\n",
    "console.print(\"[bold cyan]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold cyan]\\n\")\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ production pipeline –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "if 'result' in globals() and result:\n",
    "    production_metrics = result.get('metrics', {})\n",
    "    production_stats = result.get('statistics', {})\n",
    "    production_perf = result.get('performance', {})\n",
    "    \n",
    "    console.print(\"[bold yellow]1. –ß—Ç–æ –ø–æ–∫–∞–∑–∞–ª Production Pipeline:[/bold yellow]\")\n",
    "    console.print(f\"  ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {production_stats.get('documents', 'N/A')}\")\n",
    "    console.print(f\"  ‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {production_stats.get('chunks', 'N/A')}\")\n",
    "    console.print(f\"  ‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ Qdrant: {production_stats.get('qdrant_points', 'N/A')} —Ç–æ—á–µ–∫\")\n",
    "    console.print(f\"  ‚úÖ Retrieved —á–∞–Ω–∫–æ–≤: {production_stats.get('retrieved', 'N/A')}\")\n",
    "    console.print(f\"  ‚úÖ Reranked —á–∞–Ω–∫–æ–≤: {production_stats.get('reranked', 'N/A')}\")\n",
    "    \n",
    "    if production_metrics:\n",
    "        console.print(f\"\\n  üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:\")\n",
    "        console.print(f\"     ‚Ä¢ Faithfulness: {production_metrics.get('faithfulness', 0):.3f} {'‚úÖ (‚â•0.85)' if production_metrics.get('faithfulness', 0) >= 0.85 else '‚ö†Ô∏è  (<0.85)'}\")\n",
    "        console.print(f\"     ‚Ä¢ Answer Relevancy: {production_metrics.get('answer_relevancy', 0):.3f} {'‚úÖ (‚â•0.80)' if production_metrics.get('answer_relevancy', 0) >= 0.80 else '‚ö†Ô∏è  (<0.80)'}\")\n",
    "    \n",
    "    if production_perf.get('total_time'):\n",
    "        console.print(f\"\\n  ‚è±Ô∏è  –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\")\n",
    "        console.print(f\"     ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {production_perf['total_time']:.2f}—Å\")\n",
    "        for stage_name, stage_time in production_perf.get('stages', []):\n",
    "            if stage_time > 0:\n",
    "                console.print(f\"     ‚Ä¢ {stage_name}: {stage_time:.2f}—Å\")\n",
    "else:\n",
    "    console.print(\"[yellow]‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ Production Pipeline –Ω–µ –Ω–∞–π–¥–µ–Ω—ã[/yellow]\")\n",
    "\n",
    "console.print(\"\\n[bold yellow]2. –ì–¥–µ —Å–∏—Å—Ç–µ–º–∞ –¥–∞—ë—Ç –≤—ã–∏–≥—Ä—ã—à:[/bold yellow]\")\n",
    "console.print(\"  ‚úÖ Production-–≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: —Ä–µ–∞–ª—å–Ω—ã–µ API (GigaChat, Qdrant Cloud)\")\n",
    "console.print(\"  ‚úÖ –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –ª–µ–≥–∫–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏\")\n",
    "console.print(\"  ‚úÖ –ü–æ–ª–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞: RAGAS –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ\")\n",
    "console.print(\"  ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å: Qdrant Cloud –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ –æ–±—ä—ë–º—ã –¥–∞–Ω–Ω—ã—Ö\")\n",
    "console.print(\"  ‚úÖ –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å: –≤–∏–¥–Ω–æ –≤—Å–µ —ç—Ç–∞–ø—ã pipeline –∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ\")\n",
    "\n",
    "if 'experiments_prod' in globals() and experiments_prod:\n",
    "    console.print(\"\\n[bold yellow]3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:[/bold yellow]\")\n",
    "    best_faith = max(experiments_prod, key=lambda x: x['faithfulness'])\n",
    "    best_relevancy = max(experiments_prod, key=lambda x: x['answer_relevancy'])\n",
    "    \n",
    "    console.print(f\"  üèÜ –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:\")\n",
    "    console.print(f\"     ‚Ä¢ –ü–æ Faithfulness: K={best_faith['config']['k']}, rerank={'on' if best_faith['config']['reranking'] else 'off'}\")\n",
    "    console.print(f\"     ‚Ä¢ –ü–æ Answer Relevancy: K={best_relevancy['config']['k']}, rerank={'on' if best_relevancy['config']['reranking'] else 'off'}\")\n",
    "    \n",
    "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ reranking\n",
    "    with_rerank = [e for e in experiments_prod if e['config']['reranking']]\n",
    "    without_rerank = [e for e in experiments_prod if not e['config']['reranking']]\n",
    "    if with_rerank and without_rerank:\n",
    "        avg_faith_with = sum(e['faithfulness'] for e in with_rerank) / len(with_rerank)\n",
    "        avg_faith_without = sum(e['faithfulness'] for e in without_rerank) / len(without_rerank)\n",
    "        console.print(f\"\\n  üìä –í–ª–∏—è–Ω–∏–µ reranking:\")\n",
    "        console.print(f\"     ‚Ä¢ –° reranking: Faithfulness = {avg_faith_with:.3f}\")\n",
    "        console.print(f\"     ‚Ä¢ –ë–µ–∑ reranking: Faithfulness = {avg_faith_without:.3f}\")\n",
    "        if avg_faith_with > avg_faith_without:\n",
    "            console.print(f\"     ‚úÖ Reranking —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ (+{avg_faith_with - avg_faith_without:.3f})\")\n",
    "        else:\n",
    "            console.print(f\"     ‚ö†Ô∏è  Reranking –Ω–µ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ\")\n",
    "\n",
    "console.print(\"\\n[bold yellow]4. –ö–∞–∫–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã:[/bold yellow]\")\n",
    "if 'result' in globals() and result:\n",
    "    metrics = result.get('metrics', {})\n",
    "    if metrics.get('faithfulness', 0) >= 0.85:\n",
    "        console.print(\"  ‚úÖ Faithfulness ‚â• 0.85 –¥–æ—Å—Ç–∏–∂–∏–º–æ —Å–æ —Å—Ç—Ä–æ–≥–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –≤ prompt\")\n",
    "    if metrics.get('answer_relevancy', 0) >= 0.80:\n",
    "        console.print(\"  ‚úÖ Answer Relevancy ‚â• 0.80 –¥–æ—Å—Ç–∏–∂–∏–º–æ —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º retrieval\")\n",
    "console.print(\"  ‚úÖ Production —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ API\")\n",
    "console.print(\"  ‚úÖ RAGAS –º–µ—Ç—Ä–∏–∫–∏ –¥–∞—é—Ç –æ–±—ä–µ–∫—Ç–∏–≤–Ω—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
    "console.print(\"  ‚úÖ –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å\")\n",
    "\n",
    "console.print(\"\\n[bold yellow]5. –ß—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∞—Ç—å –¥–∞–ª—å—à–µ:[/bold yellow]\")\n",
    "console.print(\"  üîÑ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è chunk_size –∏ overlap –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞\")\n",
    "console.print(\"  üîÑ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ embedding –º–æ–¥–µ–ª—è–º–∏\")\n",
    "console.print(\"  üîÑ –£–ª—É—á—à–µ–Ω–∏–µ reranking (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ cross-encoder –º–æ–¥–µ–ª–µ–π)\")\n",
    "console.print(\"  üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –¥–∞—Ç–µ)\")\n",
    "console.print(\"  üîÑ –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (Recall@K, MRR, NDCG)\")\n",
    "console.print(\"  üîÑ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –≤ production\")\n",
    "console.print(\"  üîÑ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\")\n",
    "console.print(\"  üîÑ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\")\n",
    "\n",
    "console.print(\"\\n[bold green]‚úÖ Production Pipeline –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞![/bold green]\")\n",
    "console.print(\"\\n–≠—Ç–æ—Ç notebook –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:\")\n",
    "console.print(\"  ‚Ä¢ –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≤ production —Ä–µ–∂–∏–º–µ\")\n",
    "console.print(\"  ‚Ä¢ –†–µ–∞–ª—å–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å GigaChat API –∏ Qdrant Cloud\")\n",
    "console.print(\"  ‚Ä¢ –û–±—ä–µ–∫—Ç–∏–≤–Ω—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ RAGAS\")\n",
    "console.print(\"  ‚Ä¢ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "console.print(\"  ‚Ä¢ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Telegram Bot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "\n",
    "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Production Pipeline –≤ Telegram-–±–æ—Ç –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ—Ä–µ–∑ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä.\n",
    "\n",
    "**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**\n",
    "- –í–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ Telegram\n",
    "- –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "- –ü—Ä–æ—Å—Ç–æ–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å\n",
    "\n",
    "**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –æ—à–∏–±–∫–∏ –∏–º–ø–æ—Ä—Ç–∞ aiogram, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\n",
    "```python\n",
    "!pip install --upgrade --force-reinstall aiogram pydantic\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Telegram Bot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "# ============================================\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è Telegram-–±–æ—Ç–∞\n",
    "# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Ä—Å–∏–∏\n",
    "!pip install -q \"pydantic>=2.0.0,<3.0.0\"  # –°–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è pydantic\n",
    "!pip install -q --upgrade aiogram  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é aiogram (—Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å pydantic 2.x)\n",
    "!pip install -q nest_asyncio  # –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ async –≤ Jupyter/Colab\n",
    "\n",
    "# –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ:\n",
    "# !pip install --upgrade --force-reinstall aiogram pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Telegram Bot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "# ============================================\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç—ã\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "# –ü—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å aiogram —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n",
    "try:\n",
    "    from aiogram import Bot, Dispatcher, F\n",
    "    from aiogram.filters import Command\n",
    "    from aiogram.types import Message\n",
    "    from aiogram.client.default import DefaultBotProperties\n",
    "    from aiogram.enums import ParseMode\n",
    "    AIOGRAM_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ aiogram: {e}\")\n",
    "    print(\"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å: !pip install --upgrade aiogram pydantic\")\n",
    "    AIOGRAM_AVAILABLE = False\n",
    "    # –°–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫\n",
    "    Bot = None\n",
    "    Dispatcher = None\n",
    "    F = None\n",
    "    Command = None\n",
    "    Message = None\n",
    "    DefaultBotProperties = None\n",
    "    ParseMode = None\n",
    "\n",
    "# –î–ª—è Colab (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Telegram Bot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "# ============================================\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∏–∑ Colab secrets –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "TELEGRAM_TOKEN = None\n",
    "try:\n",
    "    if IN_COLAB and 'userdata' in globals():\n",
    "        TELEGRAM_TOKEN = userdata.get(\"TELEGRAM_BOT_TOKEN\")\n",
    "    else:\n",
    "        import os\n",
    "        TELEGRAM_TOKEN = os.getenv(\"TELEGRAM_BOT_TOKEN\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å TELEGRAM_BOT_TOKEN: {e}\")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω)\n",
    "bot = None\n",
    "dp = None\n",
    "\n",
    "if TELEGRAM_TOKEN:\n",
    "    bot = Bot(token=TELEGRAM_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))\n",
    "    dp = Dispatcher()\n",
    "    \n",
    "    # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–ø—É—Å–∫–∞\n",
    "    bot_running = False\n",
    "    \n",
    "    @dp.message(Command(\"start\"))\n",
    "    async def cmd_start(message: Message):\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /start\"\"\"\n",
    "        await message.answer(\n",
    "            \"üëã –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Neuro Doc Assistant.\\n\\n\"\n",
    "            \"üìù –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å, –∏ —è –Ω–∞–π–¥—É –æ—Ç–≤–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.\\n\\n\"\n",
    "            \"–ö–æ–º–∞–Ω–¥—ã:\\n\"\n",
    "            \"/start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É\\n\"\n",
    "            \"/help - –ø–æ–º–æ—â—å\\n\"\n",
    "            \"/status - —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã\"\n",
    "        )\n",
    "    \n",
    "    @dp.message(Command(\"help\"))\n",
    "    async def cmd_help(message: Message):\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã /help\"\"\"\n",
    "        await message.answer(\n",
    "            \"‚ÑπÔ∏è <b>–ü–æ–º–æ—â—å</b>\\n\\n\"\n",
    "            \"–û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, –∏ —è:\\n\"\n",
    "            \"1. –ù–∞–π–¥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\\n\"\n",
    "            \"2. –°–≥–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\\n\"\n",
    "            \"3. –ü–æ–∫–∞–∂—É –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞\\n\\n\"\n",
    "            \"–ö–æ–º–∞–Ω–¥—ã:\\n\"\n",
    "            \"/start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É\\n\"\n",
    "            \"/help - —ç—Ç–∞ —Å–ø—Ä–∞–≤–∫–∞\\n\"\n",
    "            \"/status - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã\"\n",
    "        )\n",
    "    \n",
    "    @dp.message(Command(\"status\"))\n",
    "    async def cmd_status(message: Message):\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
    "        status_msg = \"‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ\\n\\n\"\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n",
    "        if 'run_production_pipeline' in globals():\n",
    "            status_msg += \"‚úÖ Production Pipeline –¥–æ—Å—Ç—É–ø–µ–Ω\\n\"\n",
    "        else:\n",
    "            status_msg += \"‚ö†Ô∏è Production Pipeline –Ω–µ –Ω–∞–π–¥–µ–Ω\\n\"\n",
    "        \n",
    "        if 'QDRANT_URL' in globals() and QDRANT_URL:\n",
    "            status_msg += \"‚úÖ Qdrant Cloud –ø–æ–¥–∫–ª—é—á–µ–Ω\\n\"\n",
    "        else:\n",
    "            status_msg += \"‚ö†Ô∏è Qdrant Cloud –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω\\n\"\n",
    "        \n",
    "        await message.answer(status_msg)\n",
    "    \n",
    "    @dp.message(F.text & ~F.text.startswith(\"/\"))\n",
    "    async def handle_query(message: Message):\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\"\"\"\n",
    "        user_query = message.text.strip()\n",
    "        \n",
    "        if not user_query:\n",
    "            await message.answer(\"‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –Ω–µ–ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å.\")\n",
    "            return\n",
    "        \n",
    "        # –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "        processing_msg = await message.answer(\"‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è.\")\n",
    "        \n",
    "        try:\n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è run_production_pipeline –¥–æ—Å—Ç—É–ø–Ω–∞\n",
    "            if 'run_production_pipeline' not in globals():\n",
    "                await processing_msg.edit_text(\n",
    "                    \"‚ùå –û—à–∏–±–∫–∞: Production Pipeline –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.\\n\"\n",
    "                    \"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫–∏ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –º–æ–¥—É–ª–µ–π.\"\n",
    "                )\n",
    "                return\n",
    "            \n",
    "            # –ó–∞–ø—É—Å–∫–∞–µ–º production pipeline\n",
    "            result = run_production_pipeline(user_query=user_query)\n",
    "            \n",
    "            if result is None:\n",
    "                await processing_msg.edit_text(\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.\")\n",
    "                return\n",
    "            \n",
    "            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç\n",
    "            answer = result.get('answer', '–û—Ç–≤–µ—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω')\n",
    "            metrics = result.get('metrics', {})\n",
    "            stats = result.get('statistics', {})\n",
    "            \n",
    "            # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç\n",
    "            response_text = f\"üí¨ <b>–í–∞—à –≤–æ–ø—Ä–æ—Å:</b>\\n{user_query}\\n\\n\"\n",
    "            response_text += f\"ü§ñ <b>–û—Ç–≤–µ—Ç:</b>\\n{answer}\\n\\n\"\n",
    "            \n",
    "            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "            if metrics:\n",
    "                response_text += \"üìä <b>–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:</b>\\n\"\n",
    "                faithfulness = metrics.get('faithfulness', 0)\n",
    "                relevancy = metrics.get('answer_relevancy', 0)\n",
    "                response_text += f\"  ‚Ä¢ Faithfulness: {faithfulness:.3f}\"\n",
    "                if faithfulness >= 0.85:\n",
    "                    response_text += \" ‚úÖ\\n\"\n",
    "                else:\n",
    "                    response_text += \" ‚ö†Ô∏è\\n\"\n",
    "                response_text += f\"  ‚Ä¢ Answer Relevancy: {relevancy:.3f}\"\n",
    "                if relevancy >= 0.80:\n",
    "                    response_text += \" ‚úÖ\\n\"\n",
    "                else:\n",
    "                    response_text += \" ‚ö†Ô∏è\\n\"\n",
    "                response_text += \"\\n\"\n",
    "            \n",
    "            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "            if stats:\n",
    "                response_text += \"üìà <b>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:</b>\\n\"\n",
    "                response_text += f\"  ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {stats.get('retrieved', 'N/A')}\\n\"\n",
    "                response_text += f\"  ‚Ä¢ –û—Ç–æ–±—Ä–∞–Ω–æ –ø–æ—Å–ª–µ reranking: {stats.get('reranked', 'N/A')}\\n\"\n",
    "                response_text += f\"  ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {stats.get('documents', 'N/A')}\\n\"\n",
    "            \n",
    "            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Telegram ~4096 —Å–∏–º–≤–æ–ª–æ–≤)\n",
    "            max_length = 4000\n",
    "            if len(response_text) > max_length:\n",
    "                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å\n",
    "                await processing_msg.edit_text(response_text[:max_length])\n",
    "                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º\n",
    "                await message.answer(response_text[max_length:])\n",
    "            else:\n",
    "                await processing_msg.edit_text(response_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}\", exc_info=True)\n",
    "            await processing_msg.edit_text(\n",
    "                f\"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞:\\n{str(e)}\\n\\n\"\n",
    "                \"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏.\"\n",
    "            )\n",
    "    \n",
    "    async def start_bot():\n",
    "        \"\"\"–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞\"\"\"\n",
    "        global bot_running\n",
    "        if bot_running:\n",
    "            logger.info(\"–ë–æ—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω\")\n",
    "            return\n",
    "        \n",
    "        if not bot or not dp:\n",
    "            logger.error(\"–ë–æ—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ TELEGRAM_BOT_TOKEN\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            bot_running = True\n",
    "            logger.info(\"–ó–∞–ø—É—Å–∫ Telegram-–±–æ—Ç–∞...\")\n",
    "            await dp.start_polling(bot, skip_updates=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}\", exc_info=True)\n",
    "            bot_running = False\n",
    "    \n",
    "    print(\"‚úÖ Telegram Bot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≥–æ—Ç–æ–≤–∞!\")\n",
    "    if TELEGRAM_TOKEN:\n",
    "        print(f\"‚úÖ –¢–æ–∫–µ–Ω –Ω–∞–π–¥–µ–Ω. –î–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: await start_bot()\")\n",
    "        print(f\"üí° –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: asyncio.run(start_bot())\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω.\")\n",
    "        print(\"üí° –î–ª—è —Ä–∞–±–æ—Ç—ã –±–æ—Ç–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–æ–∫–µ–Ω –≤ Colab Secrets (TELEGRAM_BOT_TOKEN)\")\n",
    "        print(\"   –∏–ª–∏ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è TELEGRAM_BOT_TOKEN\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Telegram Bot –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "    print(\"üí° –î–ª—è —Ä–∞–±–æ—Ç—ã –±–æ—Ç–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–æ–∫–µ–Ω –≤ Colab Secrets (TELEGRAM_BOT_TOKEN)\")\n",
    "    print(\"   –∏–ª–∏ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è TELEGRAM_BOT_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–ø—É—Å–∫ Telegram-–±–æ—Ç–∞\n",
    "\n",
    "**–î–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —è—á–µ–π–∫—É** (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω):\n",
    "\n",
    "**–í–∞–∂–Ω–æ:** –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏ –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —è—á–µ–π–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# –ó–∞–ø—É—Å–∫ Telegram-–±–æ—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "# ============================================\n",
    "\n",
    "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞:\n",
    "# await start_bot()\n",
    "\n",
    "# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ asyncio.run() –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤–Ω–µ async –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:\n",
    "# asyncio.run(start_bot())\n",
    "\n",
    "# –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –±–æ—Ç–∞ –Ω–∞–∂–º–∏—Ç–µ Stop –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ Jupyter/Colab\n",
    "\n",
    "print(\"üí° –î–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å—Ç—Ä–æ–∫—É —Å await start_bot() –∏–ª–∏ asyncio.run(start_bot())\")\n",
    "print(\"‚ö†Ô∏è  –ë–æ—Ç –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —è—á–µ–π–∫–∏ (Stop –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
