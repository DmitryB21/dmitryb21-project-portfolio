#!/usr/bin/env python
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º—É.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/add_document.py <path_to_file> [--category CATEGORY] [--update]
    
–ü—Ä–∏–º–µ—Ä—ã:
    python scripts/add_document.py data/new_doc.md --category hr
    python scripts/add_document.py data/updated_doc.md --category it --update
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

load_dotenv()

from app.storage.s3_storage import S3DocumentStorage
from app.storage.document_repository import DocumentRepository, DocumentMetadata
from app.ingestion.loader import DocumentLoader
from app.ingestion.chunker import Chunker
from app.ingestion.embedding_service import EmbeddingService
from app.ingestion.indexer import QdrantIndexer
from qdrant_client import QdrantClient


def get_mime_type(file_path: Path) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç MIME —Ç–∏–ø –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é —Ñ–∞–π–ª–∞"""
    mime_types = {
        '.md': 'text/markdown',
        '.txt': 'text/plain',
        '.pdf': 'application/pdf',
        '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
    }
    return mime_types.get(file_path.suffix.lower(), 'application/octet-stream')


def determine_category(file_path: Path, provided_category: Optional[str] = None) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    if provided_category:
        return provided_category.lower()
    
    path_str = str(file_path).lower()
    if '/hr/' in path_str or '\\hr\\' in path_str:
        return "hr"
    elif '/it/' in path_str or '\\it\\' in path_str:
        return "it"
    elif '/compliance/' in path_str or '\\compliance\\' in path_str:
        return "compliance"
    elif '/onboarding/' in path_str or '\\onboarding\\' in path_str:
        return "onboarding"
    
    return "unknown"


def delete_old_chunks(qdrant_client: QdrantClient, collection_name: str, doc_id: str) -> int:
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Qdrant"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–æ—á–∫–∏ —Å –¥–∞–Ω–Ω—ã–º doc_id
        points, _ = qdrant_client.scroll(
            collection_name=collection_name,
            scroll_filter={
                "must": [{"key": "doc_id", "match": {"value": doc_id}}]
            },
            limit=10000
        )
        
        if not points:
            return 0
        
        # –£–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏
        point_ids = [point.id for point in points]
        qdrant_client.delete(
            collection_name=collection_name,
            points_selector=point_ids
        )
        
        return len(point_ids)
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏: {e}")
        return 0


def add_document(
    file_path: Path,
    category: Optional[str] = None,
    update: bool = False
) -> int:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Å–∏—Å—Ç–µ–º—É.
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ (hr, it, compliance, onboarding)
        update: –ï—Å–ª–∏ True, –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
    
    Returns:
        0 –ø—Ä–∏ —É—Å–ø–µ—Ö–µ, 1 –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    print("=" * 80)
    print("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º—É")
    print("=" * 80)
    print()
    
    if not file_path.exists():
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return 1
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
    category = determine_category(file_path, category)
    if category == "unknown":
        print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'unknown'")
        print("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --category –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
    
    try:
        s3_storage = S3DocumentStorage()
        print("   ‚úÖ S3 —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ S3: {e}")
        return 1
    
    try:
        doc_repository = DocumentRepository()
        print("   ‚úÖ PostgreSQL")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: PostgreSQL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        doc_repository = None
    
    try:
        qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333"))
        )
        print("   ‚úÖ Qdrant")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ Qdrant: {e}")
        return 1
    
    loader = DocumentLoader(storage_backend="auto")
    chunker = Chunker()
    
    # Embedding service
    initial_embedding_dim = int(os.getenv("EMBEDDING_DIM", "1024"))
    gigachat_auth_key = os.getenv("GIGACHAT_AUTH_KEY")
    gigachat_scope = os.getenv("GIGACHAT_SCOPE", "GIGACHAT_API_PERS")
    use_mock_mode = not gigachat_auth_key or os.getenv("GIGACHAT_MOCK_MODE", "false").lower() == "true"
    
    embedding_service = EmbeddingService(
        model_version=os.getenv("EMBEDDING_MODEL_VERSION", "GigaChat"),
        embedding_dim=initial_embedding_dim,
        auth_key=gigachat_auth_key,
        scope=gigachat_scope,
        mock_mode=use_mock_mode
    )
    
    collection_name = os.getenv("QDRANT_COLLECTION", "neuro_docs")
    indexer = QdrantIndexer(
        qdrant_client=qdrant_client,
        collection_name=collection_name,
        embedding_dim=initial_embedding_dim
    )
    
    print()
    
    # –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3
    print("[1/7] –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3...")
    s3_key = f"{category}/{file_path.name}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    existing_doc = None
    if doc_repository:
        existing_doc = doc_repository.get_document_by_s3_key(s3_key)
    
    if existing_doc and not update:
        print(f"   ‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {s3_key}")
        print("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --update –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞")
        return 1
    
    if existing_doc and update:
        print(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {s3_key}")
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –∏–∑ Qdrant
        deleted_count = delete_old_chunks(qdrant_client, collection_name, existing_doc.id)
        if deleted_count > 0:
            print(f"   ‚úÖ –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ Qdrant: {deleted_count}")
    else:
        print(f"   üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {s3_key}")
    
    try:
        s3_uri = s3_storage.upload_document(file_path, s3_key)
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤ S3: {s3_uri}")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ S3: {e}")
        return 1
    
    # –≠—Ç–∞–ø 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL
    if doc_repository:
        print()
        print("[2/7] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL...")
        try:
            file_size = file_path.stat().st_size
            mime_type = get_mime_type(file_path)
            
            doc_metadata = DocumentMetadata(
                file_path=str(file_path),
                s3_key=s3_key,
                category=category,
                filename=file_path.name,
                file_size=file_size,
                mime_type=mime_type,
                version=(existing_doc.version + 1) if existing_doc and update else 1
            )
            
            doc_id = doc_repository.save_document(doc_metadata)
            print(f"   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã (ID: {doc_id})")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")
    
    # –≠—Ç–∞–ø 3: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
    print()
    print("[3/7] –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
    try:
        documents = loader.load_documents(s3_key, category=category)
        if not documents:
            print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")
            return 1
        doc = documents[0]
        print(f"   ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(doc.text)} —Å–∏–º–≤–æ–ª–æ–≤")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return 1
    
    # –≠—Ç–∞–ø 4: –ß–∞–Ω–∫–∏–Ω–≥
    print()
    print("[4/7] –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...")
    chunk_size = int(os.getenv("CHUNK_SIZE", "300"))
    overlap_percent = float(os.getenv("CHUNK_OVERLAP_PERCENT", "0.25"))
    
    chunks = chunker.chunk_documents(
        [doc],
        chunk_size=chunk_size,
        overlap_percent=overlap_percent
    )
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
    
    # –≠—Ç–∞–ø 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings
    print()
    print("[5/7] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings...")
    actual_mode = "Mock Embeddings" if embedding_service.mock_mode else "GigaChat Embeddings API"
    print(f"   –†–µ–∂–∏–º: {actual_mode}")
    
    chunk_texts = [chunk.text for chunk in chunks]
    
    try:
        embeddings = embedding_service.generate_embeddings(chunk_texts)
        final_embedding_dim = embedding_service.embedding_dim
        print(f"   ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ embeddings: {len(embeddings)} (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {final_embedding_dim})")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings: {e}")
        return 1
    
    # –≠—Ç–∞–ø 6: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant
    print()
    print("[6/7] –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant...")
    try:
        indexer.index_chunks(chunks, embeddings)
        print(f"   ‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ Qdrant: {len(chunks)} —á–∞–Ω–∫–æ–≤")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return 1
    
    # –≠—Ç–∞–ø 7: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    if doc_repository:
        print()
        print("[7/7] –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
        try:
            doc_repository.mark_as_indexed(s3_key, actual_mode, final_embedding_dim)
            print(f"   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã (indexed_at —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")
    
    print()
    print("=" * 80)
    print("‚úÖ –î–û–ö–£–ú–ï–ù–¢ –£–°–ü–ï–®–ù–û –î–û–ë–ê–í–õ–ï–ù")
    print("=" * 80)
    print()
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –§–∞–π–ª: {file_path.name}")
    print(f"   - S3 –∫–ª—é—á: {s3_key}")
    print(f"   - –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
    print(f"   - –ß–∞–Ω–∫–æ–≤: {len(chunks)}")
    print(f"   - Embeddings: {len(embeddings)} (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {final_embedding_dim})")
    print(f"   - –†–µ–∂–∏–º embeddings: {actual_mode}")
    print()
    
    return 0


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Å–∏—Å—Ç–µ–º—É Neuro_Doc_Assistant"
    )
    parser.add_argument(
        "file_path",
        type=Path,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è"
    )
    parser.add_argument(
        "--category",
        type=str,
        choices=["hr", "it", "compliance", "onboarding"],
        help="–ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"
    )
    parser.add_argument(
        "--update",
        action="store_true",
        help="–û–±–Ω–æ–≤–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç (—É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏)"
    )
    
    args = parser.parse_args()
    
    exit_code = add_document(
        file_path=args.file_path,
        category=args.category,
        update=args.update
    )
    
    sys.exit(exit_code)

