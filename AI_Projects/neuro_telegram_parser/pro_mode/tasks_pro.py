"""
Huey –∑–∞–¥–∞—á–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ Qdrant (Pro-—Ä–µ–∂–∏–º)
"""

import asyncio
import logging
from datetime import datetime
from typing import Optional

import asyncpg

from huey_config import huey
from config_utils import get_config
from pro_mode import pro_mode_service
from pro_mode.embedding_service import embedding_service
from pro_mode.topic_modeling_progress import TopicModelingProgressTracker

logger = logging.getLogger(__name__)


async def _fetch_messages(conn, limit: int = 1000, since: Optional[str] = None):
    where = []
    params = []
    param_count = 0
    
    # –ë–∞–∑–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è
    base_where = [
        "m.text_content IS NOT NULL",
        "m.text_content != ''",
        "LENGTH(m.text_content) >= 10"
    ]
    
    # –£—Å–ª–æ–≤–∏–µ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
    base_where.append("NOT EXISTS (SELECT 1 FROM embeddings e WHERE e.message_id = m.id)")
    
    if since:
        param_count += 1
        base_where.append(f"m.published_at >= ${param_count}")
        params.append(datetime.fromisoformat(since))
    
    where_sql = " AND ".join(base_where)
    
    param_count += 1
    limit_param = f"${param_count}"
    params.append(limit)
    
    sql = f"""
        SELECT m.id as message_id, m.channel_id, m.text_content as text, m.published_at
        FROM messages m
        WHERE {where_sql}
        ORDER BY m.published_at ASC
        LIMIT {limit_param}
    """
    logger.debug(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π: {sql[:200]}...")
    rows = await conn.fetch(sql, *params)
    logger.info(f"üìã –ü–æ–ª—É—á–µ–Ω–æ {len(rows)} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    return rows


async def _index_batch(limit: int = 1000, since: Optional[str] = None):
    config = get_config()
    # ensure initialized
    await pro_mode_service.initialize()
    dsn = config['postgresql']['dsn']
    conn = await asyncpg.connect(dsn=dsn)
    try:
        rows = await _fetch_messages(conn, limit=limit, since=since)
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –ø–æ–ª—É—á–µ–Ω–æ {len(rows)} —Å–æ–æ–±—â–µ–Ω–∏–π")
        
        if not rows:
            logger.info("–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return {"processed": 0, "indexed": 0}
        
        processed = 0
        indexed = 0
        
        for row in rows:
            text = row['text'] or ''
            if not text.strip():
                logger.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ {row['message_id']} - –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
                continue
            
            try:
                await embedding_service.process_message(
                    message_id=row['message_id'],
                    text=text,
                    channel_id=row['channel_id'],
                    published_at=(row['published_at'].isoformat() if row['published_at'] else None)
                )
                indexed += 1
                logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ {row['message_id']} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {row['message_id']}: {e}")
            
            processed += 1
            
            if processed % 10 == 0:
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{len(rows)} —Å–æ–æ–±—â–µ–Ω–∏–π, –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {indexed}")
        
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}, –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {indexed}")
        return {"processed": processed, "indexed": indexed}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ _index_batch: {e}")
        raise
    finally:
        await conn.close()


async def _index_batch_with_settings(settings: dict):
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
    config = get_config()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å—ã
    await pro_mode_service.initialize()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    batch_size = settings.get('batch_size', 50)
    limit = settings.get('limit', 1000)
    threads = settings.get('threads', 4)
    model = settings.get('model', 'sberbank-ai/sbert_large_nlu_ru')
    min_text_length = settings.get('min_text_length', 10)
    days_back = settings.get('days_back', 0)
    skip_existing = settings.get('skip_existing', True)
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏:")
    logger.info(f"   üìä –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
    logger.info(f"   üìà –õ–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π: {limit}")
    logger.info(f"   üßµ –ü–æ—Ç–æ–∫–æ–≤: {threads}")
    logger.info(f"   ü§ñ –ú–æ–¥–µ–ª—å: {model}")
    logger.info(f"   üìè –ú–∏–Ω. –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {min_text_length}")
    logger.info(f"   üìÖ –ü–µ—Ä–∏–æ–¥: {days_back if days_back > 0 else '–∑–∞ –≤—Å–µ –≤—Ä–µ–º—è'}")
    logger.info(f"   ‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ: {skip_existing}")
    
    dsn = config['postgresql']['dsn']
    conn = await asyncpg.connect(dsn=dsn)
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫
        rows = await _fetch_messages_with_settings(
            conn, 
            limit=limit, 
            min_text_length=min_text_length,
            days_back=days_back,
            skip_existing=skip_existing
        )
        
        total_messages = len(rows)
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {total_messages} —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        
        if not rows:
            logger.info("‚ÑπÔ∏è –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return {"processed": 0, "indexed": 0, "total": 0}
        
        processed = 0
        indexed = 0
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏
        for i in range(0, total_messages, batch_size):
            batch = rows[i:i + batch_size]
            logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1}/{(total_messages + batch_size - 1)//batch_size}")
            
            for row in batch:
                text = row['text'] or ''
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
                if len(text.strip()) < min_text_length:
                    logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ {row['message_id']} - —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π ({len(text)} < {min_text_length})")
                    processed += 1
                    continue
                
                try:
                    await embedding_service.process_message(
                        message_id=row['message_id'],
                        text=text,
                        channel_id=row['channel_id'],
                        published_at=(row['published_at'].isoformat() if row['published_at'] else None)
                    )
                    indexed += 1
                    logger.debug(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ {row['message_id']} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {row['message_id']}: {e}")
                
                processed += 1
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            progress_percent = round((processed / total_messages) * 100, 1)
            logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{total_messages} ({progress_percent}%), –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {indexed}")
        
        logger.info(f"üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        logger.info(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}")
        logger.info(f"   üîç –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {indexed}")
        logger.info(f"   üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {round((indexed/processed)*100, 1) if processed > 0 else 0}%")
        
        return {
            "processed": processed, 
            "indexed": indexed, 
            "total": total_messages,
            "settings": settings
        }
        
    except Exception as e:
        logger.error(f"üí• –û—à–∏–±–∫–∞ –≤ _index_batch_with_settings: {e}")
        raise
    finally:
        await conn.close()


async def _fetch_messages_with_settings(conn, limit: int = 1000, min_text_length: int = 10, days_back: int = 0, skip_existing: bool = True):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
    try:
        # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        query = """
            SELECT m.id as message_id, m.text_content as text, m.channel_id, m.published_at
            FROM messages m
            WHERE m.text_content IS NOT NULL 
            AND LENGTH(m.text_content) >= $1
        """
        params = [min_text_length]
        param_count = 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
        if days_back > 0:
            param_count += 1
            query += f" AND m.published_at >= NOW() - INTERVAL '{days_back} days'"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
        if skip_existing:
            query += """
                AND NOT EXISTS (
                    SELECT 1 FROM embeddings e 
                    WHERE e.message_id = m.message_id 
                    AND e.model = 'sberbank-ai/sbert_large_nlu_ru'
                )
            """
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –∏ –ª–∏–º–∏—Ç
        query += " ORDER BY m.published_at ASC"
        if limit > 0:
            query += f" LIMIT {limit}"
        
        logger.info(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å: {query[:100]}...")
        rows = await conn.fetch(query, *params)
        
        logger.info(f"üìã –ü–æ–ª—É—á–µ–Ω–æ {len(rows)} —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        return rows
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")
        raise


@huey.task()
def update_task_status(task_id: str, status: str, result: str = None, error: str = None):
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ –≤ Redis"""
    import redis
    from config_utils import get_config
    import time
    
    try:
        config = get_config()
        redis_client = redis.Redis(
            host=config['redis']['host'],
            port=int(config['redis']['port']),
            decode_responses=True
        )
        
        task_key = f"huey:telegram-parser:task:{task_id}"
        mapping = {
            'status': status,
            'completed_at': str(time.time())
        }
        
        if result:
            mapping['result'] = str(result)
        if error:
            mapping['error'] = str(error)
            
        redis_client.hset(task_key, mapping=mapping)
        redis_client.expire(task_key, 3600)
        logger.info(f"‚úÖ –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ {task_id} –æ–±–Ω–æ–≤–ª–µ–Ω –Ω–∞ '{status}'")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {task_id}: {e}")


@huey.task()
def index_batch_task(settings: dict):
    """Huey –∑–∞–¥–∞—á–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
    task_id = None
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º task_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ Huey –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º
        try:
            if hasattr(huey, '_current_task') and huey._current_task:
                task_id = str(huey._current_task.id)
                logger.info(f"üîç –ü–æ–ª—É—á–µ–Ω task_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ Huey: {task_id}")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å task_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ Huey")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è task_id –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ Huey: {e}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
        result = asyncio.run(_index_batch_with_settings(settings))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        if task_id:
            update_indexing_status(task_id, 'finished', result=str(result))
            logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ {task_id}")
        else:
            logger.warning("‚ö†Ô∏è task_id –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å—Ç–∞—Ç—É—Å –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω")
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ: {result}")
        
        return result
        
    except Exception as e:
        # –ü—Ä–∏ –æ—à–∏–±–∫–µ —Ç–∞–∫–∂–µ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if task_id:
            update_indexing_status(task_id, 'error', error=str(e))
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á–∏ {task_id}: {e}")
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        
        raise


@huey.task()
def update_indexing_status(task_id: str, status: str, result: str = None, error: str = None):
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ Redis"""
    import redis
    from config_utils import get_config
    import time
    
    try:
        config = get_config()
        redis_client = redis.Redis(
            host=config['redis']['host'],
            port=int(config['redis']['port']),
            decode_responses=True
        )
        
        task_key = f"huey:telegram-parser:task:{task_id}"
        mapping = {
            'status': status,
            'completed_at': str(time.time())
        }
        
        if result:
            mapping['result'] = str(result)
        if error:
            mapping['error'] = str(error)
            
        redis_client.hset(task_key, mapping=mapping)
        redis_client.expire(task_key, 3600)
        logger.info(f"‚úÖ –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ {task_id} –æ–±–Ω–æ–≤–ª–µ–Ω –Ω–∞ '{status}'")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {task_id}: {e}")


@huey.task()
def check_task_completion(task_id: str):
    """–ó–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞"""
    import redis
    from config_utils import get_config
    import time
    
    try:
        config = get_config()
        redis_client = redis.Redis(
            host=config['redis']['host'],
            port=int(config['redis']['port']),
            decode_responses=True
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –≤ Huey
        task_key = f"huey:telegram-parser:task:{task_id}"
        task_data = redis_client.hgetall(task_key)
        
        if not task_data:
            logger.warning(f"‚ö†Ô∏è –ó–∞–¥–∞—á–∞ {task_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Redis")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ —á–µ—Ä–µ–∑ Redis –Ω–∞–ø—Ä—è–º—É—é
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∑–∞–¥–∞—á–∏
            possible_result_keys = [
                f"huey:telegram-parser:result:{task_id}",
                f"huey:result:{task_id}",
                f"huey:telegram-parser:{task_id}",
                f"huey:{task_id}"
            ]
            
            result_found = False
            for result_key in possible_result_keys:
                result_data = redis_client.get(result_key)
                if result_data:
                    # –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ
                    update_indexing_status(task_id, 'finished', result=result_data)
                    logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ: {result_data}")
                    result_found = True
                    break
            
            if not result_found:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –æ—à–∏–±–∫–∏
                possible_error_keys = [
                    f"huey:telegram-parser:error:{task_id}",
                    f"huey:error:{task_id}"
                ]
                
                error_found = False
                for error_key in possible_error_keys:
                    error_data = redis_client.get(error_key)
                    if error_data:
                        update_indexing_status(task_id, 'error', error=error_data)
                        logger.error(f"‚ùå –ó–∞–¥–∞—á–∞ {task_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–æ–π: {error_data}")
                        error_found = True
                        break
                
                if not error_found:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–¥–∞—á–∞ –≤ –æ—á–µ—Ä–µ–¥–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                    # –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∏ –æ—à–∏–±–∫–∞—Ö, –≤–æ–∑–º–æ–∂–Ω–æ –æ–Ω–∞ –µ—â–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
                    # –ù–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –≤–æ–∑–º–æ–∂–Ω–æ –æ–Ω–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å
                    current_time = time.time()
                    started_at = float(task_data.get('started_at', current_time))
                    elapsed_time = current_time - started_at
                    
                    if elapsed_time > 300:  # 5 –º–∏–Ω—É—Ç
                        # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ 5 –º–∏–Ω—É—Ç, —Å—á–∏—Ç–∞–µ–º –∑–∞–¥–∞—á—É –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π
                        update_indexing_status(task_id, 'finished', result="–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ (—Ç–∞–π–º–∞—É—Ç)")
                        logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ —Ç–∞–π–º–∞—É—Ç—É")
                    else:
                        # –ó–∞–¥–∞—á–∞ –µ—â–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è - –ø–ª–∞–Ω–∏—Ä—É–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥
                        logger.info(f"üîÑ –ó–∞–¥–∞—á–∞ {task_id} –µ—â–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è ({elapsed_time:.1f}—Å), –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ 5 —Å–µ–∫")
                        try:
                            check_task_completion.schedule(args=(task_id,), delay=5)
                        except Exception as e:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É: {e}")
                        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {task_id}: {e}")
            # Fallback - –ø–ª–∞–Ω–∏—Ä—É–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            try:
                check_task_completion.schedule(args=(task_id,), delay=5)
            except Exception as e2:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É: {e2}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {task_id}: {e}")


@huey.task()
def reprocess_deduplication_task(threshold: float = 0.75, limit: int = 1000):
    """–ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    return asyncio.run(_reprocess_deduplication(threshold=threshold, limit=limit))

@huey.task()
def reclassify_messages_task(threshold: float = 0.8, limit: int = 1000):
    """–ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º"""
    return asyncio.run(_reclassify_messages(threshold=threshold, limit=limit))


async def _reprocess_deduplication(threshold: float = 0.75, limit: int = 1000):
    """–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    try:
        from pro_mode.deduplication_service import deduplication_service
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ —Å –ø–æ—Ä–æ–≥–æ–º {threshold} –∏ –ª–∏–º–∏—Ç–æ–º {limit}")
        
        result = await deduplication_service.reprocess_all_messages(threshold, limit)
        
        logger.info(f"–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")
        raise

async def _reclassify_messages(threshold: float = 0.8, limit: int = 1000):
    """–ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º"""
    try:
        from pro_mode.classification_service import classification_service
        import asyncpg
        from config_utils import get_config
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –ø–æ—Ä–æ–≥–æ–º {threshold} –∏ –ª–∏–º–∏—Ç–æ–º {limit}")
        
        config = get_config()
        conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
        
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        await conn.execute("DELETE FROM message_topics")
        logger.info("–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—á–∏—â–µ–Ω—ã")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        messages = await conn.fetch("""
            SELECT m.id, m.text_content 
            FROM messages m 
            WHERE m.text_content IS NOT NULL 
            AND LENGTH(m.text_content) > 10
            ORDER BY m.id DESC
            LIMIT $1
        """, limit)
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        
        processed = 0
        classified = 0
        
        for message in messages:
            try:
                message_id = message['id']
                text = message['text_content']
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                classifications = await classification_service.classify_message(message_id, text)
                
                if classifications:
                    classified += 1
                
                processed += 1
                
                if processed % 50 == 0:
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {message['id']}: {e}")
                continue
        
        await conn.close()
        
        result = {
            'processed': processed,
            'classified': classified,
            'threshold': threshold
        }
        
        logger.info(f"–ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
        raise


@huey.task()
def index_messages_batch(limit: int = 1000, since: Optional[str] = None):
    """–ü–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –±–∞—Ç—á–∞ —Å–æ–æ–±—â–µ–Ω–∏–π"""
    return asyncio.run(_index_batch(limit=limit, since=since))


@huey.task()
def index_new_messages_worker():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç)"""
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ ~1000 –ø–æ –¥–∞—Ç–µ
    return asyncio.run(_index_batch(limit=1000))


# ============================================================================
# TOPIC MODELING TASKS
# ============================================================================

@huey.task()
def run_topic_modeling_pipeline(limit: Optional[int] = None, days_back: int = 30, run_classification: bool = True):
    """
    –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Args:
        limit: –õ–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
        days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å—Ç–æ–≤
        run_classification: –ó–∞–ø—É—Å–∫–∞—Ç—å –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    try:
        from pro_mode.topic_modeling_service import TopicModelingService
        
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ Huey")
        logger.info(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: limit={limit}, days_back={days_back}, run_classification={run_classification}")

        task_id = None
        try:
            if hasattr(huey, '_current_task') and huey._current_task:
                task_id = str(huey._current_task.id)
        except Exception:
            task_id = None
        progress_tracker = TopicModelingProgressTracker(task_id)
        
        async def _run_pipeline():
            service = TopicModelingService(progress_tracker=progress_tracker)
            
            # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            original_fetch = service._fetch_posts_from_db
            async def fetch_with_params(limit_param=None, days_back_param=30, **kwargs):
                effective_limit = limit if limit is not None else (limit_param if limit_param is not None else kwargs.get("limit"))
                effective_days = days_back if days_back != 30 else kwargs.get("days_back", days_back_param)
                return await original_fetch(
                    limit=effective_limit,
                    days_back=effective_days
                )
            service._fetch_posts_from_db = fetch_with_params
            
            result = await service.run_full_pipeline(fetch_from_db=True, run_classification=run_classification)
            return result
        
        result = asyncio.run(_run_pipeline())
        
        logger.info("‚úÖ –ó–∞–¥–∞—á–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise


