"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å Pro-—Ä–µ–∂–∏–º–∞
–ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –∏ API endpoints
"""

import asyncio
import logging
import json
import statistics
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, timezone
import asyncpg
from config_utils import get_config
logger = logging.getLogger(__name__)

class ProModeService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å Pro-—Ä–µ–∂–∏–º–∞"""
    
    def __init__(self):
        self.initialized = False
        self._embedding_service = None
        self._deduplication_service = None
        self._classification_service = None
        self._onboarding_service = None
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
        try:
            await self._get_embedding_service().initialize()
            self.initialized = True
            logger.info("Pro-—Ä–µ–∂–∏–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Pro-—Ä–µ–∂–∏–º–∞: {e}")
            raise

    def initialize_sync(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (—É–¥–æ–±–Ω–æ –¥–ª—è —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á)"""
        if not self.initialized:
            import asyncio as _asyncio
            _asyncio.run(self.initialize())
    
    async def process_new_message(self, message_id: int, text: str, channel_id: int, 
                                published_at: datetime) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è"""
        if not self.initialized:
            await self.initialize()
        
        try:
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞–º
            classifications = await self._get_classification_service().classify_message(message_id, text)
            
            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π
            cluster_id = await self._get_deduplication_service().process_new_message(
                message_id, text, channel_id, published_at
            )
            
            return {
                'message_id': message_id,
                'classifications': classifications,
                'cluster_id': cluster_id,
                'processed_at': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}: {e}")
            raise
    
    async def search_semantic(self, query: str, filters: Optional[Dict[str, Any]] = None, 
                            limit: int = 20) -> List[Dict[str, Any]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
        if not self.initialized:
            await self.initialize()
        
        try:
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Qdrant
            results = await self._get_embedding_service().search_semantic(query, limit, filters)
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ –ë–î
            enriched_results = []
            for result in results:
                payload = result.get('payload', {})
                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∞–∫ post_id (–∏–∑ posts_search), —Ç–∞–∫ –∏ message_id (–∏–∑ —Å—Ç–∞—Ä–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏)
                message_id = payload.get('message_id') or payload.get('post_id')
                
                if not message_id:
                    logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ message_id/post_id: {payload}")
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–æ–±—â–µ–Ω–∏–∏
                message_info = await self._get_message_info(message_id)
                if message_info:
                    enriched_results.append({
                        'message_id': message_id,
                        'score': result['score'],
                        'text': message_info['text'],
                        'channel_name': message_info['channel_name'],
                        'published_at': message_info['published_at'],
                        'views': message_info['views'],
                        'forwards': message_info['forwards'],
                        'topics': message_info['topics']
                    })
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ë–î, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ payload
                    text_preview = payload.get('text', '')[:500] if payload.get('text') else ''
                    enriched_results.append({
                        'message_id': message_id,
                        'score': result['score'],
                        'text': text_preview,
                        'channel_name': None,
                        'published_at': payload.get('timestamp'),
                        'views': None,
                        'forwards': None,
                        'topics': []
                    })
            
            return enriched_results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            raise
    
    async def get_trending_events(self, period: str = 'daily', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è (–∫–ª–∞—Å—Ç–µ—Ä—ã) —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
            if period == 'daily':
                date_from = datetime.now() - timedelta(days=1)
                prev_period_from = datetime.now() - timedelta(days=2)
            elif period == 'weekly':
                date_from = datetime.now() - timedelta(weeks=1)
                prev_period_from = datetime.now() - timedelta(weeks=2)
            elif period == 'monthly':
                date_from = datetime.now() - timedelta(days=30)
                prev_period_from = datetime.now() - timedelta(days=60)
            else:
                date_from = datetime.now() - timedelta(days=1)
                prev_period_from = datetime.now() - timedelta(days=2)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è —Å –¥–∏–Ω–∞–º–∏–∫–æ–π
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å–æ–±—ã—Ç–∏—è–º (dedup_clusters), –∞ –Ω–µ –ø–æ —Ç–µ–º–∞–º
            query = """
                WITH current_period AS (
                    SELECT 
                        dc.cluster_id,
                        dc.id as event_id,
                        dc.title, 
                        dc.summary,
                        dc.primary_topic_id,
                        COALESCE(t.name, '–ë–µ–∑ —Ç–µ–º—ã') as topic_name,
                        COALESCE(t.color, '#808080') as topic_color,
                        COUNT(DISTINCT cm.message_id) as message_count,
                        AVG(cm.similarity_score) as avg_similarity,
                        COUNT(DISTINCT m.channel_id) as channel_count,
                        SUM(COALESCE(m.views_count, 0)) as total_views,
                        SUM(COALESCE(m.forwards_count, 0)) as total_forwards,
                        MIN(m.published_at) as first_mention_at,
                        MAX(m.published_at) as last_mention_at
                    FROM dedup_clusters dc
                    JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                    JOIN messages m ON cm.message_id = m.id
                    LEFT JOIN topics t ON dc.primary_topic_id = t.id
                    WHERE m.published_at >= $1
                    GROUP BY dc.cluster_id, dc.id, dc.title, dc.summary, dc.primary_topic_id, t.name, t.color
                    HAVING COUNT(DISTINCT cm.message_id) >= 2  -- –ú–∏–Ω–∏–º—É–º 2 —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∑–Ω–∞—á–∏–º–æ–≥–æ —Å–æ–±—ã—Ç–∏—è
                ),
                previous_period AS (
                    SELECT 
                        dc.cluster_id,
                        COUNT(DISTINCT cm.message_id) as prev_message_count,
                        AVG(cm.similarity_score) as prev_avg_similarity,
                        COUNT(DISTINCT m.channel_id) as prev_channel_count,
                        SUM(COALESCE(m.views_count, 0)) as prev_total_views,
                        SUM(COALESCE(m.forwards_count, 0)) as prev_total_forwards
                    FROM dedup_clusters dc
                    JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                    JOIN messages m ON cm.message_id = m.id
                    WHERE m.published_at >= $2 AND m.published_at < $1
                    GROUP BY dc.cluster_id
                )
                SELECT 
                    cp.cluster_id,
                    cp.event_id,
                    cp.title, 
                    cp.summary,
                    cp.primary_topic_id,
                    cp.topic_name,
                    cp.topic_color,
                    cp.message_count, 
                    cp.avg_similarity, 
                    cp.channel_count,
                    cp.total_views, 
                    cp.total_forwards,
                    cp.first_mention_at,
                    cp.last_mention_at,
                    COALESCE(pp.prev_message_count, 0) as prev_message_count,
                    COALESCE(pp.prev_avg_similarity, 0) as prev_avg_similarity,
                    COALESCE(pp.prev_channel_count, 0) as prev_channel_count,
                    COALESCE(pp.prev_total_views, 0) as prev_total_views,
                    COALESCE(pp.prev_total_forwards, 0) as prev_total_forwards,
                    CASE 
                        WHEN COALESCE(pp.prev_message_count, 0) = 0 THEN 100.0
                        ELSE CAST(((cp.message_count::float - pp.prev_message_count::float) / pp.prev_message_count::float) * 100::float AS NUMERIC(10,1))
                    END as growth_percentage,
                    CASE 
                        WHEN COALESCE(pp.prev_avg_similarity, 0) = 0 THEN 0.0
                        ELSE CAST((cp.avg_similarity - pp.prev_avg_similarity)::float AS NUMERIC(10,3))
                    END as similarity_change
                FROM current_period cp
                LEFT JOIN previous_period pp ON cp.cluster_id = pp.cluster_id
                ORDER BY cp.message_count DESC, cp.total_views DESC
                LIMIT $3
            """
            
            rows = await conn.fetch(query, date_from, prev_period_from, limit)
            
            trends = []
            for row in rows:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–¥ (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ/—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
                if row['growth_percentage'] > 10:
                    trend_direction = 'up'
                    trend_icon = 'üìà'
                elif row['growth_percentage'] < -10:
                    trend_direction = 'down'
                    trend_icon = 'üìâ'
                else:
                    trend_direction = 'stable'
                    trend_icon = '‚û°Ô∏è'
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å (–∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
                # –î–ª—è —Å–æ–±—ã—Ç–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–∏–µ –≤–µ—Å–∞: –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
                popularity_score = (
                    row['message_count'] * 0.5 +  # –í–µ—Å 50% - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –æ —Å–æ–±—ã—Ç–∏–∏
                    min(row['total_views'] / 10000, 100) * 0.3 +  # –í–µ—Å 30% - –æ—Ö–≤–∞—Ç
                    row['channel_count'] * 2 * 0.15 +  # –í–µ—Å 15% - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (√ó2 –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è)
                    float(row['avg_similarity']) * 100 * 0.05  # –í–µ—Å 5% - –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                )
                
                trends.append({
                    'cluster_id': row['cluster_id'],
                    'event_id': row['event_id'],
                    'title': row['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
                    'summary': row['summary'] or '',
                    'topic_id': row['primary_topic_id'],
                    'topic_name': row['topic_name'],
                    'topic_color': row['topic_color'],
                    'message_count': row['message_count'],
                    'avg_similarity': float(row['avg_similarity']),
                    'channel_count': row['channel_count'],
                    'total_views': row['total_views'],
                    'total_forwards': row['total_forwards'],
                    'first_mention_at': row['first_mention_at'].isoformat() if row['first_mention_at'] else None,
                    'last_mention_at': row['last_mention_at'].isoformat() if row['last_mention_at'] else None,
                    'growth_percentage': float(row['growth_percentage']),
                    'similarity_change': float(row['similarity_change']),
                    'trend_direction': trend_direction,
                    'trend_icon': trend_icon,
                    'popularity_score': round(popularity_score, 1),
                    'prev_message_count': row['prev_message_count'],
                    'prev_avg_similarity': float(row['prev_avg_similarity']),
                    'prev_channel_count': row['prev_channel_count']
                })
            
            await conn.close()
            return trends
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤ —Å–æ–±—ã—Ç–∏–π: {e}")
            raise
    
    async def get_trending_events_with_spikes(self, window_hours: int = 6, z_threshold: float = 2.0, limit: int = 20) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è (–∫–ª–∞—Å—Ç–µ—Ä—ã) —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –≤—Å–ø–ª–µ—Å–∫–æ–≤ –∏–Ω—Ç–µ—Ä–µ—Å–∞
        
        –õ–æ–≥–∏–∫–∞:
        - –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ window_hours —á–∞—Å–æ–≤
        - –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π —Å–æ–±—ã—Ç–∏—è –≤ –∫–∞–∂–¥–æ–º –æ–∫–Ω–µ
        - –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–∏–ª—å–Ω–æ –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ (z-score > threshold), –ø–æ–º–µ—á–∞–µ–º –≤—Å–ø–ª–µ—Å–∫
        - –†–∞–Ω–∂–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è: –≤—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ –≤—ã—à–µ
        
        Args:
            window_hours: –†–∞–∑–º–µ—Ä —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –≤ —á–∞—Å–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 6)
            z_threshold: –ü–æ—Ä–æ–≥ z-score –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Å–ø–ª–µ—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2.0)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–±—ã—Ç–∏–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Å–ø–ª–µ—Å–∫–∞—Ö
        """
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏)
            now = datetime.now(timezone.utc)
            analysis_start = now - timedelta(days=7)
            current_window_start = now - timedelta(hours=window_hours)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–æ–±—ã—Ç–∏—è (–∫–ª–∞—Å—Ç–µ—Ä—ã) —Å –∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
            query = """
                SELECT 
                    dc.cluster_id,
                    dc.id as event_id,
                    dc.title,
                    dc.summary,
                    dc.primary_topic_id,
                    COALESCE(t.name, '–ë–µ–∑ —Ç–µ–º—ã') as topic_name,
                    COALESCE(t.color, '#808080') as topic_color,
                    m.id as message_id,
                    m.published_at
                FROM dedup_clusters dc
                JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                JOIN messages m ON cm.message_id = m.id
                LEFT JOIN topics t ON dc.primary_topic_id = t.id
                WHERE m.published_at >= $1
                ORDER BY dc.cluster_id, m.published_at
            """
            
            rows = await conn.fetch(query, analysis_start)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ —Å–æ–±—ã—Ç–∏—è–º (–∫–ª–∞—Å—Ç–µ—Ä–∞–º)
            events_data = {}
            for row in rows:
                cluster_id = row['cluster_id']
                if cluster_id not in events_data:
                    events_data[cluster_id] = {
                        'cluster_id': cluster_id,
                        'event_id': row['event_id'],
                        'title': row['title'],
                        'summary': row['summary'],
                        'primary_topic_id': row['primary_topic_id'],
                        'topic_name': row['topic_name'],
                        'topic_color': row['topic_color'],
                        'messages': []
                    }
                # –ü—Ä–∏–≤–æ–¥–∏–º datetime –∫ timezone-aware, –µ—Å–ª–∏ –æ–Ω naive
                msg_time = row['published_at']
                if msg_time.tzinfo is None:
                    msg_time = msg_time.replace(tzinfo=timezone.utc)
                events_data[cluster_id]['messages'].append(msg_time)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–∞—Ö –∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
            channel_query = """
                SELECT 
                    dc.cluster_id,
                    COUNT(DISTINCT m.channel_id) as channel_count,
                    SUM(COALESCE(m.views_count, 0)) as total_views,
                    SUM(COALESCE(m.forwards_count, 0)) as total_forwards,
                    AVG(cm.similarity_score) as avg_similarity
                FROM dedup_clusters dc
                JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                JOIN messages m ON cm.message_id = m.id
                WHERE m.published_at >= $1
                GROUP BY dc.cluster_id
            """
            channel_rows = await conn.fetch(channel_query, current_window_start)
            channel_info = {
                row['cluster_id']: {
                    'channel_count': row['channel_count'],
                    'total_views': row['total_views'],
                    'total_forwards': row['total_forwards'],
                    'avg_similarity': float(row['avg_similarity']) if row['avg_similarity'] is not None else 0.0
                }
                for row in channel_rows
            }
            await conn.close()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–æ–±—ã—Ç–∏–µ –Ω–∞ –≤—Å–ø–ª–µ—Å–∫–∏
            results = []
            window_delta = timedelta(hours=window_hours)
            
            for cluster_id, event_info in events_data.items():
                messages = sorted(event_info['messages'])
                
                if len(messages) < 3:  # –ú–∏–Ω–∏–º—É–º 3 —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    continue
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –Ω–∞ –æ–∫–Ω–∞ –∏ —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–∞–∂–¥–æ–º
                window_counts = []
                current_time = analysis_start
                
                while current_time < now:
                    window_end = current_time + window_delta
                    count = sum(1 for msg_time in messages if current_time <= msg_time < window_end)
                    if count > 0:  # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–∫–Ω–∞ —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
                        window_counts.append(count)
                    current_time += timedelta(hours=1)  # –°–¥–≤–∏–≥–∞–µ–º –æ–∫–Ω–æ –Ω–∞ 1 —á–∞—Å
                
                if len(window_counts) < 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 –æ–∫–Ω–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                mean_count = statistics.mean(window_counts)
                if len(window_counts) > 1:
                    stdev_count = statistics.stdev(window_counts)
                else:
                    stdev_count = 0.0
                
                # –¢–µ–∫—É—â–µ–µ –æ–∫–Ω–æ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ window_hours —á–∞—Å–æ–≤)
                current_count = sum(1 for msg_time in messages if msg_time >= current_window_start)
                
                # –í—ã—á–∏—Å–ª—è–µ–º z-score –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –æ–∫–Ω–∞
                if stdev_count > 0:
                    z_score = (current_count - mean_count) / stdev_count
                else:
                    z_score = 0.0
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–ø–ª–µ—Å–∫
                is_spike = z_score > z_threshold
                
                # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ–±—ã—Ç–∏—é
                total_messages = len(messages)
                recent_messages = current_count
                
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–∞—Ö –∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
                info = channel_info.get(cluster_id, {
                    'channel_count': 0,
                    'total_views': 0,
                    'total_forwards': 0,
                    'avg_similarity': 0.0
                })
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å (—Å –±–æ–Ω—É—Å–æ–º –∑–∞ –≤—Å–ø–ª–µ—Å–∫)
                popularity_score = (
                    total_messages * 0.4 +
                    recent_messages * 0.3 +
                    info['channel_count'] * 2 * 0.2 +
                    (z_score * 10 if is_spike else 0) * 0.1  # –ë–æ–Ω—É—Å –∑–∞ –≤—Å–ø–ª–µ—Å–∫
                )
                
                results.append({
                    'cluster_id': cluster_id,
                    'event_id': event_info['event_id'],
                    'title': event_info['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
                    'summary': event_info['summary'] or '',
                    'topic_id': event_info['primary_topic_id'],
                    'topic_name': event_info['topic_name'],
                    'topic_color': event_info['topic_color'],
                    'total_messages': total_messages,
                    'recent_messages': recent_messages,  # –í –ø–æ—Å–ª–µ–¥–Ω–µ–º –æ–∫–Ω–µ
                    'channel_count': info['channel_count'],
                    'total_views': info['total_views'],
                    'total_forwards': info['total_forwards'],
                    'avg_similarity': round(info['avg_similarity'], 3),
                    'mean_count': round(mean_count, 2),
                    'stdev_count': round(stdev_count, 2),
                    'current_count': current_count,
                    'z_score': round(z_score, 2),
                    'is_spike': is_spike,
                    'spike_intensity': round(z_score, 2) if is_spike else 0.0,
                    'popularity_score': round(popularity_score, 1),
                    'window_hours': window_hours,
                    'first_mention_at': messages[0].isoformat() if messages else None,
                    'last_mention_at': messages[-1].isoformat() if messages else None
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –≤—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ (–ø–æ z-score), –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ (–ø–æ popularity_score)
            results.sort(key=lambda x: (
                not x['is_spike'],  # –í—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏
                -x['z_score'] if x['is_spike'] else -x['popularity_score']  # –ü–æ —É–±—ã–≤–∞–Ω–∏—é
            ))
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π —Å–æ –≤—Å–ø–ª–µ—Å–∫–∞–º–∏: {e}", exc_info=True)
            raise
    
    async def get_trending_topics_with_spikes(self, window_hours: int = 6, z_threshold: float = 2.0, limit: int = 20) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Ç–µ–º—ã —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –≤—Å–ø–ª–µ—Å–∫–æ–≤ –∏–Ω—Ç–µ—Ä–µ—Å–∞
        
        –õ–æ–≥–∏–∫–∞:
        - –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ window_hours —á–∞—Å–æ–≤
        - –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ —Ç–µ–º—ã –≤ –∫–∞–∂–¥–æ–º –æ–∫–Ω–µ
        - –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–∏–ª—å–Ω–æ –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ (z-score > threshold), –ø–æ–º–µ—á–∞–µ–º –≤—Å–ø–ª–µ—Å–∫
        - –†–∞–Ω–∂–∏—Ä—É–µ–º —Ç–µ–º—ã: –≤—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ –≤—ã—à–µ
        
        Args:
            window_hours: –†–∞–∑–º–µ—Ä —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –≤ —á–∞—Å–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 6)
            z_threshold: –ü–æ—Ä–æ–≥ z-score –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Å–ø–ª–µ—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2.0)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–º —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Å–ø–ª–µ—Å–∫–∞—Ö
        """
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π (–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –ë–î
            now = datetime.now(timezone.utc)
            analysis_start = now - timedelta(days=7)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–µ–º—ã —Å –∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
            query = """
                SELECT 
                    t.id as topic_id,
                    t.name as topic_name,
                    t.description as topic_description,
                    t.color as topic_color,
                    m.id as message_id,
                    m.published_at
                FROM topics t
                JOIN message_topics mt ON t.id = mt.topic_id
                JOIN messages m ON mt.message_id = m.id
                WHERE m.published_at >= $1
                ORDER BY t.id, m.published_at
            """
            
            rows = await conn.fetch(query, analysis_start)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ —Ç–µ–º–∞–º
            topics_data = {}
            for row in rows:
                topic_id = row['topic_id']
                if topic_id not in topics_data:
                    topics_data[topic_id] = {
                        'topic_id': topic_id,
                        'topic_name': row['topic_name'],
                        'topic_description': row['topic_description'],
                        'topic_color': row['topic_color'],
                        'messages': []
                    }
                # –ü—Ä–∏–≤–æ–¥–∏–º datetime –∫ timezone-aware, –µ—Å–ª–∏ –æ–Ω naive
                msg_time = row['published_at']
                if msg_time.tzinfo is None:
                    # –ï—Å–ª–∏ naive, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º UTC
                    msg_time = msg_time.replace(tzinfo=timezone.utc)
                topics_data[topic_id]['messages'].append(msg_time)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–∞—Ö –¥–ª—è –≤—Å–µ—Ö —Ç–µ–º –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
            current_window_start = now - timedelta(hours=window_hours)
            channel_query = """
                SELECT 
                    mt.topic_id,
                    COUNT(DISTINCT m.channel_id) as channel_count
                FROM message_topics mt
                JOIN messages m ON mt.message_id = m.id
                WHERE m.published_at >= $1
                GROUP BY mt.topic_id
            """
            channel_rows = await conn.fetch(channel_query, current_window_start)
            channel_counts = {row['topic_id']: row['channel_count'] for row in channel_rows}
            await conn.close()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Ç–µ–º—É –Ω–∞ –≤—Å–ø–ª–µ—Å–∫–∏
            results = []
            window_delta = timedelta(hours=window_hours)
            
            for topic_id, topic_info in topics_data.items():
                messages = sorted(topic_info['messages'])
                
                if len(messages) < 3:  # –ú–∏–Ω–∏–º—É–º 3 —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    continue
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –Ω–∞ –æ–∫–Ω–∞ –∏ —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–∞–∂–¥–æ–º
                window_counts = []
                current_time = analysis_start
                
                while current_time < now:
                    window_end = current_time + window_delta
                    count = sum(1 for msg_time in messages if current_time <= msg_time < window_end)
                    if count > 0:  # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–∫–Ω–∞ —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
                        window_counts.append(count)
                    current_time += timedelta(hours=1)  # –°–¥–≤–∏–≥–∞–µ–º –æ–∫–Ω–æ –Ω–∞ 1 —á–∞—Å
                
                if len(window_counts) < 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 –æ–∫–Ω–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                mean_count = statistics.mean(window_counts)
                if len(window_counts) > 1:
                    stdev_count = statistics.stdev(window_counts)
                else:
                    stdev_count = 0.0
                
                # –¢–µ–∫—É—â–µ–µ –æ–∫–Ω–æ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ window_hours —á–∞—Å–æ–≤)
                current_count = sum(1 for msg_time in messages if msg_time >= current_window_start)
                
                # –í—ã—á–∏—Å–ª—è–µ–º z-score –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –æ–∫–Ω–∞
                if stdev_count > 0:
                    z_score = (current_count - mean_count) / stdev_count
                else:
                    z_score = 0.0
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–ø–ª–µ—Å–∫
                is_spike = z_score > z_threshold
                
                # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–µ
                total_messages = len(messages)
                recent_messages = current_count
                channel_count = channel_counts.get(topic_id, 0)
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å (—Å –±–æ–Ω—É—Å–æ–º –∑–∞ –≤—Å–ø–ª–µ—Å–∫)
                popularity_score = (
                    total_messages * 0.4 +
                    recent_messages * 0.3 +
                    channel_count * 2 * 0.2 +
                    (z_score * 10 if is_spike else 0) * 0.1  # –ë–æ–Ω—É—Å –∑–∞ –≤—Å–ø–ª–µ—Å–∫
                )
                
                results.append({
                    'topic_id': topic_id,
                    'topic_name': topic_info['topic_name'],
                    'topic_description': topic_info['topic_description'],
                    'topic_color': topic_info['topic_color'],
                    'total_messages': total_messages,
                    'recent_messages': recent_messages,  # –í –ø–æ—Å–ª–µ–¥–Ω–µ–º –æ–∫–Ω–µ
                    'channel_count': channel_count,
                    'mean_count': round(mean_count, 2),
                    'stdev_count': round(stdev_count, 2),
                    'current_count': current_count,
                    'z_score': round(z_score, 2),
                    'is_spike': is_spike,
                    'spike_intensity': round(z_score, 2) if is_spike else 0.0,
                    'popularity_score': round(popularity_score, 1),
                    'window_hours': window_hours
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –≤—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ (–ø–æ z-score), –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ (–ø–æ popularity_score)
            results.sort(key=lambda x: (
                not x['is_spike'],  # –í—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏
                -x['z_score'] if x['is_spike'] else -x['popularity_score']  # –ü–æ —É–±—ã–≤–∞–Ω–∏—é
            ))
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º —Å–æ –≤—Å–ø–ª–µ—Å–∫–∞–º–∏: {e}", exc_info=True)
            raise
    
    async def get_trending_topics(self, period: str = 'daily', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Ç–µ–º—ã (deprecated, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ get_trending_topics_with_spikes)"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é –ª–æ–≥–∏–∫—É —Å–æ –≤—Å–ø–ª–µ—Å–∫–∞–º–∏
        return await self.get_trending_topics_with_spikes(window_hours=6, z_threshold=2.0, limit=limit)
    
    async def get_topic_connections(self, topic_id: int, limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –ù–∞—Ö–æ–¥–∏–º —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–æ–π
            query = """
                WITH topic_messages AS (
                    SELECT DISTINCT mt.message_id
                    FROM message_topics mt
                    WHERE mt.topic_id = $1
                ),
                related_topics AS (
                    SELECT 
                        mt2.topic_id,
                        t.name,
                        t.color,
                        COUNT(DISTINCT tm.message_id) as co_mention_count,
                        AVG(mt2.score) as avg_score
                    FROM topic_messages tm
                    JOIN message_topics mt2 ON tm.message_id = mt2.message_id
                    JOIN topics t ON mt2.topic_id = t.id
                    WHERE mt2.topic_id != $1
                    GROUP BY mt2.topic_id, t.name, t.color
                    HAVING COUNT(DISTINCT tm.message_id) >= 3
                )
                SELECT 
                    rt.topic_id,
                    rt.name,
                    rt.color,
                    rt.co_mention_count,
                    rt.avg_score,
                    CAST((rt.co_mention_count::float / (
                        SELECT COUNT(DISTINCT message_id) 
                        FROM message_topics 
                        WHERE topic_id = $1
                    ) * 100::float) AS NUMERIC(10,1)) as connection_strength
                FROM related_topics rt
                ORDER BY rt.co_mention_count DESC, rt.avg_score DESC
                LIMIT $2
            """
            
            rows = await conn.fetch(query, topic_id, limit)
            
            connections = []
            for row in rows:
                connections.append({
                    'topic_id': row['topic_id'],
                    'name': row['name'],
                    'color': row['color'],
                    'co_mention_count': row['co_mention_count'],
                    'avg_score': float(row['avg_score']),
                    'connection_strength': float(row['connection_strength'])
                })
            
            await conn.close()
            return connections
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤—è–∑–µ–π —Ç–µ–º: {e}")
            raise
    
    async def get_trending_channels_by_event(self, cluster_id: str, period: str = 'daily', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Å–æ–±—ã—Ç–∏—é (–∫–ª–∞—Å—Ç–µ—Ä—É)"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
            if period == 'daily':
                date_from = datetime.now() - timedelta(days=1)
            elif period == 'weekly':
                date_from = datetime.now() - timedelta(weeks=1)
            elif period == 'monthly':
                date_from = datetime.now() - timedelta(days=30)
            else:
                date_from = datetime.now() - timedelta(days=1)
            
            query = """
                SELECT 
                    c.id,
                    c.name,
                    c.username,
                    c.description,
                    COUNT(DISTINCT m.id) as message_count,
                    COUNT(DISTINCT cm.message_id) as event_messages,
                    AVG(cm.similarity_score) as avg_similarity,
                    SUM(COALESCE(m.views_count, 0)) as total_views,
                    SUM(COALESCE(m.forwards_count, 0)) as total_forwards,
                    CAST((AVG(cm.similarity_score) * COUNT(DISTINCT cm.message_id))::float AS NUMERIC(10,1)) as event_activity_score
                FROM channels c
                JOIN messages m ON c.id = m.channel_id
                JOIN cluster_messages cm ON m.id = cm.message_id
                WHERE cm.cluster_id = $1
                AND m.published_at >= $2
                GROUP BY c.id, c.name, c.username, c.description
                HAVING COUNT(DISTINCT cm.message_id) >= 1
                ORDER BY event_activity_score DESC, total_views DESC
                LIMIT $3
            """
            
            rows = await conn.fetch(query, cluster_id, date_from, limit)
            
            channels = []
            for row in rows:
                channels.append({
                    'channel_id': row['id'],
                    'name': row['name'],
                    'username': row['username'],
                    'description': row['description'],
                    'message_count': row['message_count'],
                    'event_messages': row['event_messages'],
                    'avg_similarity': float(row['avg_similarity']),
                    'total_views': row['total_views'],
                    'total_forwards': row['total_forwards'],
                    'event_activity_score': float(row['event_activity_score'])
                })
            
            await conn.close()
            return channels
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –ø–æ —Å–æ–±—ã—Ç–∏—é: {e}")
            raise
    
    async def get_trending_channels_by_topic(self, topic_id: int, period: str = 'daily', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–µ–º–µ (legacy –º–µ—Ç–æ–¥)"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
            if period == 'daily':
                date_from = datetime.now() - timedelta(days=1)
            elif period == 'weekly':
                date_from = datetime.now() - timedelta(weeks=1)
            elif period == 'monthly':
                date_from = datetime.now() - timedelta(days=30)
            else:
                date_from = datetime.now() - timedelta(days=1)
            
            query = """
                SELECT 
                    c.id,
                    c.name,
                    c.username,
                    c.description,
                    COUNT(DISTINCT m.id) as message_count,
                    COUNT(DISTINCT mt.message_id) as topic_messages,
                    AVG(mt.score) as avg_topic_score,
                    SUM(COALESCE(m.views_count, 0)) as total_views,
                    SUM(COALESCE(m.forwards_count, 0)) as total_forwards,
                    CAST((AVG(mt.score) * COUNT(DISTINCT mt.message_id))::float AS NUMERIC(10,1)) as topic_activity_score
                FROM channels c
                JOIN messages m ON c.id = m.channel_id
                JOIN message_topics mt ON m.id = mt.message_id
                WHERE mt.topic_id = $1
                AND m.published_at >= $2
                GROUP BY c.id, c.name, c.username, c.description
                HAVING COUNT(DISTINCT mt.message_id) >= 1
                ORDER BY topic_activity_score DESC, total_views DESC
                LIMIT $3
            """
            
            rows = await conn.fetch(query, topic_id, date_from, limit)
            
            channels = []
            for row in rows:
                channels.append({
                    'channel_id': row['id'],
                    'name': row['name'],
                    'username': row['username'],
                    'description': row['description'],
                    'message_count': row['message_count'],
                    'topic_messages': row['topic_messages'],
                    'avg_topic_score': float(row['avg_topic_score']),
                    'total_views': row['total_views'],
                    'total_forwards': row['total_forwards'],
                    'topic_activity_score': float(row['topic_activity_score'])
                })
            
            await conn.close()
            return channels
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {e}")
            raise
    
    async def get_trend_analytics(self, period: str = 'daily') -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–±—â—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ —Å–æ–±—ã—Ç–∏—è–º"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
            if period == 'daily':
                date_from = datetime.now() - timedelta(days=1)
            elif period == 'weekly':
                date_from = datetime.now() - timedelta(weeks=1)
            elif period == 'monthly':
                date_from = datetime.now() - timedelta(days=30)
            else:
                date_from = datetime.now() - timedelta(days=1)
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ–±—ã—Ç–∏—è–º (–∫–ª–∞—Å—Ç–µ—Ä–∞–º)
            analytics_query = """
                WITH event_stats AS (
                    SELECT 
                        dc.cluster_id,
                        dc.title,
                        COUNT(DISTINCT cm.message_id) as message_count,
                        AVG(cm.similarity_score) as avg_similarity,
                        COUNT(DISTINCT m.channel_id) as channel_count,
                        SUM(COALESCE(m.views_count, 0)) as total_views,
                        SUM(COALESCE(m.forwards_count, 0)) as total_forwards
                    FROM dedup_clusters dc
                    JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                    JOIN messages m ON cm.message_id = m.id
                    WHERE m.published_at >= $1
                    GROUP BY dc.cluster_id, dc.title
                    HAVING COUNT(DISTINCT cm.message_id) >= 2
                )
                SELECT 
                    COUNT(*) as total_active_events,
                    SUM(message_count) as total_messages,
                    AVG(avg_similarity) as avg_similarity,
                    SUM(channel_count) as total_active_channels,
                    SUM(total_views) as total_views,
                    SUM(total_forwards) as total_forwards,
                    MAX(message_count) as max_messages,
                    MIN(message_count) as min_messages,
                    AVG(message_count) as avg_messages_per_event
                FROM event_stats
            """
            
            analytics_row = await conn.fetchrow(analytics_query, date_from)
            
            # –¢–æ–ø-5 —Å–∞–º—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π
            top_events_query = """
                SELECT dc.title, COUNT(DISTINCT cm.message_id) as message_count
                FROM dedup_clusters dc
                JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                JOIN messages m ON cm.message_id = m.id
                WHERE m.published_at >= $1
                GROUP BY dc.cluster_id, dc.title
                HAVING COUNT(DISTINCT cm.message_id) >= 2
                ORDER BY message_count DESC
                LIMIT 5
            """
            
            top_events_rows = await conn.fetch(top_events_query, date_from)
            
            # –¢–æ–ø-5 —Å–∞–º—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
            top_channels_query = """
                SELECT c.name, COUNT(DISTINCT m.id) as message_count
                FROM channels c
                JOIN messages m ON c.id = m.channel_id
                WHERE m.published_at >= $1
                GROUP BY c.id, c.name
                ORDER BY message_count DESC
                LIMIT 5
            """
            
            top_channels_rows = await conn.fetch(top_channels_query, date_from)
            
            await conn.close()
            
            return {
                'period': period,
                'total_active_events': analytics_row['total_active_events'] or 0,
                'total_messages': analytics_row['total_messages'] or 0,
                'avg_similarity': float(analytics_row['avg_similarity'] or 0),
                'total_active_channels': analytics_row['total_active_channels'] or 0,
                'total_views': analytics_row['total_views'] or 0,
                'total_forwards': analytics_row['total_forwards'] or 0,
                'max_messages': analytics_row['max_messages'] or 0,
                'min_messages': analytics_row['min_messages'] or 0,
                'avg_messages_per_event': float(analytics_row['avg_messages_per_event'] or 0),
                'top_events': [{'name': row['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è', 'message_count': row['message_count']} for row in top_events_rows],
                'top_channels': [{'name': row['name'], 'message_count': row['message_count']} for row in top_channels_rows]
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            raise
    
    async def get_event_feed(self, user_id: str, limit: int = 20, offset: int = 0,
                           filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ª–µ–Ω—Ç—É —Å–æ–±—ã—Ç–∏–π"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            preferences = await self._get_onboarding_service().get_user_preferences(user_id)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω topic_id
            if preferences and preferences['selected_topics'] and filters and 'topic_id' not in filters:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ topic_id –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                filters['topic_id'] = preferences['selected_topics'][0]
            elif not filters:
                filters = {}
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–±—ã—Ç–∏—è
            events = await self._get_deduplication_service().get_event_clusters(limit, offset, filters)
            
            return events
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ª–µ–Ω—Ç—ã —Å–æ–±—ã—Ç–∏–π: {e}")
            raise
    
    async def save_search_query(self, user_id: str, name: str, query: str, 
                              filters: Dict[str, Any], cadence: str = 'manual') -> bool:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            await conn.execute("""
                INSERT INTO saved_searches (user_id, name, query, filters, cadence)
                VALUES ($1, $2, $3, $4, $5)
            """, user_id, name, query, filters, cadence)
            
            await conn.close()
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return False
    
    async def get_saved_searches(self, user_id: str) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            rows = await conn.fetch("""
                SELECT * FROM saved_searches 
                WHERE user_id = $1 AND is_active = TRUE
                ORDER BY created_at DESC
            """, user_id)
            
            searches = []
            for row in rows:
                searches.append({
                    'id': row['id'],
                    'name': row['name'],
                    'query': row['query'],
                    'filters': row['filters'],
                    'cadence': row['cadence'],
                    'last_run_at': row['last_run_at'],
                    'created_at': row['created_at']
                })
            
            await conn.close()
            return searches
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            raise

    async def get_saved_search(self, search_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            row = await conn.fetchrow("""
                SELECT id, user_id, name, query, filters, cadence, created_at
                FROM saved_searches WHERE id = $1
            """, search_id)
            
            await conn.close()
            
            if row:
                return {
                    'id': row['id'],
                    'user_id': row['user_id'],
                    'name': row['name'],
                    'query': row['query'],
                    'filters': json.loads(row['filters']) if row['filters'] else {},
                    'cadence': row['cadence'],
                    'created_at': row['created_at']
                }
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            raise

    async def delete_saved_search(self, search_id: int) -> bool:
        """–£–¥–∞–ª–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            result = await conn.execute("""
                DELETE FROM saved_searches WHERE id = $1
            """, search_id)
            
            await conn.close()
            
            return result == "DELETE 1"
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            raise
    
    async def _get_message_info(self, message_id: int) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–æ–±—â–µ–Ω–∏–∏"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–∞–Ω–∞–ª–æ–º
            message_row = await conn.fetchrow("""
                SELECT m.*, c.name as channel_name
                FROM messages m
                JOIN channels c ON m.channel_id = c.id
                WHERE m.id = $1
            """, message_id)
            
            if not message_row:
                await conn.close()
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–º—ã —Å–æ–æ–±—â–µ–Ω–∏—è
            topic_rows = await conn.fetch("""
                SELECT t.name, mt.score
                FROM message_topics mt
                JOIN topics t ON mt.topic_id = t.id
                WHERE mt.message_id = $1
                ORDER BY mt.score DESC
            """, message_id)
            
            topics = [{'name': row['name'], 'score': float(row['score'])} for row in topic_rows]
            
            await conn.close()
            
            return {
                'text': message_row['text_content'],
                'channel_name': message_row['channel_name'],
                'published_at': message_row['published_at'],
                'views': message_row['views_count'],
                'forwards': message_row['forwards_count'],
                'topics': topics
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ–æ–±—â–µ–Ω–∏–∏: {e}")
            return None

    def _get_embedding_service(self):
        """Lazy-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embedding_service"""
        if self._embedding_service is None:
            from pro_mode.embedding_service import embedding_service
            self._embedding_service = embedding_service
        return self._embedding_service

    def _get_deduplication_service(self):
        """Lazy-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è deduplication_service"""
        if self._deduplication_service is None:
            from pro_mode.deduplication_service import deduplication_service
            self._deduplication_service = deduplication_service
        return self._deduplication_service

    def _get_classification_service(self):
        """Lazy-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è classification_service"""
        if self._classification_service is None:
            from pro_mode.classification_service import classification_service
            self._classification_service = classification_service
        return self._classification_service

    def _get_onboarding_service(self):
        """Lazy-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è onboarding_service"""
        if self._onboarding_service is None:
            from pro_mode.classification_service import onboarding_service
            self._onboarding_service = onboarding_service
        return self._onboarding_service

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
pro_mode_service = ProModeService()
