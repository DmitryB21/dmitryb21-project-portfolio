"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BERTopic, FRIDA, GTE –∏ OpenAI GPT

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è:
- FRIDA (ai-forever/FRIDA) –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- gte-multilingual-base –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
- BERTopic –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
- OpenAI GPT API –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ç–µ–º (—Å fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
- –î–≤–∞ –∏–Ω–¥–µ–∫—Å–∞ Qdrant: posts_search (FRIDA) –∏ posts_clustering (GTE)
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ PostgreSQL (dedup_clusters, cluster_messages)
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –±–∞—Ç—á–µ–π
"""

import asyncio
import logging
import os
import re
import time
import gc
import sys
import types
from collections import Counter
from dataclasses import dataclass, fields, asdict
from datetime import datetime
from statistics import mean
from typing import Any, Dict, List, Optional, Tuple
import json
import uuid
from pathlib import Path

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è llama_cpp –î–û –∏–º–ø–æ—Ä—Ç–∞ bertopic
# (bertopic –ø—ã—Ç–∞–µ—Ç—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å llama_cpp –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ, –Ω–æ –º—ã –µ–≥–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º)
if 'llama_cpp' not in sys.modules:
    try:
        import llama_cpp
    except (ImportError, RuntimeError, FileNotFoundError, OSError):
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∑–∞–≥–ª—É—à–∫—É –¥–ª—è llama_cpp
        llama_cpp_stub = types.ModuleType('llama_cpp')
        llama_cpp_stub.Llama = None
        sys.modules['llama_cpp'] = llama_cpp_stub

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import asyncpg
import numpy as np

# –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct, 
    Filter, FieldCondition, MatchValue
)

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    SentenceTransformer = None

# –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç BERTopic (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è llama_cpp —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤—ã—à–µ)
try:
    from bertopic import BERTopic
    BERTOPIC_AVAILABLE = True
except (ImportError, RuntimeError, FileNotFoundError) as e:
    BERTOPIC_AVAILABLE = False
    BERTopic = None
    # logger –º–æ–∂–µ—Ç –±—ã—Ç—å –µ—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º print
    print(f"‚ö†Ô∏è BERTopic –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

try:
    from umap import UMAP
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    UMAP = None

try:
    from hdbscan import HDBSCAN
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False
    HDBSCAN = None

# OpenAI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –≤ openai_generator)

try:
    import psutil
except ImportError:
    psutil = None

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
from config_utils import get_config
from pro_mode.topic_modeling_progress import TopicModelingProgressTracker
from pro_mode.topic_modeling_settings import (
    load_topic_modeling_settings,
    cast_setting_value,
)

PROJECT_ROOT = Path(__file__).resolve().parent.parent
LOG_FILE = PROJECT_ROOT / "topic_modeling.log"
TOPIC_REPORT_DIR = PROJECT_ROOT / "artifacts" / "topic_modeling"
TOPIC_REPORT_FILE = TOPIC_REPORT_DIR / "topic_modeling.json"

logger = logging.getLogger(__name__)


def _ensure_topic_modeling_file_handler() -> None:
    """Attach a dedicated file handler so the pipeline always logs to topic_modeling.log."""
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    handler_exists = any(
        isinstance(handler, logging.FileHandler)
        and getattr(handler, "baseFilename", "") == str(LOG_FILE)
        for handler in logger.handlers
    )
    if handler_exists:
        return

    file_handler = logging.FileHandler(LOG_FILE, encoding="utf-8")
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    logger.setLevel(logging.INFO)


_ensure_topic_modeling_file_handler()


class TopicModelingCancelled(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ—Ç–º–µ–Ω—ã –ø–∞–π–ø–ª–∞–π–Ω–∞."""
    pass


# ============================================================================
# –®–ê–ì 2: –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø EMBEDDER'–û–í
# ============================================================================

class FRIDAEmbedder:
    """
    Embedder –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ FRIDA (ai-forever/FRIDA)
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º—ã:
    - search_document: –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - search_query: –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    - categorize_topic: –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º
    
    –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ä–µ–∂–∏–º–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å "{mode}: {text}"
    """
    
    def __init__(self, model_name: str = "ai-forever/FRIDA", device: str = "cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FRIDA embedder
        
        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –≤ HuggingFace
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ("cpu" –∏–ª–∏ "cuda")
        """
        self.model_name = model_name
        self.device = device
        self._model: Optional[SentenceTransformer] = None
        self._dimension: Optional[int] = None
        self._cache: Dict[str, List[float]] = {}  # –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    def _load_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers"
            )
        
        if self._model is None:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ FRIDA: {self.model_name} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
            gc.collect()
            
            # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º CUDA
            if self.device == "cuda":
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        logger.info("   üßπ GPU –∫—ç—à –æ—á–∏—â–µ–Ω –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π FRIDA")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å GPU –∫—ç—à: {e}")
            
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ model_kwargs
                self._model = SentenceTransformer(
                    self.model_name, 
                    device=self.device,
                    model_kwargs={"low_cpu_mem_usage": True} if hasattr(SentenceTransformer, 'model_kwargs') else {}
                )
            except (TypeError, AttributeError):
                # –ï—Å–ª–∏ model_kwargs –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
                self._model = SentenceTransformer(self.model_name, device=self.device)
            except RuntimeError as e:
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ CPU
                if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                    logger.warning(f"   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏ –¥–ª—è FRIDA, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU: {e}")
                    if self.device == "cuda":
                        try:
                            import torch
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                        except Exception:
                            pass
                        self.device = "cpu"
                        self._model = SentenceTransformer(self.model_name, device="cpu")
                        logger.info("   ‚úÖ FRIDA –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU (fallback –∏–∑-–∑–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ GPU –ø–∞–º—è—Ç–∏)")
                    else:
                        raise
                else:
                    raise
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –º–æ–¥–µ–ª–∏
            test_embedding = self._model.encode(["test"], show_progress_bar=False)
            self._dimension = len(test_embedding[0])
            logger.info(f"‚úÖ FRIDA –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self._dimension}")
            # –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            gc.collect()
    
    def encode(self, texts: List[str], mode: Optional[str] = None) -> List[List[float]]:
        """
        –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            mode: –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã ("search_document", "search_query", "categorize_topic")
                 –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω, –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ—Ñ–∏–∫—Å "{mode}: {text}"
        
        Returns:
            –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∫–∞–∂–¥—ã–π - —Å–ø–∏—Å–æ–∫ float)
        """
        if not texts:
            return []
        
        self._load_model()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏, –µ—Å–ª–∏ —Ä–µ–∂–∏–º —É–∫–∞–∑–∞–Ω
        if mode:
            prefixed_texts = [f"{mode}: {text}" for text in texts]
        else:
            prefixed_texts = texts
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ —Ä–µ–∂–∏–º–∞)
        if len(texts) == 1 and not mode:
            cache_key = texts[0]
            if cache_key in self._cache:
                return [self._cache[cache_key]]
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –±–∞—Ç—á–∏–Ω–≥–∞
        # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–π batch_size –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        # –î–ª—è GPU –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π batch_size
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ batch_size
        total_chars = sum(len(text) for text in prefixed_texts)
        avg_text_length = total_chars / len(prefixed_texts) if prefixed_texts else 0
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —É–º–µ–Ω—å—à–∞–µ–º batch_size –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        if self.device == "cpu":
            # –î–ª—è CPU: —É–º–µ–Ω—å—à–∞–µ–º batch_size –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            if avg_text_length > 2000:  # –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                batch_size = 4
            elif avg_text_length > 1000:  # –î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                batch_size = 8
            else:  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                batch_size = 16
        else:
            # –î–ª—è GPU: –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–π batch_size
            batch_size = 32 if avg_text_length < 1000 else 16
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.info(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã encode: {len(prefixed_texts)} —Ç–µ–∫—Å—Ç–æ–≤, batch_size={batch_size}, avg_length={avg_text_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤, total_chars={total_chars}")
        
        encode_start = time.perf_counter()
        embeddings = self._model.encode(
            prefixed_texts,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        )
        encode_duration = time.perf_counter() - encode_start
        logger.debug(f"   ‚è±Ô∏è encode() –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {encode_duration:.1f}—Å ({len(prefixed_texts)/encode_duration:.2f} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ —Ä–µ–∂–∏–º–∞)
        if len(texts) == 1 and not mode:
            self._cache[texts[0]] = embeddings[0].tolist()
        
        return embeddings.tolist()
    
    def get_dimension(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self._dimension is None:
            self._load_model()
        return self._dimension


class GTEEmbedder:
    """
    Embedder –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ gte-multilingual-base (Alibaba-NLP/gte-multilingual-base)
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –í–ê–ñ–ù–û: –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤!
    –¢–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–æ–≤.
    """
    
    def __init__(self, model_name: str = "Alibaba-NLP/gte-multilingual-base", device: str = "cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GTE embedder
        
        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –≤ HuggingFace
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ("cpu" –∏–ª–∏ "cuda")
        """
        self.model_name = model_name
        self.device = device
        self._model: Optional[SentenceTransformer] = None
        self._dimension: Optional[int] = None
        self._cache: Dict[str, List[float]] = {}  # –ü—Ä–æ—Å—Ç–æ–π –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    def _load_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers"
            )
        
        if self._model is None:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ GTE: {self.model_name}")
            # GTE –º–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç trust_remote_code=True
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º transformers –Ω–∞–ø—Ä—è–º—É—é
            try:
                from transformers import AutoModel, AutoTokenizer
                import torch
                
                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ GTE —á–µ—Ä–µ–∑ transformers —Å trust_remote_code=True...")
                tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
                model = AutoModel.from_pretrained(self.model_name, trust_remote_code=True)
                model.eval()
                if self.device == "cuda" and torch.cuda.is_available():
                    model = model.cuda()
                    device_torch = "cuda"
                else:
                    device_torch = "cpu"
                
                # –°–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å SentenceTransformer API
                class GTEWrapper:
                    def __init__(self, model, tokenizer, device_torch):
                        self.model = model
                        self.tokenizer = tokenizer
                        self.device_torch = device_torch
                    
                    def encode(self, texts, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True, **kwargs):
                        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
                        if isinstance(texts, str):
                            texts = [texts]
                        
                        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                        inputs = self.tokenizer(
                            texts,
                            padding=True,
                            truncation=True,
                            max_length=512,
                            return_tensors="pt"
                        )
                        if self.device_torch == "cuda":
                            inputs = {k: v.cuda() for k, v in inputs.items()}
                        
                        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                        with torch.no_grad():
                            outputs = self.model(**inputs)
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º mean pooling
                            embeddings = outputs.last_hidden_state
                            attention_mask = inputs['attention_mask']
                            embeddings = (embeddings * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1e-9)
                            
                            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                            if normalize_embeddings:
                                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
                        if convert_to_numpy:
                            embeddings = embeddings.cpu().numpy()
                        
                        return embeddings
                
                self._model = GTEWrapper(model, tokenizer, device_torch)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GTE –º–æ–¥–µ–ª–∏: {e}")
                import traceback
                traceback.print_exc()
                raise
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –º–æ–¥–µ–ª–∏
            test_embedding = self._model.encode(["test"], show_progress_bar=False)
            if isinstance(test_embedding, list):
                self._dimension = len(test_embedding[0])
            else:
                self._dimension = test_embedding.shape[1] if len(test_embedding.shape) > 1 else len(test_embedding[0])
            logger.info(f"‚úÖ GTE –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self._dimension}")
    
    def encode(self, texts: List[str], mode: Optional[str] = None) -> List[List[float]]:
        """
        –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            mode: –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º)
                 GTE –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã!
        
        Returns:
            –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∫–∞–∂–¥—ã–π - —Å–ø–∏—Å–æ–∫ float)
        """
        if not texts:
            return []
        
        self._load_model()
        
        # –í–ê–ñ–ù–û: GTE –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã, —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        # –ü–∞—Ä–∞–º–µ—Ç—Ä mode –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if len(texts) == 1:
            cache_key = texts[0]
            if cache_key in self._cache:
                return [self._cache[cache_key]]
        
        # –ö–æ–¥–∏—Ä—É–µ–º –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å –æ–±–µ—Ä—Ç–∫–æ–π GTEWrapper
        if hasattr(self._model, 'encode'):
            embeddings = self._model.encode(
                texts,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            )
        else:
            # Fallback –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ SentenceTransformer
            embeddings = self._model.encode(
                texts,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if len(texts) == 1:
            self._cache[texts[0]] = embeddings[0].tolist()
        
        return embeddings.tolist()
    
    def get_dimension(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self._dimension is None:
            self._load_model()
        return self._dimension


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

@dataclass
class TopicModelingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è TopicModelingService"""
    
    # –ú–æ–¥–µ–ª–∏
    frida_model_name: str = "ai-forever/FRIDA"
    gte_model_name: str = "Alibaba-NLP/gte-multilingual-base"
    frida_device: str = "cpu"
    gte_device: str = "cpu"
    
    # Qdrant
    qdrant_host: str = "127.0.0.1"
    qdrant_port: int = 6333
    search_collection: str = "posts_search"
    clustering_collection: str = "posts_clustering"
    
    # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–±—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π)
    frida_dimension: int = 1536  # FRIDA (–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    gte_dimension: int = 768  # gte-multilingual-base (–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    
    # BERTopic –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    umap_n_neighbors: int = 15
    umap_n_components: int = 5
    umap_min_dist: float = 0.0
    hdbscan_min_cluster_size: int = 3
    hdbscan_min_samples: int = 1
    nr_topics: str = "auto"  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–º (legacy)
    nr_topics_auto: bool = True
    max_topics: int = 100
    
    # OpenAI –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    use_openai_for_titles: bool = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenAI GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    openai_model: str = "gpt-3.5-turbo"  # –ú–æ–¥–µ–ª—å OpenAI (gpt-3.5-turbo, gpt-4, etc.)
    openai_temperature: float = 0.3  # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    openai_max_tokens: int = 50  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
    openai_timeout: float = 30.0  # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    max_title_length: int = 100
    num_sample_texts: int = 3
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    batch_size_qdrant: int = 50
    max_posts_for_clustering: int = 50_000
    rerun_interval_hours: int = 24
    
    @classmethod
    def from_config_file(cls) -> "TopicModelingConfig":
        """–°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ config.ini"""
        config = get_config()
        
        # –ß–∏—Ç–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Qdrant
        qdrant_host = "127.0.0.1"
        qdrant_port = 6333
        if 'qdrant' in config:
            qdrant_host = config['qdrant'].get('host', qdrant_host)
            qdrant_port = int(config['qdrant'].get('port', qdrant_port))
        
        # –ß–∏—Ç–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π (–µ—Å–ª–∏ –µ—Å—Ç—å —Å–µ–∫—Ü–∏—è topic_modeling)
        frida_model = "ai-forever/FRIDA"
        gte_model = "Alibaba-NLP/gte-multilingual-base"
        qwen_path = ""
        
        topic_section = config['topic_modeling'] if 'topic_modeling' in config else {}

        def get_int(key: str, default: int) -> int:
            try:
                return int(topic_section.get(key, default))
            except (TypeError, ValueError):
                return default

        def get_float(key: str, default: float) -> float:
            try:
                return float(topic_section.get(key, default))
            except (TypeError, ValueError):
                return default

        def get_bool(key: str, default: bool) -> bool:
            value = topic_section.get(key)
            if value is None:
                return default
            value = value.strip().lower()
            if value in {"1", "true", "yes", "on"}:
                return True
            if value in {"0", "false", "no", "off"}:
                return False
            return default

        if topic_section:
            frida_model = topic_section.get('frida_model', frida_model)
            gte_model = topic_section.get('gte_model', gte_model)
        
        kwargs = {
            "frida_model_name": frida_model,
            "gte_model_name": gte_model,
            "qdrant_host": qdrant_host,
            "qdrant_port": qdrant_port,
            "frida_device": topic_section.get('frida_device', cls.frida_device),
            "gte_device": topic_section.get('gte_device', cls.gte_device),
            "batch_size_qdrant": get_int('batch_size_qdrant', cls.batch_size_qdrant),
            "max_posts_for_clustering": get_int('max_posts_for_clustering', cls.max_posts_for_clustering),
            "rerun_interval_hours": get_int('rerun_interval_hours', cls.rerun_interval_hours),
            "use_openai_for_titles": get_bool('use_openai_for_titles', cls.use_openai_for_titles),
            "openai_model": topic_section.get('openai_model', cls.openai_model),
            "openai_temperature": get_float('openai_temperature', cls.openai_temperature),
            "openai_max_tokens": get_int('openai_max_tokens', cls.openai_max_tokens),
            "openai_timeout": get_float('openai_timeout', cls.openai_timeout),
            "max_title_length": get_int('max_title_length', cls.max_title_length),
            "num_sample_texts": get_int('num_sample_texts', cls.num_sample_texts),
            "umap_n_neighbors": get_int('umap_n_neighbors', cls.umap_n_neighbors),
            "umap_n_components": get_int('umap_n_components', cls.umap_n_components),
            "umap_min_dist": get_float('umap_min_dist', cls.umap_min_dist),
            "hdbscan_min_cluster_size": get_int('hdbscan_min_cluster_size', cls.hdbscan_min_cluster_size),
            "hdbscan_min_samples": get_int('hdbscan_min_samples', cls.hdbscan_min_samples),
            "nr_topics_auto": get_bool('nr_topics_auto', cls.nr_topics_auto),
            "max_topics": get_int('max_topics', cls.max_topics),
        }

        cfg = cls(**kwargs)
        cfg.nr_topics = "auto" if cfg.nr_topics_auto else cfg.max_topics

        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ UI (JSON)
        overrides = load_topic_modeling_settings()
        for key, value in overrides.items():
            if hasattr(cfg, key):
                cfg_value = cast_setting_value(key, value)
                setattr(cfg, key, cfg_value)

        cfg.nr_topics = "auto" if cfg.nr_topics_auto else cfg.max_topics
        return cfg


class TopicModelingService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BERTopic
    
    –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –≤ –¥–≤–∞ –∏–Ω–¥–µ–∫—Å–∞ Qdrant (search –∏ clustering)
    - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ BERTopic
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ç–µ–º —á–µ—Ä–µ–∑ Qwen2.5
    - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ PostgreSQL
    """
    
    def __init__(
        self,
        config: Optional[TopicModelingConfig] = None,
        progress_tracker: Optional[TopicModelingProgressTracker] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞. –ï—Å–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ config.ini
            progress_tracker: –¢—Ä–µ–∫–µ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å UI
        """
        self.config = config or TopicModelingConfig.from_config_file()
        self.progress_tracker = progress_tracker
        
        # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π (–ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self._frida_embedder: Optional['FRIDAEmbedder'] = None
        self._gte_embedder: Optional['GTEEmbedder'] = None
        self._openai_generator: Optional['OpenAITitleGenerator'] = None
        self._bertopic: Optional[BERTopic] = None
        
        # –°–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        self._timings: Dict[str, float] = {}
        self._resource_usage: Dict[str, float] = {"peak_ram_gb": 0.0}
        self._title_stats: Dict[str, Any] = {"count": 0, "durations": []}
        self._metrics_snapshot: Dict[str, Any] = {}
        
        # Qdrant –∫–ª–∏–µ–Ω—Ç
        self.qdrant_client = QdrantClient(
            host=self.config.qdrant_host,
            port=self.config.qdrant_port,
            timeout=60.0
        )
        
        # PostgreSQL DSN (–±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self._pg_dsn: Optional[str] = None
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._last_post_ids: Optional[List[int]] = None
        self._last_topics: Optional[List[int]] = None
        self._last_probs: Optional[List[float]] = None
        self._last_texts: Optional[List[str]] = None
        
        logger.info("‚úÖ TopicModelingService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"   Qdrant: {self.config.qdrant_host}:{self.config.qdrant_port}")
        logger.info(f"   –ö–æ–ª–ª–µ–∫—Ü–∏–∏: {self.config.search_collection}, {self.config.clustering_collection}")

    # ------------------------------------------------------------------
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´
    # ------------------------------------------------------------------

    def _progress_start(self, settings: Optional[Dict[str, Any]] = None):
        if self.progress_tracker:
            self.progress_tracker.start(settings or {})

    def _progress_step(self, step_id: str, status: str, details: Optional[Dict[str, Any]] = None):
        if self.progress_tracker:
            self.progress_tracker.update_step(step_id, status, details)

    def _progress_log(self, message: str, level: str = "info"):
        log_fn = getattr(logger, level, logger.info)
        log_fn(message)
        if self.progress_tracker:
            self.progress_tracker.log(message, level)

    def _progress_metrics(self, metrics: Dict[str, Any]):
        self._metrics_snapshot.update(metrics)
        if self.progress_tracker:
            self.progress_tracker.update_metrics(metrics)

    def _check_cancellation(self):
        if self.progress_tracker and self.progress_tracker.is_cancel_requested():
            raise TopicModelingCancelled("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–º–µ–Ω–∏–ª –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞")

    def _update_resource_usage(self):
        if not psutil:
            return
        process = psutil.Process(os.getpid())
        rss_gb = process.memory_info().rss / (1024 ** 3)
        peak = self._resource_usage.get("peak_ram_gb", 0.0)
        if rss_gb > peak:
            self._resource_usage["peak_ram_gb"] = round(rss_gb, 2)

    def _record_timing(self, key: str, duration: float):
        self._timings[key] = duration

    def _record_title_duration(self, duration: float):
        self._title_stats.setdefault("durations", []).append(duration)
        self._title_stats["count"] = len(self._title_stats["durations"])

    @staticmethod
    def _json_default(value: Any):
        if isinstance(value, datetime):
            return value.isoformat()
        if isinstance(value, (np.generic,)):
            return value.item()
        if isinstance(value, set):
            return list(value)
        raise TypeError(f"Object of type {type(value)!r} is not JSON serializable")

    def _summarize_loaded_posts(self, posts: Optional[List[Dict[str, Any]]]) -> Dict[str, Any]:
        if not posts:
            return {"total": 0, "examples": []}
        timestamps = [
            p.get("timestamp") for p in posts
            if isinstance(p.get("timestamp"), datetime)
        ]
        examples = []
        for post in posts[:5]:
            examples.append({
                "post_id": post.get("post_id"),
                "timestamp": post.get("timestamp").isoformat() if isinstance(post.get("timestamp"), datetime) else None,
                "text_preview": (post.get("text") or "")[:280]
            })
        summary = {
            "total": len(posts),
            "examples": examples
        }
        if timestamps:
            summary["time_range"] = {
                "start": min(timestamps).isoformat(),
                "end": max(timestamps).isoformat()
            }
        return summary

    def _prepare_documents_distribution(self) -> Tuple[List[Dict[str, Any]], Dict[int, Dict[str, Any]], List[Dict[str, Any]]]:
        if not self._last_post_ids or not self._last_topics:
            return [], {}, []
        documents: List[Dict[str, Any]] = []
        distribution: Dict[int, Dict[str, Any]] = {}
        noise_docs: List[Dict[str, Any]] = []
        probs_available = self._last_probs is not None and len(self._last_probs) == len(self._last_post_ids)
        texts_available = self._last_texts is not None and len(self._last_texts) == len(self._last_post_ids)

        for idx, post_id in enumerate(self._last_post_ids):
            raw_topic = self._last_topics[idx]
            topic_id = int(raw_topic) if raw_topic is not None else None
            semantic_score: Optional[float] = None
            if probs_available and self._last_probs[idx] is not None:
                prob_row = self._last_probs[idx]
                try:
                    max_prob = float(np.max(prob_row))
                except Exception:
                    max_prob = None
                semantic_score = round(max_prob, 4) if isinstance(max_prob, (int, float)) else None
            text_preview = ""
            if texts_available:
                text_preview = (self._last_texts[idx] or "")[:500]
            doc_entry = {
                "post_id": int(post_id) if post_id is not None else None,
                "topic_id": topic_id,
                "semantic_score": semantic_score,
                "text_preview": text_preview
            }
            documents.append(doc_entry)

            bucket = distribution.setdefault(topic_id, {"count": 0, "scores": []})
            bucket["count"] += 1
            if semantic_score is not None:
                bucket["scores"].append(semantic_score)

            if topic_id == -1:
                noise_docs.append(doc_entry)

        for topic_id, bucket in distribution.items():
            scores = bucket.pop("scores")
            bucket["avg_semantic_score"] = round(mean(scores), 4) if scores else None

        return documents, distribution, noise_docs

    async def _get_classification_data(self) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ –ë–î"""
        try:
            dsn = self._get_pg_dsn()
            conn = await asyncpg.connect(dsn=dsn)
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–µ–º—ã —Å –∏—Ö –æ–ø–∏—Å–∞–Ω–∏—è–º–∏
                topics_rows = await conn.fetch("""
                    SELECT id, name, description, color
                    FROM topics
                    ORDER BY name
                """)
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
                classifications_rows = await conn.fetch("""
                    SELECT 
                        mt.topic_id,
                        mt.message_id,
                        mt.confidence_score,
                        m.text_content,
                        m.published_at,
                        m.channel_id,
                        c.name as channel_name,
                        c.username as channel_username
                    FROM message_topics mt
                    JOIN messages m ON mt.message_id = m.id
                    LEFT JOIN channels c ON m.channel_id = c.id
                    ORDER BY mt.topic_id, mt.confidence_score DESC
                """)
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–º–∞–º
                topics_dict = {row['id']: {
                    'id': row['id'],
                    'name': row['name'],
                    'description': row['description'],
                    'color': row['color']
                } for row in topics_rows}
                
                messages_by_topic: Dict[int, List[Dict[str, Any]]] = {}
                for row in classifications_rows:
                    topic_id = row['topic_id']
                    if topic_id not in messages_by_topic:
                        messages_by_topic[topic_id] = []
                    
                    # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç—á–µ—Ç–∞
                    text_content = row['text_content'] or ''
                    text_preview = text_content[:200] + '...' if len(text_content) > 200 else text_content
                    
                    messages_by_topic[topic_id].append({
                        'message_id': row['message_id'],
                        'confidence_score': round(float(row['confidence_score']), 4) if row['confidence_score'] else None,
                        'text_preview': text_preview,
                        'published_at': row['published_at'].isoformat() if row['published_at'] else None,
                        'channel_id': row['channel_id'],
                        'channel_name': row['channel_name'],
                        'channel_username': row['channel_username']
                    })
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                classification_topics = []
                for topic_id, topic_info in topics_dict.items():
                    messages = messages_by_topic.get(topic_id, [])
                    if messages:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–º—ã —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
                        classification_topics.append({
                            'topic_id': topic_id,
                            'name': topic_info['name'],
                            'description': topic_info['description'],
                            'color': topic_info['color'],
                            'messages_count': len(messages),
                            'avg_confidence': round(
                                sum(m['confidence_score'] or 0 for m in messages) / len(messages), 
                                4
                            ) if messages else None,
                            'messages': messages
                        })
                
                return {
                    'topics_count': len(classification_topics),
                    'total_messages': sum(len(messages_by_topic.get(tid, [])) for tid in topics_dict.keys()),
                    'topics': classification_topics
                }
            finally:
                await conn.close()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return None

    def _build_topic_modeling_report(
        self,
        *,
        new_posts: Optional[List[Dict[str, Any]]],
        topic_info: Dict[str, Any],
        save_stats: Dict[str, Any],
        metrics: Dict[str, Any],
        posts_indexed: int,
        fetch_mode: str,
        classification_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        config_snapshot = asdict(self.config)

        def _cast_topic_id(value: Any) -> Any:
            try:
                return int(value)
            except (TypeError, ValueError):
                return value

        documents, distribution, noise_docs = self._prepare_documents_distribution()
        documents_by_topic: Dict[int, List[Dict[str, Any]]] = {}
        for doc in documents:
            documents_by_topic.setdefault(doc["topic_id"], []).append(doc)

        cluster_cards = save_stats.get("cluster_cards", [])
        topic_keywords = topic_info.get("topic_keywords", {})
        clusters_detail = []
        for card in cluster_cards:
            topic_id = _cast_topic_id(card.get("topic_id"))
            cluster_docs = documents_by_topic.get(topic_id, [])
            scores = [doc["semantic_score"] for doc in cluster_docs if doc["semantic_score"] is not None]
            clusters_detail.append({
                "topic_id": topic_id,
                "title": card["title"],
                "keywords": topic_keywords.get(topic_id, card.get("keywords", [])),
                "size": len(cluster_docs),
                "avg_semantic_score": round(mean(scores), 4) if scores else None,
                "messages": cluster_docs
            })

        min_cluster_size = self.config.hdbscan_min_cluster_size
        skipped_topics = []
        for topic_id, stats in distribution.items():
            if topic_id in (None, -1):
                continue
            if stats["count"] < min_cluster_size:
                skipped_topics.append({
                    "topic_id": topic_id,
                    "size": stats["count"],
                    "keywords": topic_keywords.get(topic_id, []),
                    "messages": documents_by_topic.get(topic_id, [])
                })

        noise_section = {
            "count": len(noise_docs),
            "messages": noise_docs
        }

        report = {
            "generated_at": datetime.utcnow().isoformat(),
            "settings": {
                "config": config_snapshot,
                "runtime": {
                    "fetch_mode": fetch_mode,
                    "posts_indexed": posts_indexed
                }
            },
            "input_posts": self._summarize_loaded_posts(new_posts),
            "documents_summary": {
                "total": len(documents),
                "distribution": distribution,
                "noise": noise_section
            },
            "undistributed_messages": noise_docs,
            "clusters": clusters_detail,
            "skipped_topics": skipped_topics,
            "topic_keywords": {
                _cast_topic_id(topic_id): words
                for topic_id, words in topic_keywords.items()
            },
            "llm_titles": {
                _cast_topic_id(card.get("topic_id")): card["title"]
                for card in cluster_cards
            },
            "metrics": metrics
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if classification_data:
            report["classification"] = classification_data
        
        return report

    def _write_topic_modeling_report(self, report_data: Dict[str, Any]) -> None:
        try:
            TOPIC_REPORT_DIR.mkdir(parents=True, exist_ok=True)
            with TOPIC_REPORT_FILE.open("w", encoding="utf-8") as fp:
                json.dump(report_data, fp, ensure_ascii=False, indent=2, default=self._json_default)
            logger.info(f"üìù –û—Ç—á–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {TOPIC_REPORT_FILE}")
        except Exception as exc:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {exc}")

    def _build_metrics(
        self,
        topic_info: Dict[str, Any],
        save_stats: Dict[str, Any],
        qdrant_stats: Dict[str, Optional[Dict[str, Any]]],
        execution_time: float,
        posts_indexed: int,
        topics_count: int
    ) -> Dict[str, Any]:
        total_documents = len(self._last_topics or [])
        outliers = sum(1 for t in (self._last_topics or []) if t == -1)
        posts_in_clusters = total_documents - outliers
        avg_cluster_size = round(
            mean(topic_info["topic_sizes"].values()), 2
        ) if topic_info["topic_sizes"] else 0
        posts_in_clusters_pct = round(
            (posts_in_clusters / total_documents) * 100, 1
        ) if total_documents else 0
        noise_pct = round(
            (outliers / total_documents) * 100, 1
        ) if total_documents else 0
        title_avg = round(
            mean(self._title_stats["durations"]), 3
        ) if self._title_stats["durations"] else 0.0

        qdrant_batches_total = 0
        qdrant_points_total = 0
        for key in ("search", "clustering"):
            stats = qdrant_stats.get(key)
            if stats:
                qdrant_batches_total += stats.get("batches", 0)
                qdrant_points_total += stats.get("points", 0)

        metrics = {
            "execution_time_sec": round(execution_time, 2),
            "posts_indexed": posts_indexed,
            "documents_in_model": total_documents,
            "topics_found": topics_count,
            "avg_cluster_size": avg_cluster_size,
            "posts_in_clusters_pct": posts_in_clusters_pct,
            "noise_pct": noise_pct,
            "cluster_size_distribution": save_stats.get("size_distribution", {}),
            "word_cloud": save_stats.get("keyword_cloud", []),
            "sample_clusters": save_stats.get("samples", []),
            "title_generation_avg_sec": title_avg,
            "title_generation_count": self._title_stats["count"],
            "qdrant_batches_total": qdrant_batches_total,
            "qdrant_points_total": qdrant_points_total,
            "resource_usage": self._resource_usage,
            "step_timings": {k: round(v, 2) for k, v in self._timings.items()},
            "last_run_at": datetime.utcnow().isoformat()
        }
        return metrics
    
    @property
    def frida_embedder(self) -> 'FRIDAEmbedder':
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ FRIDA embedder"""
        if self._frida_embedder is None:
            self._frida_embedder = FRIDAEmbedder(
                model_name=self.config.frida_model_name,
                device=self.config.frida_device
            )
        return self._frida_embedder
    
    @property
    def gte_embedder(self) -> 'GTEEmbedder':
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ GTE embedder"""
        if self._gte_embedder is None:
            self._gte_embedder = GTEEmbedder(
                model_name=self.config.gte_model_name,
                device=self.config.gte_device
            )
        return self._gte_embedder
    
    @property
    def openai_generator(self) -> Optional['OpenAITitleGenerator']:
        """
        –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ OpenAI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        
        Returns:
            OpenAITitleGenerator –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
        """
        if not self.config.use_openai_for_titles:
            logger.debug("OpenAI –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö (use_openai_for_titles=false). –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.")
            return None
        if self._openai_generator is None:
            try:
                from pro_mode.openai_title_generator import OpenAITitleGenerator
                import os
                
                api_key = os.getenv('OPENAI_API_KEY', '').strip()
                if not api_key:
                    self._progress_log(
                        "OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. –ó–∞–≥–æ–ª–æ–≤–∫–∏ –±—É–¥—É—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.",
                        level="warning"
                    )
                    return None
                
                logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI TitleGenerator: –º–æ–¥–µ–ª—å {self.config.openai_model}")
                self._openai_generator = OpenAITitleGenerator(
                    api_key=api_key,
                    model=self.config.openai_model,
                    temperature=self.config.openai_temperature,
                    max_tokens=self.config.openai_max_tokens,
                    timeout=self.config.openai_timeout
                )
            except ImportError as e:
                self._progress_log(
                    f"openai –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {e}. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai",
                    level="error"
                )
                self._progress_log("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)", level="warning")
                return None
            except Exception as e:
                self._progress_log(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}", level="error")
                self._progress_log("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)", level="warning")
                return None
        return self._openai_generator
    
    def _get_pg_dsn(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å DSN –¥–ª—è PostgreSQL"""
        if self._pg_dsn is None:
            config = get_config()
            if 'postgresql' not in config or 'dsn' not in config['postgresql']:
                raise ValueError("PostgreSQL DSN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ config.ini")
            self._pg_dsn = config['postgresql']['dsn']
        return self._pg_dsn
    
    async def _save_embeddings_metadata(
        self,
        posts: List[Dict[str, Any]],
        model_name: str,
        collection_name: str,
        embedding_dim: int
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É embeddings
        
        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–ª–µ–º post_id
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ ("FRIDA" –∏–ª–∏ "GTE")
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant ("posts_search" –∏–ª–∏ "posts_clustering")
            embedding_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (1536 –¥–ª—è FRIDA, 768 –¥–ª—è GTE)
        """
        if not posts:
            return
        
        try:
            dsn = self._get_pg_dsn()
            conn = await asyncpg.connect(dsn=dsn)
            
            try:
                # –ü–∞–∫–µ—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                saved_count = 0
                for post in posts:
                    post_id = post.get('post_id')
                    if not post_id:
                        continue
                    
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ON CONFLICT –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π
                        await conn.execute("""
                            INSERT INTO embeddings (message_id, model, vector_id, embedding_dim, collection_name)
                            VALUES ($1, $2, $3, $4, $5)
                            ON CONFLICT (message_id, model, collection_name) DO UPDATE SET
                                vector_id = EXCLUDED.vector_id,
                                embedding_dim = EXCLUDED.embedding_dim,
                                created_at = NOW()
                        """, 
                        post_id,  # message_id
                        model_name,  # model
                        str(post_id),  # vector_id (ID –≤ Qdrant)
                        embedding_dim,  # embedding_dim
                        collection_name  # collection_name
                        )
                        saved_count += 1
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è post_id={post_id}: {e}")
                        continue
                
                if saved_count > 0:
                    logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {saved_count} –∑–∞–ø–∏—Å–µ–π (–º–æ–¥–µ–ª—å={model_name}, –∫–æ–ª–ª–µ–∫—Ü–∏—è={collection_name})")
                
            finally:
                await conn.close()
                
        except Exception as e:
            # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏, –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ —É–¥–∞–ª–æ—Å—å
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ PostgreSQL: {e}")
    
    # ============================================================================
    # –®–ê–ì 3: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° QDRANT
    # ============================================================================
    
    async def _ensure_collection(
        self, 
        collection_name: str, 
        vector_size: int
    ) -> None:
        """
        –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ Qdrant
        
        Args:
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collections = self.qdrant_client.get_collections().collections
            collection_exists = any(c.name == collection_name for c in collections)
            
            if collection_exists:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                collection_info = self.qdrant_client.get_collection(collection_name)
                try:
                    existing_size = collection_info.config.params.vectors.size
                except AttributeError:
                    try:
                        existing_size = collection_info.config.vectors.size
                    except AttributeError:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é...")
                        self.qdrant_client.delete_collection(collection_name=collection_name)
                        collection_exists = False
                
                if collection_exists and existing_size != vector_size:
                    logger.warning(
                        f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name} –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: "
                        f"–æ–∂–∏–¥–∞–µ—Ç—Å—è {vector_size}, –Ω–∞–π–¥–µ–Ω–æ {existing_size}. –ü–µ—Ä–µ—Å–æ–∑–¥–∞—é..."
                    )
                    self.qdrant_client.delete_collection(collection_name=collection_name)
                    collection_exists = False
            
            if not collection_exists:
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
                self.qdrant_client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–∑–¥–∞–Ω–∞ —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {vector_size}")
            else:
                logger.debug(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}: {e}")
            raise
    
    async def upsert_to_search_index(self, posts: List[Dict[str, Any]]) -> None:
        """
        –î–æ–±–∞–≤–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å –ø–æ—Å—Ç—ã –≤ –∏–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫–∞ (posts_search)
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç FRIDA embedder —Å —Ä–µ–∂–∏–º–æ–º "search_document"
        
        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏:
                - post_id: ID –ø–æ—Å—Ç–∞ (int)
                - text: –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ (str)
                - timestamp: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ (datetime –∏–ª–∏ str)
        """
        if not posts:
            return
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤
        valid_posts = []
        for post in posts:
            if not isinstance(post, dict):
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –ø–æ—Å—Ç (–Ω–µ —Å–ª–æ–≤–∞—Ä—å): {post}")
                continue
            if 'post_id' not in post or 'text' not in post:
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω –ø–æ—Å—Ç –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π: {post}")
                continue
            text = post.get('text', '').strip()
            if not text or len(text) < 10:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –ø–æ—Å—Ç {post.get('post_id')} —Å –ø—É—Å—Ç—ã–º –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º")
                continue
            # –û–±—Ä–µ–∑–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            if len(text) > 10000:
                text = text[:10000]
                post = {**post, 'text': text}
            valid_posts.append(post)
        
        if not valid_posts:
            logger.warning("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ search –∏–Ω–¥–µ–∫—Å")
            return
        
        posts = valid_posts
        
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {len(posts)} –ø–æ—Å—Ç–æ–≤ –≤ search –∏–Ω–¥–µ–∫—Å...")
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        frida_dim = self.frida_embedder.get_dimension()
        await self._ensure_collection(self.config.search_collection, frida_dim)
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ FRIDA —Å —Ä–µ–∂–∏–º–æ–º search_document
        logger.info(f"   –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ FRIDA –¥–ª—è {len(posts)} –ø–æ—Å—Ç–æ–≤...")
        texts = [post['text'] for post in posts]
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        text_lengths = [len(text) for text in texts]
        avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
        max_length = max(text_lengths) if text_lengths else 0
        total_chars = sum(text_lengths)
        logger.info(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤: avg={avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤, max={max_length}, total={total_chars}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (FRIDA –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤)
        # –ü—Ä–∏–º–µ—Ä–Ω–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        max_text_length = 2000
        texts_trimmed = []
        trimmed_count = 0
        for text in texts:
            if len(text) > max_text_length:
                texts_trimmed.append(text[:max_text_length])
                trimmed_count += 1
            else:
                texts_trimmed.append(text)
        if trimmed_count > 0:
            logger.info(f"   ‚úÇÔ∏è –û–±—Ä–µ–∑–∞–Ω–æ {trimmed_count} —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ {max_text_length} —Å–∏–º–≤–æ–ª–æ–≤")
        texts = texts_trimmed
        
        embed_batch = min(self.config.batch_size_qdrant, len(texts))
        frida_vectors: List[List[float]] = []
        total_batches = (len(texts) + embed_batch - 1) // embed_batch
        batch_start_time = time.perf_counter()
        for idx in range(0, len(texts), embed_batch):
            batch_texts = texts[idx:idx + embed_batch]
            batch_num = idx // embed_batch + 1
            batch_batch_start = time.perf_counter()
            logger.info(f"   FRIDA –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch_texts)} —Ç–µ–∫—Å—Ç–æ–≤)...")
            frida_vectors.extend(self.frida_embedder.encode(batch_texts, mode="search_document"))
            batch_duration = time.perf_counter() - batch_batch_start
            logger.info(f"   ‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {batch_duration:.1f}—Å ({len(batch_texts)/batch_duration:.2f} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫)")
        total_duration = time.perf_counter() - batch_start_time
        logger.info(f"   ‚úÖ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ {total_duration:.1f}—Å ({len(texts)/total_duration:.1f} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫)")
        embeddings = frida_vectors
        logger.info(f"   ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {len(embeddings)} –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {len(embeddings[0]) if embeddings else 0}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
        points = []
        for i, post in enumerate(posts):
            point = PointStruct(
                id=post['post_id'],
                vector=embeddings[i],
                payload={
                    "post_id": post['post_id'],
                    "text": post['text'],
                    "timestamp": post['timestamp'].isoformat() if isinstance(post['timestamp'], datetime) else str(post['timestamp'])
                }
            )
            points.append(point)
        
        # –ü–∞–∫–µ—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞
        batch_size = self.config.batch_size_qdrant
        total_batches = (len(points) + batch_size - 1) // batch_size
        logger.info(f"   –ù–∞—á–∞–ª–æ –ø–∞–∫–µ—Ç–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏: {total_batches} –±–∞—Ç—á–µ–π –ø–æ {batch_size} —Ç–æ—á–µ–∫")
        
        for i in range(0, len(points), batch_size):
            self._check_cancellation()
            batch = points[i:i + batch_size]
            batch_num = i // batch_size + 1
            logger.info(f"   –í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch)} —Ç–æ—á–µ–∫)...")
            
            try:
                self.qdrant_client.upsert(
                    collection_name=self.config.search_collection,
                    points=batch
                )
                logger.info(f"   ‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –≤—Å—Ç–∞–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                self._update_resource_usage()
            except Exception as e:
                logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –±–∞—Ç—á–∞ {batch_num}: {e}")
                raise
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ PostgreSQL
        try:
            await self._save_embeddings_metadata(
                posts=posts,
                model_name="FRIDA",
                collection_name=self.config.search_collection,
                embedding_dim=frida_dim
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ FRIDA —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ search –∏–Ω–¥–µ–∫—Å –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(posts)} –ø–æ—Å—Ç–æ–≤")
        return {
            "points": len(points),
            "batches": total_batches
        }
    
    async def upsert_to_clustering_index(self, posts: List[Dict[str, Any]]) -> None:
        """
        –î–æ–±–∞–≤–∏—Ç—å/–æ–±–Ω–æ–≤–∏—Ç—å –ø–æ—Å—Ç—ã –≤ –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (posts_clustering)
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GTE embedder (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤!)
        
        Args:
            posts: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏:
                - post_id: ID –ø–æ—Å—Ç–∞ (int)
                - text: –¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ (str)
                - timestamp: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ (datetime –∏–ª–∏ str)
        """
        if not posts:
            return
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ (—Ç–∞ –∂–µ –ª–æ–≥–∏–∫–∞, —á—Ç–æ –∏ –¥–ª—è search)
        valid_posts = []
        for post in posts:
            if not isinstance(post, dict):
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –ø–æ—Å—Ç (–Ω–µ —Å–ª–æ–≤–∞—Ä—å): {post}")
                continue
            if 'post_id' not in post or 'text' not in post:
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω –ø–æ—Å—Ç –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π: {post}")
                continue
            text = post.get('text', '').strip()
            if not text or len(text) < 10:
                logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –ø–æ—Å—Ç {post.get('post_id')} —Å –ø—É—Å—Ç—ã–º –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º")
                continue
            # –û–±—Ä–µ–∑–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            if len(text) > 10000:
                text = text[:10000]
                post = {**post, 'text': text}
            valid_posts.append(post)
        
        if not valid_posts:
            logger.warning("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ clustering –∏–Ω–¥–µ–∫—Å")
            return
        
        posts = valid_posts
        
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {len(posts)} –ø–æ—Å—Ç–æ–≤ –≤ clustering –∏–Ω–¥–µ–∫—Å...")
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        gte_dim = self.gte_embedder.get_dimension()
        await self._ensure_collection(self.config.clustering_collection, gte_dim)
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ GTE (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤!)
        logger.info(f"   –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ GTE –¥–ª—è {len(posts)} –ø–æ—Å—Ç–æ–≤...")
        texts = [post['text'] for post in posts]
        embed_batch = min(self.config.batch_size_qdrant, len(texts))
        gte_vectors: List[List[float]] = []
        total_batches = (len(texts) + embed_batch - 1) // embed_batch
        for idx in range(0, len(texts), embed_batch):
            batch_texts = texts[idx:idx + embed_batch]
            batch_num = idx // embed_batch + 1
            logger.info(f"   GTE –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch_texts)} —Ç–µ–∫—Å—Ç–æ–≤)")
            gte_vectors.extend(self.gte_embedder.encode(batch_texts))
        embeddings = gte_vectors
        logger.info(f"   ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {len(embeddings)} –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {len(embeddings[0]) if embeddings else 0}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
        points = []
        for i, post in enumerate(posts):
            point = PointStruct(
                id=post['post_id'],
                vector=embeddings[i],
                payload={
                    "post_id": post['post_id'],
                    "text": post['text'],
                    "timestamp": post['timestamp'].isoformat() if isinstance(post['timestamp'], datetime) else str(post['timestamp'])
                }
            )
            points.append(point)
        
        # –ü–∞–∫–µ—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞
        batch_size = self.config.batch_size_qdrant
        total_batches = (len(points) + batch_size - 1) // batch_size
        logger.info(f"   –ù–∞—á–∞–ª–æ –ø–∞–∫–µ—Ç–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏: {total_batches} –±–∞—Ç—á–µ–π –ø–æ {batch_size} —Ç–æ—á–µ–∫")
        
        for i in range(0, len(points), batch_size):
            self._check_cancellation()
            batch = points[i:i + batch_size]
            batch_num = i // batch_size + 1
            logger.info(f"   –í—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch)} —Ç–æ—á–µ–∫)...")
            
            try:
                self.qdrant_client.upsert(
                    collection_name=self.config.clustering_collection,
                    points=batch
                )
                logger.info(f"   ‚úÖ –ë–∞—Ç—á {batch_num}/{total_batches} –≤—Å—Ç–∞–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                self._update_resource_usage()
            except Exception as e:
                logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –±–∞—Ç—á–∞ {batch_num}: {e}")
                raise
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ PostgreSQL
        try:
            await self._save_embeddings_metadata(
                posts=posts,
                model_name="GTE",
                collection_name=self.config.clustering_collection,
                embedding_dim=gte_dim
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ GTE —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        
        logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ clustering –∏–Ω–¥–µ–∫—Å –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(posts)} –ø–æ—Å—Ç–æ–≤")
        return {
            "points": len(points),
            "batches": total_batches
        }
    
    async def fetch_all_for_clustering(
        self, 
        limit: Optional[int] = None
    ) -> Tuple[List[int], List[str], List[List[float]]]:
        """
        –í—ã–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ posts_clustering –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (post_ids, texts, embeddings):
                - post_ids: –°–ø–∏—Å–æ–∫ ID –ø–æ—Å—Ç–æ–≤
                - texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å—Ç–æ–≤
                - embeddings: –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∫–∞–∂–¥—ã–π - —Å–ø–∏—Å–æ–∫ float)
        """
        effective_limit = limit or self.config.max_posts_for_clustering
        effective_limit = min(effective_limit, self.config.max_posts_for_clustering)
        logger.info(f"–í—ã–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {self.config.clustering_collection} (–ª–∏–º–∏—Ç: {effective_limit})...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collections = self.qdrant_client.get_collections().collections
        collection_exists = any(c.name == self.config.clustering_collection for c in collections)
        
        if not collection_exists:
            logger.warning(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {self.config.clustering_collection} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ.")
            return [], [], []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        try:
            count_result = self.qdrant_client.count(self.config.clustering_collection)
            if count_result.count == 0:
                logger.warning(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {self.config.clustering_collection} –ø—É—Å—Ç–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ.")
                return [], [], []
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º scroll –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
        try:
            scroll_result = self.qdrant_client.scroll(
                collection_name=self.config.clustering_collection,
                limit=effective_limit,
                with_payload=True,
                with_vectors=True
            )
            
            points, _ = scroll_result
            
            post_ids = []
            texts = []
            embeddings = []
            
            for point in points:
                if point.payload and 'text' in point.payload and 'post_id' in point.payload:
                    post_ids.append(point.payload['post_id'])
                    texts.append(point.payload['text'])
                    embeddings.append(point.vector)
            
            logger.info(f"‚úÖ –í—ã–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
            return post_ids, texts, embeddings
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Qdrant: {e}")
            raise
    
    # ============================================================================
    # –®–ê–ì 4: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° BERTOPIC
    # ============================================================================
    
    async def build_topic_model(self) -> BERTopic:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ BERTopic
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ posts_clustering –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant.
        –ü–µ—Ä–µ–¥–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ BERTopic (–Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ö –∑–∞–Ω–æ–≤–æ).
        
        Returns:
            –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å BERTopic
        """
        logger.info("üî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ BERTopic...")
        
        # –í—ã–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        post_ids, texts, embeddings = await self.fetch_all_for_clustering()
        
        if len(texts) < self.config.hdbscan_min_cluster_size:
            raise ValueError(
                f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: "
                f"–Ω–∞–π–¥–µ–Ω–æ {len(texts)} –ø–æ—Å—Ç–æ–≤, —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º {self.config.hdbscan_min_cluster_size}"
            )
        
        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts)} –ø–æ—Å—Ç–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫
        if not UMAP_AVAILABLE:
            raise ImportError(
                "umap-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install umap-learn"
            )
        if not HDBSCAN_AVAILABLE:
            raise ImportError(
                "hdbscan –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install hdbscan"
            )
        if not BERTOPIC_AVAILABLE:
            raise ImportError(
                "bertopic –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install bertopic"
            )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º UMAP
        umap_model = UMAP(
            n_neighbors=self.config.umap_n_neighbors,
            n_components=self.config.umap_n_components,
            min_dist=self.config.umap_min_dist,
            metric='cosine',
            random_state=42
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º HDBSCAN
        hdbscan_model = HDBSCAN(
            min_cluster_size=self.config.hdbscan_min_cluster_size,
            min_samples=self.config.hdbscan_min_samples,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True  # –í–∞–∂–Ω–æ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º BERTopic
        # embedding_model=None, —Ç.–∫. –º—ã –ø–µ—Ä–µ–¥–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self._bertopic = BERTopic(
            embedding_model=None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            nr_topics=self.config.nr_topics,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ–ª–∫–∏—Ö —Ç–µ–º
            verbose=True
        )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º embeddings –≤ numpy array
        embeddings_array = np.array(embeddings)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        logger.info("üéì –û–±—É—á–µ–Ω–∏–µ BERTopic...")
        topics, probs = self._bertopic.fit_transform(texts, embeddings=embeddings_array)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º post_ids –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self._last_post_ids = post_ids
        self._last_topics = topics
        self._last_probs = probs
        self._last_texts = list(texts)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        unique_topics = set(topics)
        noise_count = sum(1 for t in topics if t == -1)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞:")
        logger.info(f"   - –ù–∞–π–¥–µ–Ω–æ —Ç–µ–º: {len(unique_topics) - (1 if -1 in unique_topics else 0)}")
        logger.info(f"   - –®—É–º (outliers): {noise_count}")
        logger.info(f"   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–µ–º—ã: {len(texts) / max(1, len(unique_topics) - 1):.1f}")
        
        return self._bertopic
    
    def get_topic_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–º–∞—Ö –∏–∑ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERTopic
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–µ–º–∞—Ö:
                - topics_df: DataFrame —Å —Ç–µ–º–∞–º–∏, –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, —Ä–∞–∑–º–µ—Ä–∞–º–∏
                - topic_sizes: –°–ª–æ–≤–∞—Ä—å {topic_id: —Ä–∞–∑–º–µ—Ä}
                - topic_keywords: –°–ª–æ–≤–∞—Ä—å {topic_id: [–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞]}
        """
        if self._bertopic is None:
            raise ValueError("–ú–æ–¥–µ–ª—å BERTopic –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ build_topic_model() —Å–Ω–∞—á–∞–ª–∞")
        
        # –ü–æ–ª—É—á–∞–µ–º DataFrame —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–µ–º–∞—Ö
        topics_df = self._bertopic.get_topic_info()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ç–µ–º
        topic_sizes = {}
        for topic_id in topics_df['Topic'].values:
            if topic_id != -1:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —à—É–º
                size = len([t for t in self._bertopic.topics_ if t == topic_id])
                topic_sizes[topic_id] = size
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
        topic_keywords = {}
        for topic_id in topics_df['Topic'].values:
            if topic_id != -1:
                keywords = self._bertopic.get_topic(topic_id)
                topic_keywords[topic_id] = [word for word, _ in keywords[:10]]  # –¢–æ–ø-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        return {
            "topics_df": topics_df,
            "topic_sizes": topic_sizes,
            "topic_keywords": topic_keywords
        }
    
    async def save_topics_to_db(
        self,
        topic_info: Optional[Dict[str, Any]] = None,
        texts: Optional[List[str]] = None,
        topics: Optional[List[int]] = None,
        post_ids: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ PostgreSQL
        
        –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã:
        - –°–æ–∑–¥–∞–µ—Ç –∑–∞–ø–∏—Å—å –≤ dedup_clusters (–∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é)
        - –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ Qwen2.5
        - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        
        –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞:
        - –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–≤—è–∑—å –≤ cluster_messages (post_id -> cluster_id)
        
        Args:
            topic_info: –†–µ–∑—É–ª—å—Ç–∞—Ç get_topic_info() - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–º–∞—Ö (–µ—Å–ª–∏ None, –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å—Ç–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç build_topic_model)
            topics: –°–ø–∏—Å–æ–∫ topic_id –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç)
            post_ids: –°–ø–∏—Å–æ–∫ ID –ø–æ—Å—Ç–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π:
                - clusters_created: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                - posts_linked: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
        """
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ PostgreSQL...")
        
        if self._bertopic is None:
            raise ValueError("–ú–æ–¥–µ–ª—å BERTopic –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ build_topic_model() —Å–Ω–∞—á–∞–ª–∞")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if topics is None:
            if self._last_topics is None:
                raise ValueError("–¢–µ–º—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã –∏ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã. –£–∫–∞–∂–∏—Ç–µ topics –∏–ª–∏ –≤—ã–∑–æ–≤–∏—Ç–µ build_topic_model()")
            topics = self._last_topics
        
        if post_ids is None:
            if self._last_post_ids is None:
                raise ValueError("ID –ø–æ—Å—Ç–æ–≤ –Ω–µ —É–∫–∞–∑–∞–Ω—ã –∏ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã. –£–∫–∞–∂–∏—Ç–µ post_ids –∏–ª–∏ –≤—ã–∑–æ–≤–∏—Ç–µ build_topic_model()")
            post_ids = self._last_post_ids
        
        if texts is None:
            # –ù—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –∏–∑ Qdrant –ø–æ post_ids
            # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è, –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–Ω–æ–≤–æ
            post_ids_from_qdrant, texts_from_qdrant, _ = await self.fetch_all_for_clustering()
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ post_id -> text
            text_map = {pid: text for pid, text in zip(post_ids_from_qdrant, texts_from_qdrant)}
            texts = [text_map.get(pid, "") for pid in post_ids]
        
        self._last_texts = texts
        
        if topic_info is None:
            topic_info = self.get_topic_info()
        
        if len(texts) != len(topics):
            raise ValueError(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤: {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤, {len(topics)} —Ç–µ–º")
        
        if len(post_ids) != len(texts):
            raise ValueError(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤: {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤, {len(post_ids)} ID")
        
        dsn = self._get_pg_dsn()
        conn = await asyncpg.connect(dsn=dsn)
        
        try:
            clusters_created = 0
            posts_linked = 0
            cluster_cards: List[Dict[str, Any]] = []
            keyword_counter: Counter = Counter()
            size_distribution: Counter = Counter()
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –ø–æ —Ç–µ–º–∞–º
            topic_to_posts: Dict[int, List[Tuple[int, str]]] = {}
            for i, topic_id in enumerate(topics):
                if topic_id == -1:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º
                    continue
                if topic_id not in topic_to_posts:
                    topic_to_posts[topic_id] = []
                topic_to_posts[topic_id].append((post_ids[i], texts[i]))
            
            eligible_topics = {
                topic_id: posts_data
                for topic_id, posts_data in topic_to_posts.items()
                if len(posts_data) >= self.config.hdbscan_min_cluster_size
            }
            total_topics = len(eligible_topics)
            completed_titles = 0
            if total_topics:
                # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                if self.config.use_openai_for_titles:
                    logger.info(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ OpenAI –¥–ª—è {total_topics} —Ç–µ–º...")
                else:
                    logger.info(f"üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (OpenAI –æ—Ç–∫–ª—é—á–µ–Ω) –¥–ª—è {total_topics} —Ç–µ–º...")
                
                self._progress_step("title_generation", "running", {
                    "topics": total_topics,
                    "completed": completed_titles
                })
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Ç–µ–º—É
            for topic_id, posts_data in eligible_topics.items():
                size_distribution[len(posts_data)] += 1
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                keywords = topic_info['topic_keywords'].get(topic_id, [])
                keyword_counter.update(keywords[:10])

                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤
                sample_texts = [
                    text[:500]
                    for _, text in posts_data[:self.config.num_sample_texts]
                ]
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ OpenAI (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
                title = None
                openai_gen = self.openai_generator  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
                if openai_gen is not None and self.config.use_openai_for_titles:
                    try:
                        logger.info(f"   –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ —á–µ—Ä–µ–∑ OpenAI –¥–ª—è —Ç–µ–º—ã {topic_id}...")
                        title_start = time.perf_counter()
                        title = await openai_gen.generate_title(
                            topic_id=topic_id,
                            keywords=keywords,
                            sample_texts=sample_texts,
                            temperature=self.config.openai_temperature,
                            max_tokens=self.config.openai_max_tokens
                        )
                        self._record_title_duration(time.perf_counter() - title_start)
                        logger.info(f"   ‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ OpenAI: {title[:50]}...")
                    except Exception as e:
                        logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —á–µ—Ä–µ–∑ OpenAI –¥–ª—è —Ç–µ–º—ã {topic_id}: {e}")
                else:
                    logger.debug(f"   OpenAI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –¥–ª—è —Ç–µ–º—ã {topic_id}")
                
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
                if not title or len(title.strip()) < 5:
                    if keywords:
                        # –£–ª—É—á—à–µ–Ω–Ω—ã–π fallback: —Ñ–æ—Ä–º–∏—Ä—É–µ–º –±–æ–ª–µ–µ —á–∏—Ç–∞–µ–º—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                        # –ë–µ—Ä–µ–º —Ç–æ–ø-3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ
                        if len(keywords) >= 3:
                            title = f"{keywords[0]}, {keywords[1]} –∏ {keywords[2]}"
                        elif len(keywords) == 2:
                            title = f"{keywords[0]} –∏ {keywords[1]}"
                        else:
                            title = keywords[0] if keywords else "–¢–µ–º–∞"
                    elif sample_texts:
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
                        first_words = sample_texts[0].split()[:8]
                        title = " ".join(first_words)
                    else:
                        title = f"–¢–µ–º–∞ {topic_id}"
                    
                    logger.info(f"   üìù –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è —Ç–µ–º—ã {topic_id}: {title[:50]}...")
                
                # –°–æ–∑–¥–∞–µ–º UUID –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
                cluster_id = str(uuid.uuid4())

                cluster_cards.append({
                    "topic_id": topic_id,
                    "title": (title or "").strip()[:self.config.max_title_length],
                    "keywords": keywords[:5],
                    "size": len(posts_data),
                    "sample": sample_texts[0][:200] if sample_texts else ""
                })
                title = cluster_cards[-1]["title"]
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä –≤ dedup_clusters
                await conn.execute("""
                    INSERT INTO dedup_clusters (
                        cluster_id, title, summary, created_at, stats
                    ) VALUES ($1, $2, $3, $4, $5::jsonb)
                    ON CONFLICT (cluster_id) DO UPDATE SET
                        title = EXCLUDED.title,
                        summary = EXCLUDED.summary,
                        updated_at = NOW(),
                        stats = EXCLUDED.stats
                """,
                    cluster_id,
                    title,
                    sample_texts[0][:500] if sample_texts else "",  # summary
                    datetime.now(),
                    json.dumps({
                        'message_count': len(posts_data),
                        'topic_id': topic_id,
                        'keywords': keywords[:10]
                    })
                )
                
                clusters_created += 1
                
                # –°–≤—è–∑—ã–≤–∞–µ–º –ø–æ—Å—Ç—ã —Å –∫–ª–∞—Å—Ç–µ—Ä–æ–º
                for post_id, _ in posts_data:
                    try:
                        await conn.execute("""
                            INSERT INTO cluster_messages (
                                cluster_id, message_id, similarity_score, is_primary
                            ) VALUES ($1, $2, $3, $4)
                            ON CONFLICT (cluster_id, message_id) DO UPDATE SET
                                similarity_score = EXCLUDED.similarity_score
                        """,
                            cluster_id,
                            post_id,
                            1.0,  # similarity_score (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è probs –∏–∑ BERTopic)
                            False  # is_primary (–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø–µ—Ä–≤—ã–π –∏–ª–∏ —Å–∞–º—ã–π —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π)
                        )
                        posts_linked += 1
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–≤—è–∑–∫–∏ –ø–æ—Å—Ç–∞ {post_id} –∫ –∫–ª–∞—Å—Ç–µ—Ä—É {cluster_id}: {e}")
                        continue
                
                completed_titles += 1
                self._progress_step("title_generation", "running", {
                    "topics": total_topics,
                    "completed": completed_titles,
                    "current_topic": int(topic_id) if isinstance(topic_id, (int, np.integer)) else topic_id
                })
                self._progress_log(
                    f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {completed_titles}/{total_topics} (—Ç–µ–º–∞ {topic_id})",
                    level="info"
                )
            
            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
            logger.info(f"   - –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {clusters_created}")
            logger.info(f"   - –ü—Ä–∏–≤—è–∑–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {posts_linked}")
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å OpenAI –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            if self._openai_generator is not None:
                logger.info("üßπ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ OpenAI –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤...")
                self._openai_generator.release_model()
                self._openai_generator = None
                gc.collect()
                # –û—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞ (–µ—Å–ª–∏ –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω)
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        logger.info("   ‚úÖ GPU –∫—ç—à –æ—á–∏—â–µ–Ω")
                except Exception:
                    pass
            
            return {
                "clusters_created": clusters_created,
                "posts_linked": posts_linked,
                "samples": cluster_cards[:10],
                "cluster_cards": cluster_cards,
                "keyword_cloud": [
                    {"text": word, "weight": count}
                    for word, count in keyword_counter.most_common(50)
                ],
                "size_distribution": dict(size_distribution)
            }
            
        finally:
            await conn.close()

    async def regenerate_titles(
        self,
        limit: Optional[int] = None
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ OpenAI –ø–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º."""
        openai_gen = self.openai_generator
        if openai_gen is None:
            raise RuntimeError("OpenAI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Topic Modeling –∏ OPENAI_API_KEY.")

        limit_value = limit if isinstance(limit, int) and limit > 0 else 100
        logger.info(f"üîÅ –ü–µ—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —á–µ—Ä–µ–∑ OpenAI (limit={limit_value})")
        start_time = time.perf_counter()
        conn = await asyncpg.connect(self._get_pg_dsn())
        updated = 0
        try:
            rows = await conn.fetch(
                """
                SELECT cluster_id, stats, summary
                FROM dedup_clusters
                ORDER BY updated_at DESC NULLS LAST
                LIMIT $1
                """,
                limit_value
            )

            for idx, row in enumerate(rows, start=1):
                stats = row.get('stats') or {}
                keywords = []
                if isinstance(stats, dict):
                    keywords = stats.get('keywords') or []
                elif isinstance(stats, str):
                    try:
                        keywords = json.loads(stats).get('keywords', [])
                    except Exception:
                        keywords = []

                if not isinstance(keywords, list):
                    keywords = list(keywords) if keywords else []

                sample_texts = [str(row['summary'])] if row.get('summary') else []
                logger.info(f"   –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ —á–µ—Ä–µ–∑ OpenAI –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {row['cluster_id']} ({idx}/{len(rows)})")
                title_start = time.perf_counter()
                title = await openai_gen.generate_title(
                    topic_id=idx,
                    keywords=keywords,
                    sample_texts=sample_texts,
                    temperature=self.config.openai_temperature,
                    max_tokens=self.config.openai_max_tokens
                )
                self._record_title_duration(time.perf_counter() - title_start)

                await conn.execute(
                    """
                    UPDATE dedup_clusters
                    SET title = $1, updated_at = NOW()
                    WHERE cluster_id = $2
                    """,
                    title[:self.config.max_title_length],
                    row['cluster_id']
                )
                updated += 1

        finally:
            await conn.close()

        duration = time.perf_counter() - start_time
        logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {updated} –∑–∞–ø–∏—Å–µ–π –∑–∞ {duration:.1f}c")
        return {"processed": updated, "limit": limit_value, "duration": round(duration, 1)}
    
    # ============================================================================
    # –®–ê–ì 7: –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î - –ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù
    # ============================================================================
    
    async def run_full_pipeline(
        self,
        new_posts: Optional[List[Dict[str, Any]]] = None,
        fetch_from_db: bool = True,
        run_classification: bool = True
    ) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Ç—Ä–µ–∫–∏–Ω–≥–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
        """
        start_time = time.perf_counter()
        self._timings.clear()
        self._resource_usage = {"peak_ram_gb": 0.0}
        self._title_stats = {"count": 0, "durations": []}
        qdrant_stats: Dict[str, Optional[Dict[str, Any]]] = {"search": None, "clustering": None}
        current_step = None

        settings_snapshot = {
            "fetch_from_db": fetch_from_db,
            "max_posts_for_clustering": self.config.max_posts_for_clustering,
            "batch_size_qdrant": self.config.batch_size_qdrant,
            "use_openai_for_titles": self.config.use_openai_for_titles
        }
        self._progress_start(settings_snapshot)

        try:
            # –®–ê–ì 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã
            current_step = "fetch_posts"
            self._progress_step(current_step, "running", {
                "mode": "postgresql" if fetch_from_db else "manual"
            })
            step_start = time.perf_counter()
            if new_posts is None and fetch_from_db:
                new_posts = await self._fetch_posts_from_db(
                    limit=None,
                    days_back=self.config.rerun_interval_hours // 24 if self.config.rerun_interval_hours >= 24 else 30
                )
            elif new_posts is None:
                new_posts = []
            fetch_duration = time.perf_counter() - step_start
            self._record_timing("fetch_posts", fetch_duration)
            self._progress_step(current_step, "done", {
                "count": len(new_posts),
                "duration": round(fetch_duration, 2)
            })
            self._progress_log(f"–ü–æ–ª—É—á–µ–Ω–æ {len(new_posts)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", "info")
            self._check_cancellation()

            # –®–ê–ì 2-4. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
            posts_indexed = len(new_posts)
            if new_posts:
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å OpenAI –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π FRIDA (–µ—Å–ª–∏ OpenAI –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω)
                if self._openai_generator is not None:
                    logger.info("üßπ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ OpenAI –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π FRIDA...")
                    self._openai_generator.release_model()
                    self._openai_generator = None
                    gc.collect()
                    # –û—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                    try:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            logger.info("   ‚úÖ GPU –∫—ç—à –æ—á–∏—â–µ–Ω")
                    except Exception:
                        pass
                
                self._progress_step("frida_embeddings", "running", {"count": len(new_posts)})
                self._progress_step("qdrant_indexing", "running", {"collections": 2})
                step_start = time.perf_counter()
                qdrant_stats["search"] = await self.upsert_to_search_index(new_posts)
                frida_duration = time.perf_counter() - step_start
                self._record_timing("frida_embeddings", frida_duration)
                self._progress_step("frida_embeddings", "done", {
                    "vectors": qdrant_stats["search"]["points"],
                    "duration": round(frida_duration, 2)
                })
                self._progress_log(f"FRIDA –∑–∞–≤–µ—Ä—à–∏–ª–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é {len(new_posts)} –ø–æ—Å—Ç–æ–≤ –∑–∞ {frida_duration:.1f}—Å", "info")
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π GTE
                self._progress_log("–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ FRIDA –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π GTE", "info")
                self._frida_embedder = None
                gc.collect()

                self._check_cancellation()
                self._progress_step("gte_embeddings", "running", {"count": len(new_posts)})
                step_start = time.perf_counter()
                qdrant_stats["clustering"] = await self.upsert_to_clustering_index(new_posts)
                gte_duration = time.perf_counter() - step_start
                self._record_timing("gte_embeddings", gte_duration)
                self._progress_step("gte_embeddings", "done", {
                    "vectors": qdrant_stats["clustering"]["points"],
                    "duration": round(gte_duration, 2)
                })
                self._progress_log(f"GTE –∑–∞–≤–µ—Ä—à–∏–ª–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é {len(new_posts)} –ø–æ—Å—Ç–æ–≤ –∑–∞ {gte_duration:.1f}—Å", "info")
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ BERTopic
                self._progress_log("–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ GTE –ø–µ—Ä–µ–¥ BERTopic", "info")
                self._gte_embedder = None
                gc.collect()

                total_batches = (qdrant_stats["search"]["batches"] + qdrant_stats["clustering"]["batches"])
                self._progress_step("qdrant_indexing", "done", {"batches_total": total_batches})
            else:
                self._progress_step("frida_embeddings", "skipped", {"reason": "–ù–µ—Ç –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤"})
                self._progress_step("gte_embeddings", "skipped", {"reason": "–ù–µ—Ç –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤"})
                self._progress_step("qdrant_indexing", "skipped", {"reason": "–ù–µ—Ç –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤"})

            self._check_cancellation()

            # –®–ê–ì 5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BERTopic
            current_step = "bertopic"
            self._progress_step(current_step, "running", {
                "limit": self.config.max_posts_for_clustering
            })
            step_start = time.perf_counter()
            bertopic_model = await self.build_topic_model()
            bertopic_duration = time.perf_counter() - step_start
            self._record_timing("bertopic", bertopic_duration)
            documents_count = len(self._last_topics or [])
            self._progress_step(current_step, "done", {
                "documents": documents_count,
                "duration": round(bertopic_duration, 2)
            })

            topic_info = self.get_topic_info()
            topics_count = len(topic_info["topic_sizes"])
            self._progress_log(f"BERTopic –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {topics_count} —Ç–µ–º", "info")

            # –®–ê–ì 6-7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self._progress_step("title_generation", "running", {"topics": topics_count})
            self._progress_step("save_to_db", "running", {})
            save_stats = await self.save_topics_to_db(topic_info=topic_info)
            title_avg = round(mean(self._title_stats["durations"]), 3) if self._title_stats["durations"] else 0.0
            self._progress_step("title_generation", "done", {
                "generated": self._title_stats["count"],
                "avg_time_sec": title_avg
            })
            self._progress_step("save_to_db", "done", {
                "clusters": save_stats["clusters_created"],
                "posts_linked": save_stats["posts_linked"]
            })
            self._progress_log(
                f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {save_stats['clusters_created']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –ø—Ä–∏–≤—è–∑–∞–Ω–æ {save_stats['posts_linked']} –ø–æ—Å—Ç–æ–≤",
                "info"
            )

            # –®–ê–ì 8. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            classification_stats = None
            if run_classification:
                try:
                    current_step = "classification"
                    self._progress_step(current_step, "running", {})
                    step_start = time.perf_counter()
                    
                    from pro_mode.classification_service import ClassificationService
                    classification_service = ClassificationService()
                    
                    # –ü–æ–ª—É—á–∞–µ–º ID —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    message_ids = [post.get('post_id') for post in new_posts if post.get('post_id')]
                    if not message_ids and fetch_from_db:
                        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                        message_ids = None
                    
                    classification_stats = await classification_service.classify_all_messages_in_pipeline(
                        message_ids=message_ids,
                        limit=len(new_posts) if new_posts else None
                    )
                    
                    classification_duration = time.perf_counter() - step_start
                    self._record_timing("classification", classification_duration)
                    self._progress_step(current_step, "done", {
                        "processed": classification_stats.get('processed', 0),
                        "classified": classification_stats.get('classified', 0),
                        "success_rate": round(classification_stats.get('success_rate', 0), 1),
                        "duration": round(classification_duration, 2)
                    })
                    self._progress_log(
                        f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {classification_stats.get('processed', 0)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, "
                        f"{classification_stats.get('classified', 0)} –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ "
                        f"({classification_stats.get('success_rate', 0):.1f}%)",
                        "info"
                    )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")
                    self._progress_step(current_step, "error", {"message": str(e)})
                    classification_stats = {
                        'processed': 0,
                        'classified': 0,
                        'errors': 1,
                        'success_rate': 0
                    }
            else:
                logger.info("‚è≠Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ (–æ—Ç–∫–ª—é—á–µ–Ω–∞)")

            execution_time = time.perf_counter() - start_time
            metrics = self._build_metrics(
                topic_info=topic_info,
                save_stats=save_stats,
                qdrant_stats=qdrant_stats,
                execution_time=execution_time,
                posts_indexed=posts_indexed,
                topics_count=topics_count
            )
            self._progress_metrics(metrics)

            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞
            classification_data = None
            if run_classification and classification_stats:
                try:
                    classification_data = await self._get_classification_data()
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞: {e}")

            report_payload = self._build_topic_modeling_report(
                new_posts=new_posts,
                topic_info=topic_info,
                save_stats=save_stats,
                metrics=metrics,
                posts_indexed=posts_indexed,
                fetch_mode="postgresql" if fetch_from_db else "manual",
                classification_data=classification_data
            )
            self._write_topic_modeling_report(report_payload)

            result = {
                "posts_indexed": posts_indexed,
                "topics_found": topics_count,
                "clusters_created": save_stats["clusters_created"],
                "posts_linked": save_stats["posts_linked"],
                "execution_time": execution_time,
                "metrics": metrics,
                "classification": classification_stats
            }

            logger.info("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            logger.info(f"   –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {posts_indexed}")
            logger.info(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–µ–º: {topics_count}")
            logger.info(f"   –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {save_stats['clusters_created']}")
            logger.info(f"   –ü—Ä–∏–≤—è–∑–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {save_stats['posts_linked']}")
            logger.info(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f} —Å–µ–∫")

            if self.progress_tracker:
                self.progress_tracker.finish("success", result)
            return result

        except TopicModelingCancelled as cancel_exc:
            if current_step:
                self._progress_step(current_step, "cancelled", {"message": str(cancel_exc)})
            execution_time = time.perf_counter() - start_time
            partial_result = {
                "posts_indexed": len(new_posts) if new_posts else 0,
                "topics_found": len(set(self._last_topics or [])) - 1 if self._last_topics else 0,
                "clusters_created": 0,
                "posts_linked": 0,
                "execution_time": execution_time,
                "metrics": {}
            }
            if self.progress_tracker:
                self.progress_tracker.finish("cancelled", partial_result, error=str(cancel_exc))
            logger.warning("‚ö†Ô∏è –ü–∞–π–ø–ª–∞–π–Ω –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            self._progress_log("–ü–∞–π–ø–ª–∞–π–Ω –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º", "warning")
            raise
        except Exception as e:
            if current_step:
                self._progress_step(current_step, "error", {"message": str(e)})
            execution_time = time.perf_counter() - start_time
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            self._progress_log(f"–û—à–∏–±–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}", "error")
            if self.progress_tracker:
                self.progress_tracker.finish(
                    "error",
                    {"execution_time": execution_time},
                    error=str(e)
                )
            raise
    
    async def _fetch_posts_from_db(
        self,
        limit: Optional[int] = None,
        days_back: int = 30
    ) -> List[Dict[str, Any]]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å—Ç—ã –∏–∑ PostgreSQL –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ (–µ—Å–ª–∏ None, –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
            days_back: –ó–∞–≥—Ä—É–∂–∞—Ç—å –ø–æ—Å—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤: [{"post_id": int, "text": str, "timestamp": datetime}, ...]
        """
        logger.info(f"üì• –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ—Å—Ç–æ–≤ –∏–∑ PostgreSQL (days_back={days_back}, limit={limit})")
        dsn = self._get_pg_dsn()
        logger.debug(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL: {dsn.split('@')[1] if '@' in dsn else '—Å–∫—Ä—ã—Ç–æ'}")
        
        conn = await asyncpg.connect(dsn=dsn)
        logger.debug("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ —Ç–∞–±–ª–∏—Ü—ã messages
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º text_content –∫–∞–∫ —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            # –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å INTERVAL –¥–ª—è PostgreSQL
            query = """
                SELECT id, text_content, published_at
                FROM messages
                WHERE text_content IS NOT NULL 
                  AND text_content != ''
                  AND LENGTH(TRIM(text_content)) >= 10
                  AND published_at >= NOW() - ($1 || ' days')::INTERVAL
                ORDER BY published_at DESC
            """
            
            logger.debug(f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –∑–∞–ø—Ä–æ—Å–∞: days_back={days_back}, limit={limit}")
            if limit:
                query += " LIMIT $2"
                rows = await conn.fetch(query, str(days_back), limit)
            else:
                rows = await conn.fetch(query, str(days_back))
            
            logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(rows)} —Å—Ç—Ä–æ–∫ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
            
            posts = []
            skipped = 0
            for row in rows:
                text = row['text_content']
                # –í–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                if not text or len(text.strip()) < 10:
                    skipped += 1
                    continue
                # –û–±—Ä–µ–∑–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (–¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
                if len(text) > 10000:
                    text = text[:10000] + "..."
                
                posts.append({
                    "post_id": row['id'],
                    "text": text.strip(),
                    "timestamp": row['published_at']
                })
            
            if skipped > 0:
                logger.info(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {skipped} –ø–æ—Å—Ç–æ–≤ (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ)")
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(posts)} –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –∏–∑ PostgreSQL")
            return posts
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å—Ç–æ–≤ –∏–∑ PostgreSQL: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
        finally:
            await conn.close()
            logger.debug("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –∑–∞–∫—Ä—ã—Ç–æ")


# ============================================================================
# –®–ê–ì 5: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–ì–û–õ–û–í–ö–û–í –ß–ï–†–ï–ó OPENAI GPT
# ============================================================================
# OpenAI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ pro_mode/openai_title_generator.py
# ============================================================================


# ============================================================================
# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# ============================================================================

async def example_usage():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è TopicModelingService
    
    –≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ—Ä–≤–∏—Å –¥–ª—è:
    1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
    2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
    3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
    service = TopicModelingService()
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å—Ç—ã –∏–∑ –ë–î, –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç, —Å—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª—å –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç
    result = await service.run_full_pipeline(fetch_from_db=True)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
    
    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ—à–∞–≥–æ–≤–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    # 1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
    new_posts = [
        {
            "post_id": 1,
            "text": "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–∞",
            "timestamp": datetime.now()
        }
    ]
    await service.upsert_to_search_index(new_posts)
    await service.upsert_to_clustering_index(new_posts)
    
    # 2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    bertopic_model = await service.build_topic_model()
    
    # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–º–∞—Ö
    topic_info = service.get_topic_info()
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–µ–º: {len(topic_info['topic_sizes'])}")
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    save_stats = await service.save_topics_to_db()
    print(f"–°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {save_stats['clusters_created']}")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–∞
    asyncio.run(example_usage())
