"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–æ–±—ã—Ç–∏–π
"""

import asyncio
import logging
import uuid
from typing import List, Dict, Any, Optional, Tuple
import json
from datetime import datetime, timedelta
import asyncpg
from config_utils import get_config
from pro_mode.embedding_service import embedding_service
import numpy as np

# HDBSCAN –∏ PCA –∏–º–ø–æ—Ä—Ç—ã
try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False

# PCA –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ sklearn
try:
    from sklearn.decomposition import PCA
    PCA_AVAILABLE = True
except ImportError:
    PCA_AVAILABLE = False

logger = logging.getLogger(__name__)

class SemanticClusteringService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ —Å–æ–±—ã—Ç–∏–π"""
    
    def __init__(self):
        self.similarity_threshold = 0.75  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ –∫–ª–∞—Å—Ç–µ—Ä
        self.max_cluster_age_days = 7  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        self.min_cluster_size = 2  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.search_window_size = 30  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞ (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 10 –¥–æ 30)
        self.adaptive_threshold_enabled = True  # –í–∫–ª—é—á–∏—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–∞
        
        # LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —É–¥–∞–ª–µ–Ω (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è Yandex GPT)
        # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ fallback –º–µ—Ç–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        self.llm_generator = None
    
    async def process_new_message(self, message_id: int, text: str, channel_id: int, 
                                published_at: datetime) -> Optional[str]:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä (DEPRECATED - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ run_hdbscan_clustering)"""
        try:
            # –ë—ã—Å—Ç—Ä—ã–π –≤—ã—Ö–æ–¥, –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –ø—Ä–∏–≤—è–∑–∞–Ω–æ –∫ –∫–ª–∞—Å—Ç–µ—Ä—É
            try:
                config = get_config()
                conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
                existing_cluster = await conn.fetchval(
                    "SELECT cluster_id FROM cluster_messages WHERE message_id = $1 LIMIT 1",
                    message_id
                )
                await conn.close()
                if existing_cluster:
                    logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ {message_id} —É–∂–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ {existing_cluster}, –ø—Ä–æ–ø—É—Å–∫")
                    return existing_cluster
            except Exception:
                pass

            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–æ–±—â–µ–Ω–∏—è
            embedding = await embedding_service.provider.get_embedding(text)
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –Ω–µ–¥–∞–≤–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
            similar_messages = await self._find_similar_messages(
                embedding, published_at, limit=self.search_window_size
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
            effective_threshold = self.similarity_threshold
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –µ—Å–ª–∏ —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞, –Ω–æ –Ω–µ —Å–∏–ª—å–Ω–æ
            # (—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç–æ–ø-1 –∏ —Ç–æ–ø-2/3 –Ω–µ–±–æ–ª—å—à–∞—è), —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–ª–æ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
            # –∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∂–µ—Å—Ç–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            if similar_messages and len(similar_messages) >= 2:
                top_score = similar_messages[0]['score']
                second_score = similar_messages[1]['score']
                score_spread = top_score - second_score
                
                # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –º–∞–ª–µ–Ω—å–∫–∞—è (–ø–ª–æ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å), –ø–æ–¥–Ω–∏–º–∞–µ–º –ø–æ—Ä–æ–≥
                if score_spread < 0.05 and top_score >= self.similarity_threshold:
                    # –í –ø–ª–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∂–µ—Å—Ç–∫–∏–π –ø–æ—Ä–æ–≥
                    effective_threshold = max(self.similarity_threshold, top_score * 0.98)
                    logger.debug(f"–ü–ª–æ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥: {effective_threshold:.3f}")
            
            if similar_messages and similar_messages[0]['score'] >= effective_threshold:
                # –ü—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å cluster_id –∏–∑ payload –ø–æ—Ö–æ–∂–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                payload = similar_messages[0].get('payload') or {}
                cluster_id = payload.get('cluster_id')
                if not cluster_id:
                    # –ï—Å–ª–∏ –≤ payload –Ω–µ—Ç cluster_id, –ø—Ä–æ–≤–µ—Ä–∏–º –≤ –ë–î, –ø—Ä–∏–≤—è–∑–∞–Ω–æ –ª–∏ –ø–æ—Ö–æ–∂–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ –∫–∞–∫–æ–º—É-–ª–∏–±–æ –∫–ª–∞—Å—Ç–µ—Ä—É
                    try:
                        config = get_config()
                        conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
                        existing = await conn.fetchval(
                            "SELECT cluster_id FROM cluster_messages WHERE message_id = $1 LIMIT 1",
                            payload.get('message_id')
                        )
                        await conn.close()
                        cluster_id = existing
                    except Exception:
                        cluster_id = None
                
                if cluster_id:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑–¥—É–≤–∞–Ω–∏–µ)
                    config = get_config()
                    conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
                    cluster_size = await conn.fetchval("""
                        SELECT COUNT(*) FROM cluster_messages WHERE cluster_id = $1
                    """, cluster_id)
                    await conn.close()
                    
                    # –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
                    if cluster_size > 50:
                        logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({cluster_size} —Å–æ–æ–±—â–µ–Ω–∏–π), —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}")
                        cluster_id = await self._create_new_cluster(message_id, text, channel_id, published_at)
                        logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä {cluster_id} –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}")
                        return cluster_id
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫ –Ω–∞–π–¥–µ–Ω–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É
                    await self._add_message_to_cluster(message_id, cluster_id, similar_messages[0]['score'])
                    logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ {message_id} –¥–æ–±–∞–≤–ª–µ–Ω–æ –∫ –∫–ª–∞—Å—Ç–µ—Ä—É {cluster_id} (size={cluster_size+1})")
                    return cluster_id
                
                cluster_id = await self._create_new_cluster(message_id, text, channel_id, published_at)
                logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä {cluster_id} –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}")
                return cluster_id
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                cluster_id = await self._create_new_cluster(message_id, text, channel_id, published_at)
                logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä {cluster_id} –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}")
                return cluster_id
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}: {e}")
            raise
    
    async def _find_similar_messages(self, embedding: List[float], published_at: datetime, 
                                   limit: int = 5) -> List[Dict[str, Any]]:
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –Ω–µ–¥–∞–≤–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö"""
        try:
            # –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—Ç–æ–ª—å–∫–æ –Ω–µ–¥–∞–≤–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
            date_from = published_at - timedelta(days=self.max_cluster_age_days)
            
            filters = {
                'date_from': date_from.isoformat(),
                'date_to': published_at.isoformat()
            }
            
            # –ò—â–µ–º —á–µ—Ä–µ–∑ Qdrant
            results = await embedding_service.qdrant.search_similar(
                query_vector=embedding,
                limit=limit,
                filters=filters
            )
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ–≤
            if self.adaptive_threshold_enabled and results and len(results) > 3:
                # –í—ã—á–∏—Å–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å - —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç–æ–ø-1 –∏ —Ç–æ–ø-3
                top_scores = [r['score'] for r in results[:min(5, len(results))]]
                if len(top_scores) >= 3:
                    score_gap = top_scores[0] - top_scores[2]
                    
                    # –ï—Å–ª–∏ —Ä–∞–∑—Ä—ã–≤ –º–∞–ª–µ–Ω—å–∫–∏–π - –º–Ω–æ–≥–æ –ø–æ—Ö–æ–∂–∏—Ö, –º–æ–∂–Ω–æ –ø–æ–¥–Ω—è—Ç—å –ø–æ—Ä–æ–≥
                    # –ï—Å–ª–∏ —Ä–∞–∑—Ä—ã–≤ –±–æ–ª—å—à–æ–π - –º–∞–ª–æ –ø–æ—Ö–æ–∂–∏—Ö, –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥
                    if score_gap > 0.15:
                        # –ë–∏–Ω–∞—Ä–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ - –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∂–µ—Å—Ç–∫–∏–π –ø–æ—Ä–æ–≥
                        adaptive_threshold = top_scores[0] * 0.95
                    else:
                        # –ü–ª–æ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π –ø–æ—Ä–æ–≥
                        adaptive_threshold = self.similarity_threshold
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –≤—ã—à–µ –±–∞–∑–æ–≤–æ–≥–æ
                    if adaptive_threshold > self.similarity_threshold:
                        filtered_results = [r for r in results if r['score'] >= adaptive_threshold]
                        if filtered_results:
                            logger.debug(f"–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥: {adaptive_threshold:.3f} (–±–∞–∑–æ–≤—ã–π: {self.similarity_threshold:.3f})")
                            return filtered_results
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")
            return []
    
    async def _create_new_cluster(self, message_id: int, text: str, channel_id: int, 
                                published_at: datetime) -> str:
        """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä —Å–æ–±—ã—Ç–∏–π"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            cluster_id = str(uuid.uuid4())
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ LLM —Å fallback
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–±—É–¥—É—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –≤ run_clustering)
            title = await self._generate_event_title(
                text, 
                cluster_id,
                max_texts=getattr(self, '_current_max_title_texts', 10),
                max_chars_per_text=getattr(self, '_current_max_title_chars_per_text', 500)
            )
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä –≤ –ë–î
            await conn.execute("""
                INSERT INTO dedup_clusters (cluster_id, title, summary, created_at, stats)
                VALUES ($1, $2, $3, $4, $5::jsonb)
            """, cluster_id, title, text[:500], published_at, json.dumps({
                'message_count': 1,
                'channel_count': 1,
                'channels': [channel_id]
            }))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∫–ª–∞—Å—Ç–µ—Ä (–ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç similarity 1.0)
            await conn.execute("""
                INSERT INTO cluster_messages (cluster_id, message_id, similarity_score, is_primary)
                VALUES ($1, $2, $3, $4)
                ON CONFLICT (cluster_id, message_id) DO UPDATE SET similarity_score = EXCLUDED.similarity_score
            """, cluster_id, message_id, 1.0, True)

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º primary_topic_id –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ —Ç–æ–ø-–º–µ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
            try:
                primary_topic_id = await conn.fetchval(
                    """
                    SELECT topic_id
                    FROM message_topics
                    WHERE message_id = $1
                    ORDER BY score DESC
                    LIMIT 1
                    """,
                    message_id
                )
                if primary_topic_id is not None:
                    await conn.execute(
                        "UPDATE dedup_clusters SET primary_topic_id = $1, updated_at = NOW() WHERE cluster_id = $2",
                        int(primary_topic_id), cluster_id
                    )
            except Exception:
                pass
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ Qdrant —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
            embedding = await embedding_service.provider.get_embedding(text)
            payload = {
                'message_id': message_id,
                'channel_id': channel_id,
                'date': published_at.isoformat(),
                'cluster_id': cluster_id,
                'text_preview': text[:200] + "..." if len(text) > 200 else text
            }
            
            # –∏—Å–ø–æ–ª—å–∑—É–µ–º —á–∏—Å–ª–æ–≤–æ–π ID —Ç–æ—á–∫–∏, –∫–∞–∫ –∏ –≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            await embedding_service.qdrant.upsert_embedding(message_id, embedding, payload)
            
            await conn.close()
            return cluster_id
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
            raise

    async def backfill_primary_topics(self) -> int:
        """–ü—Ä–æ—Å—Ç–∞–≤–∏—Ç—å primary_topic_id –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≥–¥–µ –æ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            updated = await conn.execute(
                """
                WITH ranked_topics AS (
                    SELECT dc.cluster_id,
                           mt.topic_id,
                           ROW_NUMBER() OVER (
                               PARTITION BY dc.cluster_id
                               ORDER BY MAX(mt.score) DESC
                           ) AS rn
                    FROM dedup_clusters dc
                    JOIN cluster_messages cm ON cm.cluster_id = dc.cluster_id
                    JOIN message_topics mt ON mt.message_id = cm.message_id
                    WHERE dc.primary_topic_id IS NULL
                    GROUP BY dc.cluster_id, mt.topic_id
                )
                UPDATE dedup_clusters dc
                SET primary_topic_id = rt.topic_id,
                    updated_at = NOW()
                FROM ranked_topics rt
                WHERE dc.cluster_id = rt.cluster_id AND rt.rn = 1
                """
            )
            await conn.close()
            # asyncpg returns e.g. 'UPDATE 42'
            try:
                return int(str(updated).split(' ')[-1])
            except Exception:
                return 0
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±—ç–∫—Ñ–∏–ª–ª–∞ primary_topic_id: {e}")
            return 0
    
    async def _add_message_to_cluster(self, message_id: int, cluster_id: str, similarity_score: float):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–ª–∞—Å—Ç–µ—Ä –≤ PostgreSQL
            cluster_exists = await conn.fetchval(
                "SELECT 1 FROM dedup_clusters WHERE cluster_id = $1 LIMIT 1",
                cluster_id
            )
            
            if not cluster_exists:
                logger.warning(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ PostgreSQL, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}")
                await conn.close()
                return
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∫–ª–∞—Å—Ç–µ—Ä
            insert_result = await conn.execute("""
                INSERT INTO cluster_messages (cluster_id, message_id, similarity_score, is_primary)
                VALUES ($1, $2, $3, $4)
                ON CONFLICT (cluster_id, message_id) DO NOTHING
            """, cluster_id, message_id, similarity_score, False)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–ª–∞—Å—Ç–µ—Ä–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—É—é —Å–≤—è–∑—å
            if insert_result and insert_result.endswith(" 1"):
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                await conn.execute("""
                    UPDATE dedup_clusters 
                    SET stats = jsonb_set(
                        jsonb_set(stats, '{message_count}', to_jsonb((stats->>'message_count')::int + 1)),
                        '{channel_count}', to_jsonb(array_length(array(SELECT DISTINCT m.channel_id FROM cluster_messages cm JOIN messages m ON cm.message_id = m.id WHERE cm.cluster_id = $1), 1))
                    ),
                    updated_at = NOW()
                    WHERE cluster_id = $1
                """, cluster_id)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM/–∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
                try:
                    new_title = await self._generate_event_title(
                        "", 
                        cluster_id,
                        max_texts=getattr(self, '_current_max_title_texts', 10),
                        max_chars_per_text=getattr(self, '_current_max_title_chars_per_text', 500)
                    )
                    await conn.execute("""
                        UPDATE dedup_clusters 
                        SET title = $1, updated_at = NOW()
                        WHERE cluster_id = $2
                    """, new_title, cluster_id)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
                
                # –í–ê–ñ–ù–û: –û–±–Ω–æ–≤–ª—è–µ–º payload –≤ Qdrant —Å –Ω–æ–≤—ã–º cluster_id
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è payload
                    message_data = await conn.fetchrow("""
                        SELECT m.text_content, m.channel_id, m.published_at
                        FROM messages m
                        WHERE m.id = $1
                    """, message_id)
                    
                    if message_data:
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                        embedding = await embedding_service.provider.get_embedding(message_data['text_content'])
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º payload –≤ Qdrant —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º cluster_id
                        payload = {
                            'message_id': message_id,
                            'channel_id': message_data['channel_id'],
                            'date': message_data['published_at'].isoformat(),
                            'cluster_id': cluster_id,
                            'text_preview': message_data['text_content'][:200] + "..." if len(message_data['text_content']) > 200 else message_data['text_content']
                        }
                        
                        await embedding_service.qdrant.upsert_embedding(message_id, embedding, payload)
                        logger.debug(f"–û–±–Ω–æ–≤–ª–µ–Ω payload –≤ Qdrant –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {message_id} —Å cluster_id {cluster_id}")
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è payload –≤ Qdrant –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {message_id}: {e}")
            
            await conn.close()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è –∫ –∫–ª–∞—Å—Ç–µ—Ä—É: {e}")
            raise
    
    async def _generate_event_title(self, text: str, cluster_id: str = None,
                                   max_texts: int = 10, max_chars_per_text: int = 500) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ LLM —Å fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            texts = [text]
            if cluster_id:
                try:
                    config = get_config()
                    conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
                    cluster_texts = await conn.fetch("""
                        SELECT m.text_content
                        FROM cluster_messages cm
                        JOIN messages m ON cm.message_id = m.id
                        WHERE cm.cluster_id = $1 AND m.text_content IS NOT NULL
                        ORDER BY cm.similarity_score DESC NULLS LAST
                    """, cluster_id)
                    if cluster_texts:
                        texts = [row['text_content'] for row in cluster_texts]
                    await conn.close()
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ (fallback –º–µ—Ç–æ–¥)
            import re
            from collections import Counter
            
            # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏ –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ
            def extract_key_phrases(text):
                # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                text = re.sub(r'https?://\S+|www\.\S+', '', text)
                text = re.sub(r'@\S+', '', text)
                text = re.sub(r'[üîπüü©üìπ‚ö°Ô∏è‚ùóÔ∏èüé•üíªüöóüìùüóû]', '', text)
                
                # –ò—â–µ–º –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ (—Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã)
                proper_nouns = re.findall(r'\b[–ê-–Ø–Å][–∞-—è—ë]+\b', text)
                
                # –ò—â–µ–º –≤–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ –∫–∞–≤—ã—á–∫–∞—Ö
                quoted_terms = re.findall(r'¬´([^¬ª]+)¬ª', text)
                
                # –ò—â–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
                tech_terms = re.findall(r'\b[–ê-–Ø–Å]{2,}\b', text)  # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
                
                # –ò—â–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                geo_terms = re.findall(r'\b(?:–†–æ—Å—Å–∏—è|–£–∫—Ä–∞–∏–Ω–∞|–°–®–ê|–ï–°|–ù–ê–¢–û|–ú–æ—Å–∫–≤–∞|–ö–∏–µ–≤|–í–∞—à–∏–Ω–≥—Ç–æ–Ω|–ë—Ä—é—Å—Å–µ–ª—å|–ü–∞—Ä–∏–∂|–ë–µ—Ä–ª–∏–Ω|–õ–æ–Ω–¥–æ–Ω|–¢–æ–∫–∏–æ|–ü–µ–∫–∏–Ω)\b', text)
                
                return proper_nouns + quoted_terms + tech_terms + geo_terms
            
            # –°–æ–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
            all_phrases = []
            for doc in texts:
                phrases = extract_key_phrases(doc)
                all_phrases.extend(phrases)
            
            if not all_phrases:
                # Fallback: –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                words = re.findall(r'\b[–∞-—è—ë–ê-–Ø–Åa-zA-Z]{4,}\b', text)
                return ' '.join(words[:3]) if words else text[:50]
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Ñ—Ä–∞–∑
            phrase_freq = Counter(all_phrases)
            
            # –ë–µ—Ä–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ñ—Ä–∞–∑—ã (–∏—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ)
            common_words = {'—ç—Ç–æ', '—á—Ç–æ', '–∫–∞–∫', '–¥–ª—è', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–∏–ª–∏', '–≤–æ—Ç', '–≤—Å–µ', '–±—ã—Ç—å'}
            top_phrases = [phrase for phrase, freq in phrase_freq.most_common(10) 
                          if phrase.lower() not in common_words and len(phrase) > 2]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ —Ç–æ–ø-3 —Ñ—Ä–∞–∑
            if top_phrases:
                title_phrases = top_phrases[:3]
                title = ' ‚Ä¢ '.join(title_phrases)
                return title[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            
            # Fallback: –ø–µ—Ä–≤—ã–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
            words = re.findall(r'\b[–∞-—è—ë–ê-–Ø–Åa-zA-Z]{4,}\b', text)
            return ' '.join(words[:3]) if words else text[:50]
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞
            return ' '.join(text.split()[:4]) if len(text.split()) >= 4 else text[:100]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —á–µ—Ä–µ–∑ TF-IDF: {e}")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥
            words = text.split()[:4]
            return " ".join(words) + ("..." if len(text.split()) > 4 else "")
    
    async def get_event_clusters(self, limit: int = 20, offset: int = 0, 
                               filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–æ–±—ã—Ç–∏–π (–∫–ª–∞—Å—Ç–µ—Ä–æ–≤)"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            where_clauses = []
            params = []
            param_count = 0
            
            if filters:
                if 'date_from' in filters:
                    param_count += 1
                    where_clauses.append(f"created_at >= ${param_count}")
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ datetime –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
                    date_from = filters['date_from']
                    if isinstance(date_from, str):
                        from datetime import datetime
                        date_from = datetime.fromisoformat(date_from.replace('Z', '+00:00'))
                    params.append(date_from)
                
                if 'date_to' in filters:
                    param_count += 1
                    where_clauses.append(f"created_at <= ${param_count}")
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ datetime –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
                    date_to = filters['date_to']
                    if isinstance(date_to, str):
                        from datetime import datetime
                        date_to = datetime.fromisoformat(date_to.replace('Z', '+00:00'))
                    params.append(date_to)
                
                if 'topic_id' in filters:
                    param_count += 1
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ topic_id - —ç—Ç–æ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ, –Ω–µ —Å–ø–∏—Å–æ–∫
                    topic_id = filters['topic_id']
                    if isinstance(topic_id, list):
                        topic_id = topic_id[0] if topic_id else None
                    if topic_id is not None:
                        where_clauses.append(f"primary_topic_id = ${param_count}")
                        params.append(int(topic_id))
                
                if 'cluster_id' in filters:
                    param_count += 1
                    where_clauses.append(f"dc.cluster_id = ${param_count}")
                    params.append(filters['cluster_id'])
            
            where_sql = "WHERE " + " AND ".join(where_clauses) if where_clauses else ""
            
            query = f"""
                SELECT dc.*, 
                       array_agg(DISTINCT m.channel_id) as channel_ids,
                       array_agg(DISTINCT c.name) as channel_names,
                       COUNT(cm.message_id) as message_count
                FROM dedup_clusters dc
                LEFT JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                LEFT JOIN messages m ON cm.message_id = m.id
                LEFT JOIN channels c ON m.channel_id = c.id
                {where_sql}
                GROUP BY dc.id, dc.cluster_id
                ORDER BY dc.created_at DESC
                LIMIT ${param_count + 1} OFFSET ${param_count + 2}
            """
            
            params.extend([limit, offset])
            
            rows = await conn.fetch(query, *params)
            
            events = []
            for row in rows:
                events.append({
                    'cluster_id': row['cluster_id'],
                    'title': row['title'],
                    'summary': row['summary'],
                    'created_at': row['created_at'],
                    'updated_at': row['updated_at'],
                    'stats': row['stats'],
                    'channel_ids': row['channel_ids'],
                    'channel_names': row['channel_names'],
                    'message_count': row['message_count']
                })
            
            await conn.close()
            return events
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π: {e}")
            raise

    async def run_batch_dedup(self, limit: int = 1000, threshold: float = 0.8) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º"""
        try:
            old_threshold = self.similarity_threshold
            self.similarity_threshold = threshold
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])

            rows = await conn.fetch(
                """
                SELECT id, channel_id, text_content, published_at
                FROM messages
                WHERE text_content IS NOT NULL AND length(text_content) > 0
                ORDER BY published_at DESC
                LIMIT $1
                """,
                limit
            )

            processed = 0
            created = 0
            appended = 0

            for row in rows:
                processed += 1
                result = await self.process_new_message(
                    message_id=row['id'],
                    text=row['text_content'],
                    channel_id=row['channel_id'],
                    published_at=row['published_at']
                )
                # –ï—Å–ª–∏ –º–µ—Ç–æ–¥ –≤–µ—Ä–Ω—É–ª cluster_id, —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –∏–ª–∏ –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã–π
                if result:
                    # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –±—ã–ª–æ –ø–µ—Ä–≤—ã–º ‚Äî score=1.0 –∏ create_new_cluster –≤—ã–∑—ã–≤–∞–ª—Å—è
                    # –ú—ã –Ω–µ –∑–Ω–∞–µ–º –∑–¥–µ—Å—å —Ç–æ—á–Ω–æ, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ —Å—á–∏—Ç–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–º, –µ—Å–ª–∏ score==1.0 –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–æ—Å—å –≤—ã—à–µ.
                    # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è: —É–≤–µ–ª–∏—á–∏–º created, –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —ç—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ –±—ã–ª–æ 1 —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –≤—Å—Ç–∞–≤–∫–∏.
                    created += 1  # –¥–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫—É –Ω–∞ —Ä–∞–Ω–Ω–µ–º —ç—Ç–∞–ø–µ
                else:
                    appended += 1

            await conn.close()
            self.similarity_threshold = old_threshold

            return {
                'processed': processed,
                'created_clusters_guess': created,
                'threshold': threshold,
                'limit': limit
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {e}")
            raise
    
    async def get_cluster_details(self, cluster_id: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Ç–µ—Ä–µ
            cluster_row = await conn.fetchrow("""
                SELECT * FROM dedup_clusters WHERE cluster_id = $1
            """, cluster_id)
            
            if not cluster_row:
                await conn.close()
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            messages_rows = await conn.fetch("""
                SELECT m.*, cm.similarity_score, cm.is_primary, c.name as channel_name
                FROM cluster_messages cm
                JOIN messages m ON cm.message_id = m.id
                JOIN channels c ON m.channel_id = c.id
                WHERE cm.cluster_id = $1
                ORDER BY cm.similarity_score DESC, m.published_at DESC
            """, cluster_id)
            
            messages = []
            for row in messages_rows:
                messages.append({
                    'message_id': row['id'],
                    'text': row['text_content'],
                    'date': row['published_at'],
                    'channel_name': row['channel_name'],
                    'similarity_score': row['similarity_score'],
                    'is_primary': row['is_primary'],
                    'views': row['views_count'],
                    'forwards': row['forwards_count']
                })
            
            await conn.close()
            
            return {
                'cluster_id': cluster_row['cluster_id'],
                'title': cluster_row['title'],
                'summary': cluster_row['summary'],
                'created_at': cluster_row['created_at'],
                'updated_at': cluster_row['updated_at'],
                'stats': cluster_row['stats'],
                'messages': messages
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –∫–ª–∞—Å—Ç–µ—Ä–∞: {e}")
            raise

    async def cleanup_single_clusters(self) -> int:
        """–£–¥–∞–ª—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –æ–¥–Ω–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
            single_clusters = await conn.fetch("""
                SELECT dc.cluster_id 
                FROM dedup_clusters dc
                LEFT JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                GROUP BY dc.cluster_id
                HAVING COUNT(cm.message_id) = 1
            """)
            
            deleted_count = 0
            for cluster_row in single_clusters:
                cluster_id = cluster_row['cluster_id']
                
                # –£–¥–∞–ª—è–µ–º —Å–≤—è–∑–∏ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∫–ª–∞—Å—Ç–µ—Ä–æ–º
                await conn.execute(
                    "DELETE FROM cluster_messages WHERE cluster_id = $1",
                    cluster_id
                )
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º –∫–ª–∞—Å—Ç–µ—Ä
                await conn.execute(
                    "DELETE FROM dedup_clusters WHERE cluster_id = $1",
                    cluster_id
                )
                
                deleted_count += 1
            
            await conn.close()
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {deleted_count} –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            return deleted_count
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
            return 0

    async def reprocess_all_messages(self, threshold: float = 0.75, limit: int = 1000) -> Dict[str, int]:
        """–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            await conn.execute("DELETE FROM cluster_messages")
            await conn.execute("DELETE FROM dedup_clusters")
            
            # –û—á–∏—â–∞–µ–º Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (posts_search –∏ posts_clustering)
            # –°—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è telegram_messages –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            try:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
                try:
                    await embedding_service.qdrant.delete_collection("telegram_messages")
                    logger.info("–°—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è telegram_messages —É–¥–∞–ª–µ–Ω–∞")
                except Exception:
                    pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –º–æ–∂–µ—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å
                
                # –ö–æ–ª–ª–µ–∫—Ü–∏–∏ posts_search –∏ posts_clustering —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ TopicModelingService
                logger.info("Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –æ—á–∏—â–µ–Ω—ã")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = await conn.fetch("""
                SELECT m.id, m.text_content, m.channel_id, m.published_at
                FROM messages m
                JOIN embeddings e ON m.id = e.message_id
                WHERE m.text_content IS NOT NULL AND LENGTH(m.text_content) > 10
                ORDER BY m.published_at ASC
                LIMIT $1
            """, limit)
            
            await conn.close()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤—ã–π –ø–æ—Ä–æ–≥
            self.similarity_threshold = threshold
            
            processed_count = 0
            clustered_count = 0
            
            for message_row in messages:
                cluster_id = await self.process_new_message(
                    message_row['id'],
                    message_row['text_content'],
                    message_row['channel_id'],
                    message_row['published_at']
                )
                
                processed_count += 1
                if cluster_id:
                    clustered_count += 1
                
                if processed_count % 50 == 0:
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count}/{len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # –û—á–∏—â–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            deleted_singles = await self.cleanup_single_clusters()
            
            logger.info(f"–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processed_count} —Å–æ–æ–±—â–µ–Ω–∏–π, {clustered_count} –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–æ, {deleted_singles} –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —É–¥–∞–ª–µ–Ω–æ")
            
            return {
                'processed': processed_count,
                'clustered': clustered_count,
                'deleted_singles': deleted_singles
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π: {e}")
            return {'processed': 0, 'clustered': 0, 'deleted_singles': 0}

    async def analyze_clustering_quality(self, limit: int = 1000) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: –º–µ—Ç—Ä–∏–∫–∏, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
            clusters = await conn.fetch("""
                SELECT 
                    dc.cluster_id,
                    dc.title,
                    dc.created_at,
                    COUNT(cm.message_id) as message_count,
                    array_agg(DISTINCT cm.similarity_score) as scores
                FROM dedup_clusters dc
                LEFT JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                GROUP BY dc.cluster_id, dc.title, dc.created_at
                ORDER BY dc.created_at DESC
                LIMIT $1
            """, limit)
            
            total_clusters = len(clusters)
            if total_clusters == 0:
                await conn.close()
                return {
                    'status': 'no_clusters',
                    'message': '–ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'
                }
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            cluster_sizes = [row['message_count'] for row in clusters]
            single_clusters = sum(1 for size in cluster_sizes if size == 1)
            small_clusters = sum(1 for size in cluster_sizes if 2 <= size <= 5)
            medium_clusters = sum(1 for size in cluster_sizes if 6 <= size <= 20)
            large_clusters = sum(1 for size in cluster_sizes if size > 20)
            
            # –ê–Ω–∞–ª–∏–∑ similarity scores
            all_scores = []
            for cluster in clusters:
                scores = cluster['scores']
                if scores and len(scores) > 0:
                    all_scores.extend([float(s) for s in scores if s is not None])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            avg_score = sum(all_scores) / len(all_scores) if all_scores else 0
            min_score = min(all_scores) if all_scores else 0
            max_score = max(all_scores) if all_scores else 1
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤
            avg_cluster_size = sum(cluster_sizes) / len(cluster_sizes) if cluster_sizes else 0
            median_cluster_size = sorted(cluster_sizes)[len(cluster_sizes) // 2] if cluster_sizes else 0
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = []
            
            if single_clusters / total_clusters > 0.3:
                recommendations.append({
                    'type': 'warning',
                    'message': f'–ë–æ–ª—å—à–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ({single_clusters / total_clusters * 100:.1f}%). –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π.',
                    'action': '–ü–æ–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏'
                })
            
            if avg_score < 0.7:
                recommendations.append({
                    'type': 'warning',
                    'message': f'–ù–∏–∑–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ ({avg_score:.2f}). –ö–ª–∞—Å—Ç–µ—Ä—ã –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–ª–∞–±–æ—Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è.',
                    'action': '–ü–æ–≤—ã—Å–∏—Ç—å –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏'
                })
            
            if large_clusters > total_clusters * 0.1:
                recommendations.append({
                    'type': 'info',
                    'message': f'–ú–Ω–æ–≥–æ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ({large_clusters}). –í–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ.',
                    'action': '–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤'
                })
            
            await conn.close()
            
            return {
                'status': 'ok',
                'metrics': {
                    'total_clusters': total_clusters,
                    'avg_cluster_size': round(avg_cluster_size, 2),
                    'median_cluster_size': median_cluster_size,
                    'avg_similarity_score': round(avg_score, 3),
                    'min_similarity_score': round(min_score, 3),
                    'max_similarity_score': round(max_score, 3),
                },
                'distribution': {
                    'single_clusters': single_clusters,
                    'small_clusters': small_clusters,
                    'medium_clusters': medium_clusters,
                    'large_clusters': large_clusters
                },
                'recommendations': recommendations,
                'current_threshold': self.similarity_threshold
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return {'status': 'error', 'message': str(e)}
    
    async def get_clustering_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(DISTINCT dc.cluster_id) as total_clusters,
                    COUNT(DISTINCT cm.message_id) as total_messages_in_clusters,
                    AVG(msg_count) as avg_cluster_size,
                    MAX(msg_count) as max_cluster_size
                FROM dedup_clusters dc
                LEFT JOIN (
                    SELECT cluster_id, COUNT(*) as msg_count
                    FROM cluster_messages
                    GROUP BY cluster_id
                ) cm_stats ON dc.cluster_id = cm_stats.cluster_id
                LEFT JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
            """)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ similarity scores
            scores_stats = await conn.fetchrow("""
                SELECT 
                    AVG(similarity_score) as avg_score,
                    MIN(similarity_score) as min_score,
                    MAX(similarity_score) as max_score
                FROM cluster_messages
                WHERE similarity_score IS NOT NULL
            """)
            
            await conn.close()
            
            return {
                'total_clusters': stats['total_clusters'] or 0,
                'total_messages_in_clusters': stats['total_messages_in_clusters'] or 0,
                'avg_cluster_size': round(float(stats['avg_cluster_size']) if stats['avg_cluster_size'] else 0, 2),
                'max_cluster_size': stats['max_cluster_size'] or 0,
                'similarity_score': {
                    'avg': round(float(scores_stats['avg_score']) if scores_stats['avg_score'] else 0, 3),
                    'min': round(float(scores_stats['min_score']) if scores_stats['min_score'] else 0, 3),
                    'max': round(float(scores_stats['max_score']) if scores_stats['max_score'] else 1, 3)
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return {}

    async def _recluster_large_clusters(self, large_cluster_ids: List[str], embeddings_array: np.ndarray, 
                                       messages_data: List[Dict], min_cluster_size: int, epsilon: float) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            split_count = 0
            total_new_clusters = 0
            
            for cluster_id in large_cluster_ids:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
                    cluster_messages = await conn.fetch("""
                        SELECT m.id, m.text_content, m.channel_id, m.published_at
                        FROM cluster_messages cm
                        JOIN messages m ON cm.message_id = m.id
                        WHERE cm.cluster_id = $1
                        ORDER BY m.published_at ASC
                    """, cluster_id)
                    
                    if len(cluster_messages) <= 30:
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
                    cluster_embeddings_list = []
                    cluster_message_data = []
                    for msg_row in cluster_messages:
                        try:
                            emb = await embedding_service.provider.get_embedding(msg_row['text_content'])
                            if emb:
                                cluster_embeddings_list.append(emb)
                                cluster_message_data.append({
                                    'id': msg_row['id'],
                                    'text': msg_row['text_content'],
                                    'channel_id': msg_row['channel_id'],
                                    'published_at': msg_row['published_at']
                                })
                        except Exception as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {msg_row['id']}: {e}")
                            continue
                    
                    if len(cluster_embeddings_list) < min_cluster_size * 2:
                        continue
                    
                    cluster_embeddings = np.array(cluster_embeddings_list)
                    
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
                    from sklearn.preprocessing import StandardScaler
                    scaler = StandardScaler()
                    cluster_embeddings_scaled = scaler.fit_transform(cluster_embeddings)
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π epsilon –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                    stricter_epsilon = max(0.01, epsilon * 0.5)  # –í –¥–≤–∞ —Ä–∞–∑–∞ —Å—Ç—Ä–æ–∂–µ
                    
                    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    reclusterer = hdbscan.HDBSCAN(
                        min_cluster_size=min_cluster_size,
                        min_samples=max(2, min_cluster_size - 1),
                        metric='euclidean',
                        cluster_selection_epsilon=stricter_epsilon,
                        cluster_selection_method='eom',
                        alpha=0.3,
                        leaf_size=10
                    )
                    
                    sub_labels = reclusterer.fit_predict(cluster_embeddings_scaled)
                    n_sub_clusters = len(set(sub_labels)) - (1 if -1 in sub_labels else 0)
                    
                    if n_sub_clusters <= 1:
                        logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id[:8]}... –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å –Ω–∞ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä—ã")
                        continue
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤—ã–º –º–µ—Ç–∫–∞–º
                    sub_clusters = {}
                    for i, sub_label in enumerate(sub_labels):
                        if sub_label == -1:
                            continue
                        if sub_label not in sub_clusters:
                            sub_clusters[sub_label] = []
                        sub_clusters[sub_label].append(cluster_message_data[i])
                    
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –∫–ª–∞—Å—Ç–µ—Ä –∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–µ
                    await conn.execute("DELETE FROM cluster_messages WHERE cluster_id = $1", cluster_id)
                    await conn.execute("DELETE FROM dedup_clusters WHERE cluster_id = $1", cluster_id)
                    
                    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
                    for sub_label, sub_messages in sub_clusters.items():
                        if not sub_messages:
                            continue
                        
                        first_msg = sub_messages[0]
                        new_cluster_id = await self._create_new_cluster(
                            message_id=first_msg['id'],
                            text=first_msg['text'],
                            channel_id=first_msg['channel_id'],
                            published_at=first_msg['published_at']
                        )
                        
                        # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä–∞ (–≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä–∞)
                        sub_indices = [i for i, m in enumerate(cluster_message_data) if m['id'] in [sm['id'] for sm in sub_messages]]
                        if sub_indices:
                            sub_embeddings = cluster_embeddings[sub_indices]
                            sub_centroid = np.mean(sub_embeddings, axis=0)
                        else:
                            # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                            first_msg_idx = next(i for i, m in enumerate(cluster_message_data) if m['id'] == first_msg['id'])
                            sub_centroid = cluster_embeddings[first_msg_idx]
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ä–µ–∞–ª—å–Ω–æ–π similarity
                        def cosine_sim(v1, v2):
                            dot = np.dot(v1, v2)
                            n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)
                            return float(dot / (n1 * n2)) if n1 > 0 and n2 > 0 else 0.0
                        
                        for msg in sub_messages[1:]:
                            msg_idx = next(i for i, m in enumerate(cluster_message_data) if m['id'] == msg['id'])
                            msg_emb = cluster_embeddings[msg_idx]
                            similarity = cosine_sim(msg_emb, sub_centroid)
                            
                            await self._add_message_to_cluster(
                                message_id=msg['id'],
                                cluster_id=new_cluster_id,
                                similarity_score=similarity
                            )
                        
                        total_new_clusters += 1
                    
                    split_count += 1
                    logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id[:8]}... —Ä–∞–∑–±–∏—Ç –Ω–∞ {n_sub_clusters} –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}: {e}")
                    continue
            
            await conn.close()
            
            return {
                'split_clusters': split_count,
                'new_clusters_created': total_new_clusters
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
            return {
                'split_clusters': 0,
                'new_clusters_created': 0
            }

    async def split_large_clusters(self, max_size: int = 20, inner_threshold: float = 0.9, time_bucket_days: int = 1) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å–ª–∏—à–∫–æ–º –∫—Ä—É–ø–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ.

        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1) –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã —Å —Ä–∞–∑–º–µ—Ä–æ–º > max_size.
        2) –î–µ–ª–∏–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º (time_bucket_days).
        3) –í–Ω—É—Ç—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤—ã–ø–æ–ª–Ω—è–µ–º –∂–∞–¥–Ω—É—é —Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É (–ø–æ—Ä–æ–≥ inner_threshold).
        4) –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–¥–≥—Ä—É–ø–ø—ã —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏–º —Å–æ–æ–±—â–µ–Ω–∏—è.
        5) –ò—Å—Ö–æ–¥–Ω—ã–π –∫—Ä—É–ø–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä —É–¥–∞–ª—è–µ—Ç—Å—è.
        """
        try:
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])

            # 1) –ù–∞—Ö–æ–¥–∏–º –∫—Ä—É–ø–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            large_clusters = await conn.fetch(
                """
                SELECT dc.cluster_id, COUNT(cm.message_id) as size
                FROM dedup_clusters dc
                JOIN cluster_messages cm ON dc.cluster_id = cm.cluster_id
                GROUP BY dc.cluster_id
                HAVING COUNT(cm.message_id) > $1
                ORDER BY size DESC
                """,
                max_size
            )

            if not large_clusters:
                await conn.close()
                return {
                    'status': 'ok',
                    'processed_clusters': 0,
                    'created_clusters': 0,
                    'moved_messages': 0
                }

            processed = 0
            created_total = 0
            moved_total = 0

            for row in large_clusters:
                processed += 1
                cluster_id = row['cluster_id']

                # 2) –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
                messages = await conn.fetch(
                    """
                    SELECT m.id, m.text_content, m.channel_id, m.published_at
                    FROM cluster_messages cm
                    JOIN messages m ON cm.message_id = m.id
                    WHERE cm.cluster_id = $1 AND m.text_content IS NOT NULL AND LENGTH(m.text_content) > 10
                    ORDER BY m.published_at ASC
                    """,
                    cluster_id
                )

                if not messages:
                    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
                    await conn.execute("DELETE FROM dedup_clusters WHERE cluster_id = $1", cluster_id)
                    continue

                # 3) –î–µ–ª–∏–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Å–µ–≥–º–µ–Ω—Ç–∞–º
                from collections import defaultdict
                buckets: Dict[str, List[Any]] = defaultdict(list)
                for m in messages:
                    dt: datetime = m['published_at']
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –Ω–∞—á–∞–ª—É —Å–µ–≥–º–µ–Ω—Ç–∞ (–∫—Ä–∞—Ç–Ω–æ–º—É time_bucket_days)
                    bucket_key = dt.strftime('%Y-%m-%d')
                    if time_bucket_days > 1:
                        # –ü—Ä–æ—Å—Ç–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ floor(day / bucket)
                        # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–º–µ—Ä –¥–Ω—è –≤ –≥–æ–¥—É // bucket
                        day_index = int(dt.strftime('%j'))
                        year = dt.strftime('%Y')
                        bucket_key = f"{year}-d{(day_index-1)//time_bucket_days}"
                    buckets[bucket_key].append(m)

                # 4) –í–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ ‚Äî –∂–∞–¥–Ω–∞—è —Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
                subgroups: List[List[Any]] = []
                for _, group_msgs in buckets.items():
                    if len(group_msgs) <= max_size:
                        subgroups.append(group_msgs)
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã
                    embeddings: Dict[int, List[float]] = {}
                    for m in group_msgs:
                        try:
                            emb = await embedding_service.provider.get_embedding(m['text_content'])
                        except Exception:
                            emb = None
                        embeddings[m['id']] = emb

                    # –ü—Ä–æ—Å—Ç–∞—è –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
                    from math import sqrt
                    def cosine(a: List[float], b: List[float]) -> float:
                        if not a or not b:
                            return 0.0
                        dot = sum(x*y for x, y in zip(a, b))
                        na = sqrt(sum(x*x for x in a))
                        nb = sqrt(sum(y*y for y in b))
                        if na == 0 or nb == 0:
                            return 0.0
                        return dot / (na * nb)

                    # –ñ–∞–¥–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
                    remaining = list(group_msgs)
                    while remaining:
                        seed = remaining.pop(0)
                        seed_emb = embeddings.get(seed['id'])
                        current_group = [seed]
                        rest = []
                        for m in remaining:
                            sim = cosine(seed_emb, embeddings.get(m['id']))
                            if sim >= inner_threshold and len(current_group) < max_size:
                                current_group.append(m)
                            else:
                                rest.append(m)
                        subgroups.append(current_group)
                        remaining = rest

                # 5) –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏–º —Å–æ–æ–±—â–µ–Ω–∏—è
                created_for_this = 0
                moved_for_this = 0
                new_cluster_ids: List[str] = []
                for subgroup in subgroups:
                    if not subgroup:
                        continue
                    # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä –ø–æ–¥ –≥—Ä—É–ø–ø—É
                    first = subgroup[0]
                    new_cluster_id = await self._create_new_cluster(
                        message_id=first['id'],
                        text=first['text_content'],
                        channel_id=first['channel_id'],
                        published_at=first['published_at']
                    )
                    new_cluster_ids.append(new_cluster_id)
                    created_for_this += 1

                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                    for m in subgroup[1:]:
                        try:
                            # –û—Ü–µ–Ω–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø–µ—Ä–≤—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º –ø–æ–¥–≥—Ä—É–ø–ø—ã
                            emb_first = await embedding_service.provider.get_embedding(first['text_content'])
                            emb_cur = await embedding_service.provider.get_embedding(m['text_content'])
                            # –ö–æ—Å–∏–Ω—É—Å
                            from math import sqrt
                            def cos(a, b):
                                if not a or not b:
                                    return 0.0
                                dot = sum(x*y for x, y in zip(a, b))
                                na = sqrt(sum(x*x for x in a))
                                nb = sqrt(sum(y*y for y in b))
                                if na == 0 or nb == 0:
                                    return 0.0
                                return dot / (na * nb)
                            score = cos(emb_first, emb_cur)
                            await self._add_message_to_cluster(m['id'], new_cluster_id, score)
                            moved_for_this += 1
                        except Exception:
                            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—Ä–æ–±—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º —Å–∫–æ—Ä–æ–º
                            await self._add_message_to_cluster(m['id'], new_cluster_id, 0.0)
                            moved_for_this += 1

                # 6) –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫—Ä—É–ø–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –∏ –µ–≥–æ —Å–≤—è–∑–∏
                await conn.execute("DELETE FROM cluster_messages WHERE cluster_id = $1", cluster_id)
                await conn.execute("DELETE FROM dedup_clusters WHERE cluster_id = $1", cluster_id)

                created_total += created_for_this
                moved_total += moved_for_this

            await conn.close()

            return {
                'status': 'ok',
                'processed_clusters': processed,
                'created_clusters': created_total,
                'moved_messages': moved_total,
                'params': {
                    'max_size': max_size,
                    'inner_threshold': inner_threshold,
                    'time_bucket_days': time_bucket_days
                }
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Å–ø–ª–∏—Ç–∞ –∫—Ä—É–ø–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
            return {
                'status': 'error',
                'message': str(e)
            }

    async def run_clustering(self, limit: int = 1000, min_cluster_size: int = 3, pca_dimensions: int = 50, 
                           time_window_days: int = 7, cluster_selection_epsilon: float = None, 
                           disable_pca: bool = False, max_title_texts: int = 10, 
                           max_title_chars_per_text: int = 500) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é —Å PCA —Å–∂–∞—Ç–∏–µ–º –≤–µ–∫—Ç–æ—Ä–æ–≤
        
        Args:
            limit: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            min_cluster_size: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
            pca_dimensions: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ PCA —Å–∂–∞—Ç–∏—è
            time_window_days: –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            cluster_selection_epsilon: –ø–∞—Ä–∞–º–µ—Ç—Ä epsilon –¥–ª—è HDBSCAN (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π)
            disable_pca: –µ—Å–ª–∏ True, PCA –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å HDBSCAN –∏ PCA
        if not HDBSCAN_AVAILABLE:
            return {
                'status': 'error',
                'message': 'HDBSCAN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install hdbscan'
            }
        
        if not PCA_AVAILABLE:
            return {
                'status': 'error',
                'message': 'PCA –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn'
            }
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            self._current_max_title_texts = max_title_texts
            self._current_max_title_chars_per_text = max_title_chars_per_text
            
            config = get_config()
            conn = await asyncpg.connect(dsn=config['postgresql']['dsn'])
            
            # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
            logger.info("–û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
            await conn.execute("DELETE FROM cluster_messages")
            await conn.execute("DELETE FROM dedup_clusters")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∑–∞ –∑–∞–¥–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è
            cutoff_date = datetime.now() - timedelta(days=time_window_days)
            rows = await conn.fetch("""
                SELECT m.id, m.text_content, m.channel_id, m.published_at
                FROM messages m
                JOIN embeddings e ON m.id = e.message_id
                WHERE m.published_at >= $1
                  AND m.text_content IS NOT NULL
                  AND LENGTH(m.text_content) > 10
                ORDER BY m.published_at ASC
                LIMIT $2
            """, cutoff_date, limit)
            
            if not rows:
                await conn.close()
                return {
                    'status': 'ok',
                    'message': '–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏',
                    'clusters_created': 0,
                    'messages_processed': 0
                }
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            message_ids = []
            embeddings_list = []
            messages_data = []
            
            for row in rows:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ Qdrant —á–µ—Ä–µ–∑ embedding_service
                    msg_text = row['text_content']
                    emb = await embedding_service.provider.get_embedding(msg_text)
                    
                    if emb and len(emb) > 0:
                        message_ids.append(row['id'])
                        embeddings_list.append(emb)
                        messages_data.append({
                            'id': row['id'],
                            'text': row['text_content'],
                            'channel_id': row['channel_id'],
                            'published_at': row['published_at']
                        })
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è {row['id']}: {e}")
                    continue
            
            if len(embeddings_list) < min_cluster_size:
                await conn.close()
                return {
                    'status': 'error',
                    'message': f'–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º {min_cluster_size})'
                }
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
            embeddings_array = np.array(embeddings_list)
            logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(embeddings_array)} —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {embeddings_array.shape[1]}")
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            embeddings_scaled = scaler.fit_transform(embeddings_array)
            logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            
            # PCA —Å–∂–∞—Ç–∏–µ (–µ—Å–ª–∏ –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ)
            n_samples, n_features = embeddings_array.shape
            
            if disable_pca:
                embeddings_reduced = embeddings_scaled
                explained_variance = 1.0
                logger.info("PCA –æ—Ç–∫–ª—é—á–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã")
            else:
                # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                if n_samples < 50:
                    # –î–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                    target_dims = min(pca_dimensions, n_features // 2)
                elif n_samples < 200:
                    # –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                    target_dims = min(pca_dimensions * 2, n_features - 1)
                else:
                    # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º—É–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                    target_dims = min(pca_dimensions * 3, n_features - 1)
                
                # n_components –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å < min(n_samples, n_features)
                max_allowed = max(1, min(n_samples - 1, n_features - 1))
                n_components = min(target_dims, max_allowed)
                
                if n_components < n_features and n_components >= 10:
                    logger.info(f"–ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ PCA —Å–∂–∞—Ç–∏–µ: {n_features} -> {n_components} –∏–∑–º–µ—Ä–µ–Ω–∏–π (samples={n_samples})")
                    pca = PCA(n_components=n_components, svd_solver='auto', random_state=42)
                    embeddings_reduced = pca.fit_transform(embeddings_scaled)
                    try:
                        explained_variance = float(np.sum(pca.explained_variance_ratio_))
                    except Exception:
                        explained_variance = 0.0
                    logger.info(f"PCA –æ–±—ä—è—Å–Ω—è–µ—Ç {explained_variance:.2%} –¥–∏—Å–ø–µ—Ä—Å–∏–∏")
                    
                    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –æ–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞
                    if explained_variance < 0.8:
                        logger.warning(f"PCA –æ–±—ä—è—Å–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ {explained_variance:.2%} –¥–∏—Å–ø–µ—Ä—Å–∏–∏ - –≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ")
                else:
                    embeddings_reduced = embeddings_scaled
                    explained_variance = 1.0
                    logger.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º PCA —Å–∂–∞—Ç–∏–µ (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {n_components}, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {n_features})")
            
            # HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å PCA —Å–∂–∞—Ç–∏–µ–º
            logger.info(f"–ó–∞–ø—É—Å–∫ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å PCA —Å–∂–∞—Ç–∏–µ–º")
            logger.info(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(embeddings_reduced)} —Å–æ–æ–±—â–µ–Ω–∏–π, {embeddings_reduced.shape[1]} –∏–∑–º–µ—Ä–µ–Ω–∏–π")
            
            try:
                # HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
                if n_samples < 50:
                    # –î–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    adaptive_min_cluster = max(2, min_cluster_size)
                    default_epsilon = 0.2
                    adaptive_alpha = 0.7
                elif n_samples < 200:
                    # –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    adaptive_min_cluster = max(2, min_cluster_size - 1)
                    default_epsilon = 0.3
                    adaptive_alpha = 0.5
                else:
                    # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö - –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    adaptive_min_cluster = max(2, min_cluster_size - 1)
                    default_epsilon = 0.4
                    adaptive_alpha = 0.3
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π epsilon –∏–ª–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π
                final_epsilon = cluster_selection_epsilon if cluster_selection_epsilon is not None else default_epsilon
                
                clusterer = hdbscan.HDBSCAN(
                    min_cluster_size=adaptive_min_cluster,
                    min_samples=max(2, adaptive_min_cluster - 1),
                    metric='euclidean',
                    cluster_selection_epsilon=final_epsilon,
                    cluster_selection_method='eom',
                    alpha=adaptive_alpha,
                    leaf_size=20 if n_samples > 100 else 10
                )
                
                logger.info(f"HDBSCAN –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: min_cluster_size={adaptive_min_cluster}, epsilon={final_epsilon}, alpha={adaptive_alpha}")
                
                cluster_labels = clusterer.fit_predict(embeddings_reduced)
                
                # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ HDBSCAN
                n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                n_noise = list(cluster_labels).count(-1)
                
                logger.info(f"HDBSCAN: —Å–æ–∑–¥–∞–Ω–æ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, {n_noise} —à—É–º–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
                
                # –ï—Å–ª–∏ HDBSCAN –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
                if n_clusters == 0:
                    logger.warning("HDBSCAN –Ω–µ —Å–æ–∑–¥–∞–ª –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã")
                    
                    # 1. –ü—Ä–æ–±—É–µ–º DBSCAN
                    try:
                        from sklearn.cluster import DBSCAN
                        from sklearn.neighbors import NearestNeighbors
                        
                        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä eps –¥–ª—è DBSCAN
                        nbrs = NearestNeighbors(n_neighbors=adaptive_min_cluster).fit(embeddings_reduced)
                        distances, indices = nbrs.kneighbors(embeddings_reduced)
                        distances = np.sort(distances[:, adaptive_min_cluster-1], axis=0)
                        eps = distances[int(len(distances) * 0.1)]  # 10-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å
                        
                        dbscan = DBSCAN(eps=eps, min_samples=adaptive_min_cluster)
                        cluster_labels = dbscan.fit_predict(embeddings_reduced)
                        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                        n_noise = list(cluster_labels).count(-1)
                        
                        if n_clusters > 0:
                            logger.info(f"DBSCAN: —Å–æ–∑–¥–∞–Ω–æ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, {n_noise} —à—É–º–æ–≤—ã—Ö (eps={eps:.3f})")
                        else:
                            raise Exception("DBSCAN –Ω–µ —Å–æ–∑–¥–∞–ª –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                            
                    except Exception as e:
                        logger.warning(f"DBSCAN –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
                        
                        # 2. –ü—Ä–æ–±—É–µ–º K-means
                        try:
                            from sklearn.cluster import KMeans
                            from sklearn.metrics import silhouette_score
                            
                            best_k = 2
                            best_score = -1
                            
                            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                            for n_clusters_k in range(2, min(20, len(embeddings_reduced) // 2) + 1):
                                if n_clusters_k < len(embeddings_reduced):
                                    kmeans = KMeans(n_clusters=n_clusters_k, random_state=42, n_init=10)
                                    labels = kmeans.fit_predict(embeddings_reduced)
                                    
                                    if len(set(labels)) > 1:
                                        score = silhouette_score(embeddings_reduced, labels)
                                        if score > best_score:
                                            best_score = score
                                            best_k = n_clusters_k
                            
                            # –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –ª—É—á—à–∏–º k
                            kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
                            cluster_labels = kmeans.fit_predict(embeddings_reduced)
                            n_clusters = best_k
                            n_noise = 0
                            logger.info(f"K-means: —Å–æ–∑–¥–∞–Ω–æ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (silhouette={best_score:.3f})")
                            
                        except Exception as e2:
                            logger.warning(f"K-means –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e2}")
                            # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
                            cluster_labels = list(range(len(embeddings_reduced)))
                            n_clusters = len(embeddings_reduced)
                            n_noise = 0
                            logger.warning("–°–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π fallback")
                
                # –ï—Å–ª–∏ –∏ K-means –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
                if n_clusters == 0:
                    logger.warning("–í—Å–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã")
                    cluster_labels = list(range(len(embeddings_reduced)))
                    n_clusters = len(embeddings_reduced)
                    n_noise = 0
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
                # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
                cluster_labels = list(range(len(embeddings_reduced)))
                n_clusters = len(embeddings_reduced)
                n_noise = 0
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω –≤—ã—à–µ)
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ –ë–î
            cluster_map = {}  # label -> cluster_id
            messages_processed = 0
            
            # –°–Ω–∞—á–∞–ª–∞ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –º–µ—Ç–∫–∞–º –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            clusters_by_label = {}
            for i, label in enumerate(cluster_labels):
                if label not in clusters_by_label:
                    clusters_by_label[label] = []
                clusters_by_label[label].append(messages_data[i])
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞—Å—á—ë—Ç–∞ similarity_score
            cluster_centroids = {}
            for label, indices in clusters_by_label.items():
                if label == -1:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º
                    continue
                cluster_indices = [messages_data.index(m) for m in clusters_by_label[label]]
                if len(cluster_indices) > 0:
                    cluster_embeddings = embeddings_array[cluster_indices]
                    centroid = np.mean(cluster_embeddings, axis=0)
                    cluster_centroids[label] = centroid
            
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
            def cosine_similarity(vec1, vec2):
                """–í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
                dot_product = np.dot(vec1, vec2)
                norm1 = np.linalg.norm(vec1)
                norm2 = np.linalg.norm(vec2)
                if norm1 == 0 or norm2 == 0:
                    return 0.0
                return float(dot_product / (norm1 * norm2))
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
            similarity_stats_per_cluster = {}  # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è similarity
            
            for label, messages in clusters_by_label.items():
                if label == -1:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                    continue
                    
                # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–∑–¥–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä
                first_msg = messages[0]
                cluster_id = await self._create_new_cluster(
                    message_id=first_msg['id'],
                    text=first_msg['text'],
                    channel_id=first_msg['channel_id'],
                    published_at=first_msg['published_at']
                )
                cluster_map[label] = cluster_id
                messages_processed += 1
                
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –º–∞—Å—Å–∏–≤–∞—Ö
                first_msg_index = next(i for i, m in enumerate(messages_data) if m['id'] == first_msg['id'])
                first_msg_embedding = embeddings_array[first_msg_index]
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ü–µ–Ω—Ç—Ä–æ–∏–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                centroid = cluster_centroids.get(label, first_msg_embedding)
                
                # –°–ø–∏—Å–æ–∫ similarity scores –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
                cluster_similarities = []
                
                # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç similarity 1.0 (—ç—Ç–æ "—è–∫–æ—Ä—å" –∫–ª–∞—Å—Ç–µ—Ä–∞)
                # similarity_score=1.0 —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ _create_new_cluster
                cluster_similarities.append(1.0)
                
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–æ–±–∞–≤–ª—è–µ–º –∫ –∫–ª–∞—Å—Ç–µ—Ä—É —Å —Ä–µ–∞–ª—å–Ω–æ–π similarity
                for msg in messages[1:]:
                    msg_index = next(i for i, m in enumerate(messages_data) if m['id'] == msg['id'])
                    msg_embedding = embeddings_array[msg_index]
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞
                    similarity = cosine_similarity(msg_embedding, centroid)
                    cluster_similarities.append(similarity)
                    
                    await self._add_message_to_cluster(
                        message_id=msg['id'],
                        cluster_id=cluster_id,
                        similarity_score=similarity
                    )
                    messages_processed += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É similarity –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
                if cluster_similarities:
                    similarity_stats_per_cluster[cluster_id] = {
                        'min': float(np.min(cluster_similarities)),
                        'max': float(np.max(cluster_similarities)),
                        'avg': float(np.mean(cluster_similarities)),
                        'median': float(np.median(cluster_similarities)),
                        'std': float(np.std(cluster_similarities)),
                        'values': [float(v) for v in cluster_similarities]
                    }
                    logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id[:8]}...: similarity min={similarity_stats_per_cluster[cluster_id]['min']:.3f}, "
                              f"max={similarity_stats_per_cluster[cluster_id]['max']:.3f}, "
                              f"avg={similarity_stats_per_cluster[cluster_id]['avg']:.3f}")
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            silhouette_avg = None
            if n_clusters > 1 and len(embeddings_reduced) > n_clusters:
                try:
                    from sklearn.metrics import silhouette_score
                    # –î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –±–µ—Ä–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É
                    sample_size = min(1000, len(embeddings_reduced))
                    indices = np.random.choice(len(embeddings_reduced), sample_size, replace=False)
                    silhouette_avg = float(silhouette_score(
                        embeddings_reduced[indices],
                        cluster_labels[indices]
                    ))
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å silhouette score: {e}")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            large_clusters_split = {}
            if len(cluster_map) > 0:
                # –ù–∞—Ö–æ–¥–∏–º –±–æ–ª—å—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–±–æ–ª–µ–µ 30 —Å–æ–æ–±—â–µ–Ω–∏–π)
                large_cluster_ids = []
                for label, cluster_id in cluster_map.items():
                    if label in clusters_by_label:
                        cluster_size = len(clusters_by_label[label])
                        if cluster_size > 30:
                            large_cluster_ids.append(cluster_id)
                
                if large_cluster_ids:
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(large_cluster_ids)} –±–æ–ª—å—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
                    split_result = await self._recluster_large_clusters(
                        large_cluster_ids, 
                        embeddings_array, 
                        messages_data,
                        min_cluster_size=max(2, min_cluster_size),
                        epsilon=final_epsilon if cluster_selection_epsilon is not None else 0.05
                    )
                    large_clusters_split = split_result
            
            await conn.close()
            
            # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ similarity
            all_similarities = []
            for stats in similarity_stats_per_cluster.values():
                all_similarities.extend(stats['values'])
            
            similarity_global_stats = {}
            if all_similarities:
                similarity_global_stats = {
                    'min': float(np.min(all_similarities)),
                    'max': float(np.max(all_similarities)),
                    'avg': float(np.mean(all_similarities)),
                    'median': float(np.median(all_similarities)),
                    'std': float(np.std(all_similarities))
                }
            
            return {
                'status': 'ok',
                'clusters_created': len(cluster_map),
                'messages_processed': messages_processed,
                'noise_messages': n_noise,
                'metrics': {
                    'silhouette_score': round(silhouette_avg, 3) if silhouette_avg else None,
                    'pca_variance_explained': round(explained_variance, 3),
                    'original_dimensions': embeddings_array.shape[1],
                    'reduced_dimensions': embeddings_reduced.shape[1],
                    'similarity_distribution': similarity_global_stats,
                    'similarity_per_cluster': similarity_stats_per_cluster
                },
                'params': {
                    'min_cluster_size': min_cluster_size,
                    'pca_dimensions': pca_dimensions,
                    'time_window_days': time_window_days,
                    'limit': limit,
                    'cluster_selection_epsilon': final_epsilon,
                    'disable_pca': disable_pca
                },
                'large_clusters_split': large_clusters_split
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return {
                'status': 'error',
                'message': str(e)
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä–æ–µ –∏–º—è)
deduplication_service = SemanticClusteringService()
clustering_service = SemanticClusteringService()  # –ù–æ–≤–æ–µ –∏–º—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ–¥–µ
